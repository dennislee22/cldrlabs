{"0": {
    "doc": "Additional Config",
    "title": "Version",
    "content": "The layout includes an updated timestamp in the footer by default, but can also include a version string. This will automatically pull the tag_name from the latest release of the repository on GitHub. If there is no latest release or tag name, then it will fall back to the build_revision, i.e. the git SHA, of the commit that triggered the page build. If you don’t want this behavior, you can explicitly set the version attribute in your _config.yml file and it’s value will be displayed in the footer next to the updated timestamp. version: v1.2.3 . If you don’t want a version to be displayed at all, you can opt-out by setting version to `false. version: false . ",
    "url": "http://10.15.4.152:9090/docs/content/additional/#version",
    "relUrl": "/docs/content/additional/#version"
  },"1": {
    "doc": "Additional Config",
    "title": "Automatic Page Titles",
    "content": "The layout can automatically add a &lt;h1&gt; element at the top of the content containing the page’s title if it is present. To enable this, set auto_page_title to true in your _config.yml file. auto_page_title: true . Per-page Opt-out . If you don’t want this behavior on a specific page, you can add set auto_title to false in the front matter of the page. --- id: my-page title: My Title auto_title: false --- # My explicit title here . ",
    "url": "http://10.15.4.152:9090/docs/content/additional/#automatic-page-titles",
    "relUrl": "/docs/content/additional/#automatic-page-titles"
  },"2": {
    "doc": "Additional Config",
    "title": "Mermaid",
    "content": "This theme supports rendering of MermaidJS diagrams. It does so in a way that maintains compatibility with the native Mermaid diagram rendering in the Github UI. For example: . graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; . ",
    "url": "http://10.15.4.152:9090/docs/content/additional/#mermaid",
    "relUrl": "/docs/content/additional/#mermaid"
  },"3": {
    "doc": "Additional Config",
    "title": "Disabling Mermaid Support",
    "content": "If you don’t want to use Mermaid diagrams, you can set the mermaid_enabled option in your _config.yml file to false. ",
    "url": "http://10.15.4.152:9090/docs/content/additional/#mermaid-enabled",
    "relUrl": "/docs/content/additional/#mermaid-enabled"
  },"4": {
    "doc": "Additional Config",
    "title": "Customising the Mermaid theme",
    "content": "Mermaid supports a number of default themes. This can be configured using the mermaid_theme option in your _config.yml file. Note that this theme must be supported in the Mermaid configuration. ",
    "url": "http://10.15.4.152:9090/docs/content/additional/#mermaid-theme",
    "relUrl": "/docs/content/additional/#mermaid-theme"
  },"5": {
    "doc": "Additional Config",
    "title": "Additional Config",
    "content": "| Property | Default | . | version | site.github.latest_release.tag_name | . | auto_page_title | false | . | mermaid_enabled | true | . | meraid_theme | forest | . ",
    "url": "http://10.15.4.152:9090/docs/content/additional/",
    "relUrl": "/docs/content/additional/"
  },"6": {
    "doc": "CDP Private Cloud",
    "title": "CDP Private Cloud Deployments",
    "content": "CDP Base represents deployment of ‘Traditional’ clusters via Cloudera Manager, which you might think of historically as hadoop or big data clusters. In practice, there is an Ansible Collection called cloudera.cluster which is excellent at configuring Cloudera Manager across various versions of Linux regardless of whether that OS is running in Containers, Virtual Machines, Cloud Infra like EC2, or old school baremetal nodes. Cloudera.cluster is wrapped within Cloudera-Deploy, so again in practice you are most likely to simply use Cloudera-Deploy as the entrypoint to drive cloudera.cluster to create the cluster you want via the Definition mechanism explained in these docs. The differences are that Cloudera.cluster has been built over many generations to support Cloudera Manager versions 5 to 7, and supports many legacy options and behaviors for user convenience. The result of this is that, while simple deployments are as simple as the more modern CDP Public Cloud, complex deployments will require a greater familiarity with Ansible and Cloudera Manager templates. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_cdp/#cdp-private-cloud-deployments",
    "relUrl": "/docs/content/ansible_cdp/#cdp-private-cloud-deployments"
  },"7": {
    "doc": "CDP Private Cloud",
    "title": "Ansible Controller options",
    "content": "We still strongly recommend that you use the cloudera-runner Docker Container shipped with Cloudera-Deploy as your Ansible controller for CDP Base deployments, because the consistency and logging is going to save you time and effort. However, it is not uncommon to use the target Cloudera Manager node as the Controller. It is also not uncommon to need to deploy to air-gapped environments or environments where Docker is not permitted. As such, there is a https://github.com/cloudera-labs/cloudera-deploy/blob/main/centos7-init.sh[script] for deploying the dependencies for Cloudera-Deploy on centos7 as an example in the repo for Power Users in these scenarios. Note that the script covers dependencies for all Cloudera Products on all current Clouds, so you may wish to exclude parts irrelevant to your scenario. NOTE: It is worth noting that the admin_password field from your Profile, as outlined in the Quickstart, will be used as the Cloudera Manager admin password during cloudera.cluster deployments. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_cdp/#ansible-controller-options",
    "relUrl": "/docs/content/ansible_cdp/#ansible-controller-options"
  },"8": {
    "doc": "CDP Private Cloud",
    "title": "Inventories",
    "content": "An inventory file can be supplied using the traditional -i flag at runtime or by including either inventory_static.ini or inventory_template.ini in the definitions path.inventory_static.ini An Ansible inventory covering the hosts in-scope for the deployment. An inventory with this name will be picked up automatically if present without using the -i to specify the inventory file at runtime. Common inventory group names are: . | cloudera_manager | cluster_master_nodes | cluster_worker_nodes | cluster_edge_nodes | . Each host in these groups should be assigned one of the host templates that is defined in cluster.yml. These can be applied per host by setting host_template=TemplateName after the hostname, or per group by setting host_template=TemplateName in the group vars. See https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html#adding-variables-to-inventory[here] for more information on Ansible inventory vars. Nodes that should be added to the CDP Cluster as defined in the cluster.yml must be added as children to the cluster group. For example, if all data nodes are present under the cluster_worker_nodes group, then this group should be listed under cluster:children. See https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html#inheriting-variable-values-group-variables-for-groups-of-groups[here] for an example of setting group children. All nodes/groups should be added as children of the deployment group [deployment:children]. This includes the cluster group, as well as all non-CDP Cluster groups, for example, Cloudera Manager and supporting infrastructure nodes, perhaps running HAProxy or an RDBMS, that are deployed with the cluster but are not running CDP Roles. This allows for deployment-wide variables to be set, such as the common ssh key that can be used to access all nodes. WARNING: Do not use the [all:vars] group if you are using an independent Ansible Controller as Plays will also be applied to it via the localhost mechanic. This includes from your Laptop or the Ansible-Runner supplied in Cloudera-Deploy. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_cdp/#inventories",
    "relUrl": "/docs/content/ansible_cdp/#inventories"
  },"9": {
    "doc": "CDP Private Cloud",
    "title": "Process Overview",
    "content": "In this section we will walk through a typical deployment, commenting on options at various points . Prepare a Definition Path . You are advised to create a working directory for your definition in your preferred Projects directory. If you have checked out Cloudera-Deploy from Github following the Quickstart, then you could make a copy of the folder cloudera-deploy/examples/sandbox into the same base code Projects directory, and use it as a starting point. You might then have: . ~/Projects/cloudera-deploy # contains quickstart.sh, main.yml ~/Projects/my-definition # copied from ~/Projects/cloudera-deploy/examples/sandbox . Infrastructure and Inventory . You are advised to prepare your nodes in advance by whatever mechanism you prefer, and then include them in an Ansible Inventory matching the shape of the Cluster you want to deploy. You may use Cloudera-Deploy to automatically provision EC2 instances based on the inventory template, but this is intended for test and training purposes. An example inventory template is included in the https://github.com/cloudera-labs/cloudera-deploy/blob/main/examples/sandbox/inventory_template.ini[Cloudera-Deploy Sandbox Example]. There is also a copy of the static_inventory.ini file in this directory. If you want to use a static inventory, you may use any of the typical Ansible inventory approaches such as -i on the cmdline, the Inventory path, or including a file inventory_static.ini in your Definition Path which Cloudera-Deploy will automatically pick up. You may wish to include additional variables in the Inventory, most typically the ssh user and private key. These should probably be in the [deployment:vars] section. [deployment:vars] ansible_ssh_private_key_file=~/.ssh/root_key ansible_user=root . NOTE: When you run quickstart.sh to launch the Runner Container, it will mount your local user ~/.ssh directory into the container, so there is no need to copy keys around provided you use bash expansion for the user home directory. Cloudera-Deploy and the Runner . If you have not already done so, you should follow the instructions to setup your OS dependencies and get a copy of Cloudera-Deploy. New users are encouraged to use the https://github.com/cloudera-labs/cloudera-deploy/blob/main/readme.adoc[Quickstart], while Power Users who want to immediately understand the inner workings may wish to follow selected parts of the «developers.adoc#_detailed_setup_for_developers,Developers» guide. Regardless of which method you select, complete the setup process until you are in the orange coloured interactive shell. Ensure that /runner/project within the shell has mounted your Projects directory that contains the Definition working directory you created in step one. IMPORTANT: At this point you should ensure that the directory listing in /runner/project within the Runner matches your projects directory on your local machine, we earlier gave the example of ~/Projects for this. When using the Ansible Runner, you should be aware that most changes to the filesystem within the container are not persistent - that is, when you kill the container, most changes will be lost. When we are working on files that we want to persist, such as cluster templates, we can keep these on the local filesystem outside of the container, and mount the local filesystem directory inside the container, which is the purpose of the /runner/project mountpoint. This means that we can read and write to those files both inside and outside of the container and you can safely terminate your container without losing them. Apart from the working dir, the container will mount specific directories out of your local user profile such as ~/.ssh, ~/.config, ~/.aws etc. into the container to pass-through credentials and other requirements. You should assume that any changes outside of these profile directories and /runner/project are not persisted. NOTE: If we do not specify which directory to mount, the quickstart.sh script will mount the parent directory that the current session is in. i.e. if you run quickstart.sh from the path ~/Projects/cloudera-deploy, then ~/Projects in the host filesystem will be mounted to /runner/projects in the Container. NOTE: If you did not stop the container previously, quickstart.sh will ignore any arguments passed and simply give you a new terminal session in the existing container. Therefore, if you want to change the mounted directory you must stop and restart the container with the new path. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_cdp/#process-overview",
    "relUrl": "/docs/content/ansible_cdp/#process-overview"
  },"10": {
    "doc": "CDP Private Cloud",
    "title": "Executing Playbooks",
    "content": "To run the playbook, we use the ansible-playbook command. The entry point to the playbook is in main.yml in cloudera-deploy. With our mount point in /runner/project, the full path is /runner/project/cloudera-deploy/main.yml. This is passed as the first argument to ansible-playbook. We also need to provide some additional arguments. We do this with the -e flag. The first argument to pass is the definition_path. This is the path to the directory that contains our definition, inventory, etc. NOTE: It is usually a good idea to use Cloudera-Deploy to verify your infrastructure before committing to the full deployment, by using the same main playbook and adding the ‘verify’ tag, as below. This is particularly handy for real-world deployments for a quick sanity check. An example command is shown below. Notice that we do not need to specifically provide an inventory list. The playbook will look in the definition_path for an inventory file, which is included in the cloudera-deploy-definitions examples. Of course, you can provide an inventory file using the -i if you want. NOTE: Commands like ansible-playbook should be run from the /runner directory in the Ansible-Runner to pick up the defaults and other useful configurations . ansible-playbook /runner/project/cloudera-deploy/main.yml \\ -e \"definition_path=/runner/project/my-definition/\" \\ -t verify -v . Following this will be a lot of output into the terminal which tracks the stages of our deployment. ----- PLAY [Init Cloudera-Deploy Run] ****************************************************************************************************************************************************************************************************************************** TASK [Gathering Facts] *************************************************************************************************************************************************************************************************************************************** Thursday 13 May 2021 18:54:13 +0000 (0:00:00.014) 0:00:00.014 ********** ok: [localhost] ----- . To run the actual full deployment against your inventory, the most common tag to use is full_cluster. The complete listing of all tags can be found by reviewing the Plays in https://github.com/cloudera-labs/cloudera-deploy/blob/main/cluster.yml[cluster.yml] in cloudera-deploy.The command would look something like this, with verbosity at level 2 . ansible-playbook /runner/project/cloudera-deploy/main.yml \\ -e \"definition_path=/runner/project/my-definition/\" \\ -t full_cluster -vv . Expect the deployment to take 30 to 90 minutes or longer, assuming you encounter no errors. If you do run into issues, most runs are idempotent so you can re-run with increased verbosity of the terminal output by adding the ‘-v’ flag to the ansible-playbook command. You can scale the verbosity by adding more v’s up to ‘-vvvvv’ for maximum verbosity. There is nothing you need to do until the playbook completes, however it can be useful to have a scroll through the output and get a feel for what it is doing. Eventually, you will get to some output that looks like the following. This indicates that Cloudera Manager is being installed, and then a check runs to wait for the server to start responding. When you get past this step, you’ll be able to access the CM UI in your browser. It will be installed on the host that was under the cloudera_manager title in your inventory, and on port 7180 for HTTP and 7183 for HTTPS. The username is typically ‘admin’, and the password will be the admin_password you set in your Profile or Definition. ----- TASK [cloudera.cluster.server : Install Cloudera Manager Server] ********************************************************************************************************************************************************************************************* Thursday 13 May 2021 19:41:50 +0000 (0:00:06.555) 0:47:36.812 ********** …. RUNNING HANDLER [cloudera.cluster.common : wait cloudera-scm-server] ***************************************************************************************************************************************************************************************** Thursday 13 May 2021 19:42:45 +0000 (0:00:09.338) 0:48:31.682 ********** ----- . The next important step to watch out for comes right at the end of the playbook. This is the Import Cluster Template step. In this step, the playbook is using the CM API to insert our cluster template, which allows CM to handle the complex logic of deploying the software, pushing out configurations and completing initializations and first runs. During this step, you will not see much useful output in the terminal. Instead, you should go inside the CM web UI and go to the ‘Running Commands’ page, where you will be able to drill down into the ‘Import Cluster Template’ command and watch the individual steps that CM performs. This is the best place to debug any errors that you might encounter during the Import Cluster Template step. ----- TASK [cloudera.cluster.cluster : Import cluster template] ****************************************************** ----- . NOTE: Deploying parcels can take some time if downloading directly from Cloudera Repos over slow or long-distance connections. Consider using the local repo options if doing multiple builds. After the Template is imported, the First Run is completed, and then a cluster Restart command will run. In the terminal session, our playbook has now completed and we will see the results at the end of the output. We should see a success message and a quick recap of the steps it took. ----- TASK [Deployment results] *************************************************************************************************************** Thursday 13 May 2021 20:59:45 +0000 (0:00:00.287) 2:05:31.793 ********** ok: [localhost] =&gt; { \"msg\": \"Success!\" } PLAY RECAP *************************************************************************************************************** ccycloud-1.cddemo.root.hwx.site : ok=162 changed=49 unreachable=0 failed=0 skipped=151 rescued=0 ignored=0 ccycloud-2.cddemo.root.hwx.site : ok=71 changed=23 unreachable=0 failed=0 skipped=65 rescued=0 ignored=0 ccycloud-3.cddemo.root.hwx.site : ok=71 changed=23 unreachable=0 failed=0 skipped=65 rescued=0 ignored=0 ccycloud-4.cddemo.root.hwx.site : ok=71 changed=23 unreachable=0 failed=0 skipped=65 rescued=0 ignored=0 localhost : ok=173 changed=11 unreachable=0 failed=0 skipped=149 rescued=0 ignored=1 Thursday 13 May 2021 20:59:45 +0000 (0:00:00.064) 2:05:31.857 ********** =============================================================================== cloudera.cluster.cluster : Import cluster template --------------------------------- 4132.25s cloudera.cluster.daemons : Install Cloudera Manager daemons package ---------------- 1415.92s cloudera.cluster.user_accounts : Create local user accounts ------------------------ 294.11s cloudera.cluster.user_accounts : Set home directory permissions -------------------- 254.82s cloudera.cluster.common : wait cloudera-scm-server --------------------------------- 99.33s cloudera.cluster.agent : Install Cloudera Manager agent packages ------------------- 64.32s cloudera.cluster.os : Populate service facts --------------------------------------- 60.19s cloudera.cluster.jdk : Install JDK ------------------------------------------------- 50.16s cloudera.cluster.krb5_server : Install KRB5 server --------------------------------- 39.23s geerlingguy.postgresql : Ensure PostgreSQL packages are installed. ----------------- 38.81s cloudera.cluster.cluster : Restart Cloudera Management Service --------------------- 35.66s cloudera.cluster.mgmt : Start Cloudera Management Service -------------------------- 34.83s cloudera.cluster.krb5_client : Install KRB5 client libraries ----------------------- 34.25s cloudera.cluster.kerberos : Import KDC admin credentials --------------------------- 25.34s Gather facts from connected inventory ---------------------------------------------- 20.92s cloudera.cluster.krb5_server : Start Kerberos KDC ---------------------------------- 19.53s cloudera.cluster.deployment/repometa : Download parcel manifest information -------- 19.00s cloudera.cluster.os : Install rngd ------------------------------------------------- 18.34s cloudera.cluster.rdbms : Copy SQL to change template to UTF-8 ---------------------- 16.64s Gathering Facts -------------------------------------------------------------------- 16.31s ----- . From this, we can see that the build took 2:05:31.793 (2 hours 5 minutes) in total, around 1 hour of this was the Import Cluster Template which includes the parcel downloads. Pre-downloading and hosting a cluster-local parcel repository can speed this up dramatically. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_cdp/#executing-playbooks",
    "relUrl": "/docs/content/ansible_cdp/#executing-playbooks"
  },"11": {
    "doc": "CDP Private Cloud",
    "title": "CDP Private Cloud",
    "content": ". | CDP Private Cloud Deployments | Ansible Controller options | Inventories | Process Overview . | Prepare a Definition Path | Infrastructure and Inventory | Cloudera-Deploy and the Runner | . | Executing Playbooks | . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_cdp/",
    "relUrl": "/docs/content/ansible_cdp/"
  },"12": {
    "doc": "Contributing to these Docs",
    "title": "Contributing to these Docs",
    "content": " ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_contributions/",
    "relUrl": "/docs/content/ansible_contributions/"
  },"13": {
    "doc": "Goal-oriented Guides",
    "title": "Detailed Setup for Developers",
    "content": "This document is structured with suggested sequences of steps at the top, and the various processes you need underneath. Use the suggested sequences or create your own as appropriate. Many of the Processes in here have been come to over years of trying different approaches, in the main our philosophy has been to adopt the least-worst option, minimising user surprise in the least number of the simplest possible steps, even if that results in a more complex implementation for us in code where few users ever tread. We welcome all suggestions and discussions for improvement, but if you’re going to tell us we’re ‘using Docker Wrong’ then we’d greatly appreciate an explanation on how you’d improve it. Developers setup on OSX . The Developer onboarding is aimed at those who want to modify the automation tooling itself to change behaviors. The following chapters of this documentation provide the necessary process: . | Prepare your OSX Deps | Follow the Main Setup | (Optional) Setup Commit Signing for Contributions | (Optional) Get setup to Develop the Collections themselves | . Developer setup on Windows . Because Windows already needs to set up Linux compatibility to make containers work, we just going to leverage that across the board for simplicity. IMPORTANT: You must use WSL2 or you are going to get some really weird errors. We assume you have a recent version of 64bit Windows 10 with Administrator access and virtualization enabled, if you are restricted by your org from doing this then they should have a replacement process as this is pretty standard stuff for developers. | Install WSL2 | Handle Line Endings on Windows | Follow the Main Setup | (Optional) Setup Commit Signing for Contributions | (Optional) Get setup to Develop the Collections themselves | . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_developers/#detailed-setup-for-developers",
    "relUrl": "/docs/content/ansible_developers/#detailed-setup-for-developers"
  },"14": {
    "doc": "Goal-oriented Guides",
    "title": "Process Guides",
    "content": "Main Setup Process . This Setup Process is simply the Quickstart from the Cloudera-Deploy Readme with more steps and explanations. Prerequisites . NOTE: It is easiest to test your credentials within the Runner, as the dependencies are already installed for you . | Cloud Credentials (CDP Public) and/or Host Infrastructure (CDP Private) . | If you are deploying to a public cloud, you will need an active credential for that provider with rights to deploy resources for the Deployment you specify. | Unless otherwise instructed your default credential will be used for the cloud provider. | You can check this with aws sts get-caller-identity and comparing outputs to the AWS UI, if using AWS | For Azure, consider az account list, or if you need to refresh your credentials, az login. | For GCP, check that your Service Account credential is being picked up in the Cloudera Deploy Profile, and then test with gcloud auth list | . | CDP Credentials (CDP Public) . | If you are deploying CDP Public Cloud, you will need an active credential within a CDP Tenant. | Unless otherwise instructed, your default credential and tenant will be used. | You can check this with “cdp iam get-user” and comparing outputs to the CDP UI | . | Code Deps: . | Docker and Git installed locally. | or - Ability to install Python and CLI dependencies on the machine of your choice (slow option)(See Centos7 Script for an example) | . | . Get the Repo . | Open a terminal and switch to your preferred projects directory . cd ~/Projects . | Clone or update the repo . git clone https://github.com/cloudera-labs/cloudera-deploy.git &amp;&amp; cd cloudera-deploy &amp;&amp; chmod +x ./quickstart.sh . | . NOTE: If you already have the repo cloned, don’t forget to update it. Get the Runner . | Have Docker installed and running . | (Optional) Set your provider to download a reduced image and save disk space . | AWS . export provider=aws . | Azure . export provider=azure . | GCP . export provider=gcp . | Default is all of them, but it’s a ~2.3GB image download . | . | Run the quickstart script in the Repo you just cloned above . | Option A: Run the quickstart bare, it will mount the parent directory of the cloudera-deploy repo, which will be your code directory if you followed the directions above, e.g. ~/Projects | ./quickstart.sh . | Option B: Include an explicit path on your local machine, it will be mounted in the /runner/project folder in the container. If your code or Definitions are not in this path you may not be able to access them in the container. | Dont worry if you get it wrong, just stop the container and rerun quickstart and it will make a new one with the new mount. | ./quickstart.sh ~/code . | This will drop you into a new shell in the container with all dependencies resolved for working with CDP and Cloud Infrastructure, you’ll know it has worked because the shell prompt is Orange with cldr &lt;version&gt; #&gt; . | It will automatically load your local machine user profile so you have access to your credentials, ssh keys, etc. | If you run the quickstart script again, it will simply create another bash session on the container, providing useful parallelism | If you stop the container, the next time you run quickstart it will be updated and recreated, so any changes within the container filesystem and not persisted back to your Project directory or mounted user profile will be lost | As long as you run commands from within the /runner path, it will log your Ansible back to ~/.config/cloudera-deploy/log | . | If you already have a CDP Credential in your local user profile, you can test it with . cdp iam get-user . | It will use the default CDP credential, or you can use a different profile by setting the CDP_PROFILE environment variable, or setting cdp_profile in your cloudera-deploy Definition files. | You should compare the UUID of your user returned by this command in the terminal with the UUID of your user reported in the User Profile in the CDP UI so you are certain that you are deploying to the expected Tenant | . | Check you have a credential for your chosen Cloud Infrastructure Provider. The default is AWS, and again you can provide a specific profile or use your default. You can check it by running . aws sts get-caller-identity . | You should likewise compare the Account ID reported here with the Account ID in the AWS IAM UI to ensure you are targeting the expected Account. This is similar for other providers. | . | . Prepare your Profiles to run a Deployment . NOTE: that you should execute any Ansible commands from /runner in the Runner, as it has all the defaults set for you and it may fail to find dependencies otherwise. NOTE: If you have different settings for different deployments you can create additional profile files under the directory above to store the different configurations. To use an alternative cloudera-deploy profile, specify the -e profile=&lt;profile_name&gt; option when running the ansible-playbook command. | Edit the default user Profile to personalise the Password, Namespace, SSH Keys, etc. for your Deployments. Note that this file is created the first time you run the quickstart. vi ~/.config/cloudera-deploy/profiles/default . | You will need CDP Credentials, and Credentials for your target Infrastructure of choice. Fortunately the Runner has most of these dependencies available to you . | CDP with pre-issued Keys and optional profile | . cdp configure --profile default . | AWS with pre-issued Keys | . aws configure --profile default . AWS SSO requires awscliv2 which is not installed in the Runner by default . | Azure via interactive login | . az login . | Google Cloud via init gcloud init . | . | . Deployment Run Commands . Provided you have completed the prerequisites to set up a cloud provider credential, and CDP Public Cloud credential, and your Cloudera-Deploy Profile, the following command creates a default Sandbox with CDP Public &amp; Private Cloud in your default CDP Tenant and Cloud Infra Provider with no further interaction from the user: . ansible-playbook project/cloudera-deploy/main.yml -e \"definition_path=examples/sandbox\" -t run,default_cluster . NOTE: So that is three dependencies, and two commands, and you have a complete Hybrid Cloud Data Platform . The command is structured typically for an ansible-playbook. | If you have used quickstart.sh to mount a local project directory with your definitions.yml and application.yml into the runner container (as explained in the steps in Get the Runner above), then you won’t have your own main.yml close to hand. You can instead use the default main.yml in /opt/cloudera-deploy/, and reference your project dir at /runner/project: . ansible-playbook /opt/cloudera-deploy/main.yml -e \"definition_path=/runner/project/&lt;your definition path&gt;\" -t &lt;your tags&gt; . | Note that we pass in some ansible ‘extra variables’ using the -e flag. The only required variable points to the definition_path which contains the files that describe the deployment you want. You can provide many more extra vars, or even files directly on the command-line per usual Ansible options. | Note that we pass in Ansible Tags with -t, in this case the tags instruct Cloudera-Deploy to build CDP Public Cloud to the ‘Runtimes’ level, and also deploy a ‘default’ or basic CDP Base Cluster on EC2 machines in the same VPC. There are many other tags that may be used to control behavior, and are explained elsewhere. | . Here are additional commands which will come in handy: . | Teardown and delete everything related to this definition: . ansible-playbook project/cloudera-deploy/main.yml -e \"definition_path=examples/sandbox\" -t teardown . | . WARNING: This will teardown everything related to this definition and name_prefix, make sure that is actually what you want to be doing before running it. | Just deploy a CDP Public Datalake: . ansible-playbook project/cloudera-deploy/main.yml -e \"definition_path=examples/sandbox\" -t plat . | . NOTE: This uses the same definition, but then uses a different Ansible Tag to only deploy part of it, more explanation of the Definitions and Tags will follow elsewhere in the Architecture Docs. | Just deploy CDP Private Cluster Trial on Public Cloud Infra: . ansible-playbook project/cloudera-deploy/main.yml -e \"definition_path=examples/sandbox\" -t infra,default_cluster . | . NOTE: This leverages the dynamic inventory feature to make a simple cluster on EC2 instances on AWS without the user needing to learn how first, and is very handy for trials and platform testing . Refresh your Cloudera-Deploy local repo . If you have previously used Cloudera-Deploy but haven’t touched it in a while, here is a guide to refreshing your setup and getting results quickly . cd cloudera-deploy git fetch --all git pull docker stop $(docker ps -qa) ./quickstart.sh cdp iam get-user aws iam get-user . Manual Ansible Controller setup on Centos7 . We provide an example script for initialising an Ansible Controller on a Centos7 box here . Install Homebrew and Git on OSX . Install XCode command line tools . xcode-select --install . Install Homebrew . /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" . Install git (through Homebrew) . brew install git . If you are going to use AWS SSO, you may also with to install awscliv2 on your local machine . brew install awscli@2 . Setup Docker . Guide for Windows Follow the instructions provided by https://docs.docker.com/docker-for-windows/install/[Docker] . NOTE: You want to be using WSL2, make sure Docker can see the underlying Ubuntu (or similar) for execution. You are advised to stick to the guide here, as we have found Windows throws some intractable filesystem and networking errors with creative linux-on-Windows setups for Docker and SSH. Guide for OSX Follow the instructions provided by https://docs.docker.com/docker-for-mac/install/[Docker] . Install Windows Subsystem for Linux (WSL2) . There’s a lot of guides on how to do this, here’s the summary: . | Enable Developer mode (on older versions of Win10) . | Windows Settings &gt; Update &amp; Security | Select the Developer Mode radio button | You may not need to do this, don’t worry if it’s not there and the rest of the process works, as the latest releases don’t require you to do this to install Linux | If you did have to enable it, you may have to reboot (yay Windows) | . | Enable Windows Subsystem for Linux v2 . | Control Panel &gt; Programs &amp; Features &gt; Turn Windows Features on and off | Tick the box for ‘Windows Subsystem for Linux’ | Make sure you either setup WSL2 from the start, or do the upgrade process. WSL1 has some strange behaviors | You’ll probably have to reboot. Yay Windows! | . | Install Ubuntu 18 (other distros untested, including Ubuntu 20) . | Try to do it from the Microsoft Store ** Open the store (search store in launch bar) ** Search for Linux in the store ** Select and install Ubuntu | If you can’t do it from the store, try this in the cmd.exe prompt lxrun /install | It’ll ask you to set a username and password, keep it short and simple, doesn’t have to match your actual Windows user | . | You may have to set this as the default bash environment as Docker for Windows likes to steal it away after updates . | List the currently installed distros by opening cmd.exe | wsl --list --all | Set ubuntu as default if it is not | wsl -s Ubuntu | . | . Handle line-endings on Windows . Windows defaults to a different line ending standard than Linux/OSX. NOTE: You only need to follow this step if you plan on editing the code on Windows. The Cloudera Labs repos standardise on linux line endings, the easiest way to handle this on Windows involves basically two steps. | Set git to always not use Windows line endings, so it doesn’t rewrite your files when you checkout . | git config --global core.autocrlf false | Set your IDE to use Linux line endings, if not for everything then at least the Cloudera Labs projects | In Pycharm, this is in File &gt; Settings &gt; Editor &gt; Code Style &gt; General &gt; Line Separator: set to Unix and macOS (\\n) | . | . Setup GPG commit signing . NOTE: You can skip this step if you are not planning on contributing code . DCO: . Cloudera-Labs uses the Developer Certificate of Origin (DCO) approach to open source contribution, by signing your commits you are attesting that you are allowed to submit them by whoever owns your work (you, or your employer). We also require commit signing to validate the supply chain on our code. Background: . There is a good explanation here, it also covers setup for machines with other OS. This subguide assumes that you want to set up your development machine so that it automatically signs all your commits without bothering you too much. If you are just checking out the code to inspect it and not for contributing back to the community, then you can skip this step. You may want to modify this process to ask you for your passphrase or manually sign commits each time at your preference. References: . | https://nifi.apache.org/gpg.html | https://stackoverflow.com/a/46884134 | https://superuser.com/a/954536 | https://withblue.ink/2020/05/17/how-and-why-to-sign-git-commits.html | . Testing: . OSX Catalina 10.15.7 on MacBook Pro 2019 . Process: . | Update or install Homebrew . | Install dependencies. gpg-suite is not strictly necessary, but it makes it easier to integrate signing with IDEs like IntelliJ as it helps you manage the passphrase in your OSX keychain. Pinentry-mac makes it easy to sign commits within your IDE, like Pycharm, without having to always commit via terminal . brew install gpg2 pinentry-mac &amp;&amp; brew install --cask gpg-suite . | Create a directory for gnupg to store details . mkdir ~/.gnupg . | Put the following in ~/.gnupg/gpg-agent.conf . default-cache-ttl 600 max-cache-ttl 7200 default-cache-ttl-ssh 600 max-cache-ttl-ssh 7200 pinentry-program /usr/local/bin/pinentry-mac . | Enable it in your user profile such as ~/.bash_profile or ~/.zprofile . export GPG_TTY=$(tty) gpgconf --launch gpg-agent . | Set correct permissions on your gnupg user directory . chown -R $(whoami) ~/.gnupg/ find ~/.gnupg -type f -exec chmod 600 {} \\; find ~/.gnupg -type d -exec chmod 700 {} \\; . | Generate yourself a key . gpg --full-gen-key . | Key type 4 | Keysize 4096 | Expiration 1y or 2y or whatever | Your real name, or github username | Your real email address, or the one github recognises as yours in your Settings | A Passphrase - don’t forget it, and make sure it is strong | . | Verify your key is created and stored . gpg2 --list-secret-keys --keyid-format SHORT . Copy your key ID, it’ll look something like rsa4096/674CB45A You want the second bit, 674CB45A . | Test your key can be used . echo \"hello world\" | gpg2 --clearsign . You’ll have to enter your passphrase to sign it, then it’ll print the encrypted message . | You may want to add multiple email addresses to the key signing, such as your open source email and/or your employer email and/or your personal email . | Open your key for editing | . gpg2 --edit-key &lt;your ID here&gt; . | Then use the adduid command adduid . | Enter the identity Name and Email as before . | Then update the trust for the new identity uid 2 trust . | . You probably want trust 5 . | Save to exit save . | . | Configure Github to recognise your signed commits . | Set your git email for making commits | . git config --global user.email &lt;your@email.com&gt; . This email must be one of those in your GPG key set earlier . | Export your public key for uploading to Github | . gpg2 --armor --export &lt;your ID here&gt; . | Copy everything including the following lines into your paste buffer | . -----BEGIN PGP PUBLIC KEY BLOCK----- ... -----END PGP PUBLIC KEY BLOCK----- . | Open github and go to your key settings https://github.com/settings/keys . | Add new GPG key, paste in your PGP block from your buffer . | Configure git to autosign your commits . git config --global gpg.program gpg git config --global user.signingkey &lt;your ID here&gt; git config --global commit.gpgSign true git config --global tag.gpgSign true . | Put the following in ~/.gnupg/gpg.conf . # Enable notty for IDE signing no-tty # Enable gpg to use the gpg-agent use-agent . | Configure IntelliJ to sign your commits . | This should be as simple as restarting your IDE once this process is complete | Then when you make a new commit you can tick the box to Sign-off commit in the dialog box | . | Configure vsCode to sign commits . | Find the following flag in the config and enable it git.enableCommitSigning | . | . Using the Ansible Runner Independent of Cloudera-Deploy . In order to minimise time spent on dependency management and troubleshooting issues arising from users on different systems, we provide a standardised container image. The image is prepared in such a way that you can use it as a shell, a python environment, a container on Kubernetes, within other CICD Frameworks, Ansible Tower, or simply as an ansible-runner. Testing: . * OSX Catalina 10.15.7 on MacBook Pro 2019 * Windows 10.0.19042 on Intel gaming rig (Tasteful RGB Edition) . Manual Process: . | To run this process on Windows you are expected to be within your WSL2 subsystem . | Clone the Cloudera Labs Ansible Runner implementation repo into your preferred local Projects directory . git clone https://github.com/cloudera-labs/cldr-runner.git &amp;&amp; cd cldr-runner . | Linux only: Mark the run_project.sh script and build.sh script as executable . chmod +x ./run_project.sh chmod +x ./build.sh . | Ensure Docker is running on your host machine . | Copy the absolute path to the root of your code projects directory that contains the projects you want to execute within the container environment, e.g. /Users/dchaffelson/Projects . | Launch the runner targeting the project you want to execute by passing the absolute path as the argument to the run_project.sh script, e.g./run_project.sh /Users/dchaffelson/Projects . | The script will build the container image from the latest release bits, this will take a few minutes the first time, the resulting image will be ~2GB . | You will then be dropped into a shell session in directory /runner in the container environment. Your Project will be mounted at /runner/project. You will have all the currently known dependencies for working with CDP pre-installed with conflicts resolved . | Note that the container must be stopped for a new project directory to be mounted to a new build, if there is already a container of the same name running you will just get a new shell session in it . | . | At this point you may wish to install additional dependencies to the container, particularly those which may be behind VPN or on your corporate VCS. ansible-galaxy install -r project/deps/ansible-deps.yml pip install -r project/deps/python-deps.txt . | . NOTE: By default, the container is recreated if stopped, but it will not stop if you close your shell session as it is held open by a background tty. Try not to kill that. Getting Started with Developing Collections . NOTE: You can skip this step if you only want to use the Collections to create your own playbooks. + This step is setting up the to Develop the Collections themselves. This will guide you through setting up a directory structure convenient for developing and executing the Collections within the Runner, or other execution environments. You only need to do this if you want to contribute directly to the Collections or Python clients underlying the interactions with Cloudera products - you do not need to go through this setup process if you simply wish to use cloudera-deploy with your own YAML Definitions, as the Collections and Clients should not need to be modified in those cases and are already pre-installed in the Runner. Why do it this way: . Ansible expects to find collections within a path “collections/ansible_collections” on a series of predefined or default paths within your environment. By default, the Runner has this Path variable prepopulated in a helpful fashion to the pre-installed Collections, this process guides you through modifying that to point at your own versions which you have to maintain yourself. For development purposes, creating this path in your favourite coding Projects directory, and then checking out the collections under it and renaming them to match the expected namespace may seem slightly arcane but it is the lowest-friction method for ongoing development we have found over many years of doing this. Process: . | Make the directory tree Ansible expects in the same parent code directory that cloudera-deploy is in, e.g. cd cloudera-deploy &amp;&amp; mkdir -p ../ansible_dev/collections/ansible_collections/cloudera . | cloudera is the base namespace of our collection | Your Projects directory should also have your Ansible Playbooks and other codebase in it, so that you can mount the root of it to the Runner and have access to all your codebase, e.g. ~/Projects/cloudera-deploy should be where Cloudera-Deploy is located | . | Fork each of the sub-collections and cdpy into your personal github, and replace with your actual github account below . | Checkout each of the sub-collections into this folder, e.g.: . cd ~/Projects git clone -b devel https://github.com/&lt;myAccount&gt;/cdpy.git cdpy cd ansible_dev/collections/ansible_collections/cloudera git clone -b devel https://github.com/&lt;myAccount&gt;/cloudera.exe.git exe git clone -b devel https://github.com/&lt;myAccount&gt;/cloudera.cloud.git cloud git clone -b devel https://github.com/&lt;myAccount&gt;/cloudera.cluster.git cluster . NOTE: The cloned directories above must be named “exe”, “cloud” and “cluster”, respectively. Ensure you specify the directory name as the last parameter in the command line, as shown above. Each of the subcollections should be on the ‘devel’ branch so you can PR them back them with your changes . | Your Code Project directory should now look something like this: . /Projects/ansible_dev/collections/ansible_collections/cloudera /Projects/ansible_dev/collections/ansible_collections/cloudera/exe /Projects/ansible_dev/collections/ansible_collections/cloudera/cloud /Projects/ansible_dev/collections/ansible_collections/cloudera/cluster /Projects/cdpy /Projects/cloudera-deploy . | Before you invoke quickstart.sh, set the environment variable below to tell Ansible where to find your code inside your execution environment once it is mounted in the Container at /runner/project: export CLDR_COLLECTION_PATH=\"ansible_dev/collections\" export CLDR_PYTHON_PATH=/runner/project/cdpy/src . NOTE: You might want to set this in your bash or zsh profile on your local machine so it is persistent . | Then, when you run quickstart.sh in cloudera-deploy, it will pick up this extra Collection location and add cdpy to the PYTHONPATH, and use these instead of the release versions basked into the Container . | You can confirm this is working by running this inside the Runner ansible-galaxy collection list . It should look something like: . ---- # /runner/project/ansible_dev/collections/ansible_collections Collection Version ---------------- ------- cloudera.cloud 0.1.0 cloudera.cluster 2.0.0 cloudera.exe 0.0.1 Cloudera.runtime 0.0.1 # /home/runner/.ansible/collections/ansible_collections Collection Version -------------------- ------- amazon.aws 1.4.0 ansible.netcommon 1.5.0 ansible.posix 1.1.1 azure.azcollection 1.4.0 community.aws 1.4.0 community.crypto 1.4.0 community.general 2.1.1 community.mysql 1.2.0 community.postgresql 1.1.1 google.cloud 1.0.2 netapp.azure 21.3.0 ---- . If you see duplication of collections because you are using the runner AND mounting your own versions, you probably have not activated the CLDR_COLLECTION_PATH variable correctly, and thus quickstart.sh is not picking it up. As another test, you should also be able to invoke python inside the container and use cdpy . from cdpy.cdpy import Cdpy c = Cdpy() c.iam.get_user() . To test that cdpy is present and you can access your account as on cmdline . You may now edit the collections or cdpy and have the changes immediately available for use within the Runner, which is an awful lot easier than having to compile and crossload them after every change. | . Install Dependencies without using Runner . We prefer that you use the Runner, because it sets many defaults to avoid common issues and thus save you and us a lot of issue reproduction time. However, we understand that there are many situations where it may not be appropriate, such as air-gapped environments, or when you want to run the install locally on the hardware and not have a separate ansible controller. | Create a new virtualenv, or activate an existing one, we do not recommend installing dependencies in system python on most OS. | Install dependencies for your hosting infrastructure version following the pathway laid out in the Dockerfile in ansible-runner Install any additional dependencies you may have . | . NOTE: THe Dockerfile resolves combined dependencies for all our Hybrid Cloud Deployments, you probably only need a subset for your environment. Developing within the Runner . While we recommend using the Runner as your execution environment when doing development, actually developing directly against the Runner using something like Visual Studio may not be a great idea due to the file system latency commonly encountered. Generally when the maintainers work with this system we are editing the files directly on our host system using our IDE, and those files are RW mounted into the container for execution via the /runner/project mechanism, which then does not noticeably incur any performance degradation. Working with AWS SSO . Why: . Traditionally AWS users would use static keys, but more recently using temporary credentials via SSO is more commonplace. The upside of AWS SSO is better credential management, the downside being additional complexity, reliance on the fairly awful AWSCLIv2, and a lack of OOTB automation integration. Setup AWS SSO: . | Install awscli v2 on your local machine . brew install awscli . | Check you have version 2 (currently 2.5.4) . aws --version . | Login to AWS SSO via Okta (or however), examine the list of accounts and select the one you want to set up, copy the name to use as your AWS Profile name . | Setup and login to AWS SSO for your selected account . aws configure sso --profile &lt;selected name here&gt; . | Enter your Start URL, e.g. https://&lt;app-name&gt;.awsapps.com/start | SSO Region, e.g. us-east-1 | It’ll launch your browser (which is why we do it on your local machine) | Complete the login process to authenticate the session | Select which AWS account you wish to set up | Set your default region for this profile, e.g. us-east-1 | Set your default output for this profile, e.g. json | . | Now whenever you run a deployment you must set the profile name to match the one you have setup, e.g. in definition.yml . aws_profile: my-aws-profile . | . NOTE: You will likely have to re-login to SSO before each run . Working with named credential profiles . For CDP, AWS, and other credential-based access controls it is common to have multiple accounts that may be used from time to time. As such, it is useful to be able to easily, safely, and precisely switch between these accounts. This is typically achieved through use of multiple named profiles, a good introductory guide to this is here for AWS. To set up a named profile for CDP, follow this guide to create an API access key, and then this guide to configure your local credentials by simply adding the --profile &lt;profile name&gt; flag to the commands . Credentials are typically stored in a dot-prefix directory in the user profile on Linux, and in the user home directory on Windows, e.g. ~/.aws/credentials, ~/.cdp/credentials etc. They may be directly edited in these files if you prefer instead of using the configure CLI commands. By default, Cloudera-Deploy will use the default profiles for each account touched, you can set the Definition yaml keys cdp_profile: &lt;profile name&gt; and/or aws_profile: &lt;profile name&gt; to use a specific named profile instead. Things to know: . | There should always be a default credential which points to a ‘safe’ account for when you deliberately or accidentally run commands without specifying a profile. This is generally a dev account, or even an expired credential, to ensure you don’t accidentally delete prod. | It is typical with CDP Public Cloud to pair a CDP Tenant with specific Cloud Infrastructure accounts to avoid confusion, therefore if you have a dev and prod CDP tenant with profiles of the same name, we recommend the paired cloud infrastructure accounts are also named dev and prod. | . Using a Jumpbox . A common deployment scenario is to funnel all cluster access through a jump/bastion host. In this case, there are three possibilities: . | To run the Ansible Runner from the jump host itself | To deploy the dependencies within the boundary of the Jump Host | To run the Ansible Runner locally and tunnel connections through the jump host. | . In scenario 3, the following will be necessary to tunnel both SSH connection and HTTP calls through the jump host. | HTTP In the runner, edit /runner/env/envvars and add http_proxy=&lt;proxy&gt; where is the name:port of your http proxy (e.g. a SOCKs proxy running on localhost). | . Alternatively, edit quickstart.sh to pass this value through from your local machine if it is available. | SSH In your inventory file, under the [deployment:vars] group, add the following variable to set additional arguments on the SSH command used by Ansible. | . ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=\"ssh -W %h:%p -q &lt;jump host&gt;\"' . Optionally, in your SSH config file (e.g. ~/.ssh/config) you can configure an alias with predefined parameters for the jump host. This makes it easier to manage between different deployments and makes the argument string easier to read. Host myJump IdentityFile ~/.ssh/myKey StrictHostKeyChecking = no User myUser HostName jump.host.name . With this SSH config the proxy string would look like this: . ansible_ssh_common_args='-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=\"ssh -W %h:%p -q myJump\"' . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_developers/#process-guides",
    "relUrl": "/docs/content/ansible_developers/#process-guides"
  },"15": {
    "doc": "Goal-oriented Guides",
    "title": "CDP Private Cloud Deployments",
    "content": "CDP Base represents deployment of ‘Traditional’ clusters via Cloudera Manager, which you might think of historically as hadoop or big data clusters. In practice, there is an Ansible Collection called cloudera.cluster which is excellent at configuring Cloudera Manager across various versions of Linux regardless of whether that OS is running in Containers, Virtual Machines, Cloud Infra like EC2, or old school baremetal nodes. Cloudera.cluster is wrapped within Cloudera-Deploy, so again in practice you are most likely to simply use Cloudera-Deploy as the entrypoint to drive cloudera.cluster to create the cluster you want via the Definition mechanism explained in these docs. The differences are that Cloudera.cluster has been built over many generations to support Cloudera Manager versions 5 to 7, and supports many legacy options and behaviors for user convenience. The result of this is that, while simple deployments are as simple as the more modern CDP Public Cloud, complex deployments will require a greater familiarity with Ansible and Cloudera Manager templates. Ansible Controller options . We still strongly recommend that you use the cloudera-runner Docker Container shipped with Cloudera-Deploy as your Ansible controller for CDP Base deployments, because the consistency and logging is going to save you time and effort. However, it is not uncommon to use the target Cloudera Manager node as the Controller. It is also not uncommon to need to deploy to air-gapped environments or environments where Docker is not permitted. As such, there is a script for deploying the dependencies for Cloudera-Deploy on centos7 as an example in the repo for Power Users in these scenarios. Note that the script covers dependencies for all Cloudera Products on all current Clouds, so you may wish to exclude parts irrelevant to your scenario. NOTE: It is worth noting that the admin_password field from your Profile, as outlined in the Quickstart, will be used as the Cloudera Manager admin password during cloudera.cluster deployments. Inventories . An inventory file can be supplied using the traditional -i flag at runtime or by including either inventory_static.ini or inventory_template.ini in the definitions path. inventory_static.ini . An Ansible inventory covering the hosts in-scope for the deployment. An inventory with this name will be picked up automatically if present without using the -i to specify the inventory file at runtime. Common inventory group names are: . | cloudera_manager | cluster_master_nodes | cluster_worker_nodes | cluster_edge_nodes | . Each host in these groups should be assigned one of the host templates that is defined in cluster.yml. These can be applied per host by setting host_template=TemplateName after the hostname, or per group by setting host_template=TemplateName in the group vars. See here for more information on Ansible inventory vars. Nodes that should be added to the CDP Cluster as defined in the cluster.yml must be added as children to the cluster group. For example, if all data nodes are present under the cluster_worker_nodes group, then this group should be listed under cluster:children. See here for an example of setting group children. All nodes/groups should be added as children of the deployment group [deployment:children]. This includes the cluster group, as well as all non-CDP Cluster groups, for example, Cloudera Manager and supporting infrastructure nodes, perhaps running HAProxy or an RDBMS, that are deployed with the cluster but are not running CDP Roles. This allows for deployment-wide variables to be set, such as the common ssh key that can be used to access all nodes. WARNING: Do not use the [all:vars] group if you are using an independent Ansible Controller as Plays will also be applied to it via the localhost mechanic. This includes from your Laptop or the Ansible-Runner supplied in Cloudera-Deploy. Process Overview . In this section we will walk through a typical deployment, commenting on options at various points. | Prepare a Definition Path | . You are advised to create a working directory for your definition in your preferred Projects directory. If you have checked out Cloudera-Deploy from Github following the Quickstart, then you could make a copy of the folder cloudera-deploy/examples/sandbox into the same base code Projects directory, and use it as a starting point. You might then have: . ~/Projects/cloudera-deploy # contains quickstart.sh, main.yml ~/Projects/my-definition # copied from ~/Projects/cloudera-deploy/examples/sandbox . | Infrastructure and Inventory You are advised to prepare your nodes in advance by whatever mechanism you prefer, and then include them in an Ansible Inventory matching the shape of the Clu ster you want to deploy. | . You may use Cloudera-Deploy to automatically provision EC2 instances based on the inventory template, but this is intended for test and training purposes. An example inventory template is included in the Cloudera-Deploy Sandbox Example. There is also a copy of the static_inventory.ini file in this directory. If you want to use a static inventory, you may use any of the typical Ansible inventory approaches such as -i on the cmdline, the Inventory path, or including a file inventory_static.ini in your Definition Path which Cloudera-Deploy will automatically pick up. You may wish to include additional variables in the Inventory, most typically the ssh user and private key. These should probably be in the [deployment:vars] section. [deployment:vars] ansible_ssh_private_key_file=~/.ssh/root_key ansible_user=root . NOTE: When you run quickstart.sh to launch the Runner Container, it will mount your local user ~/.ssh directory into the container, so there is no need to copy keys around provided you use bash expansion for the user home directory. | Cloudera-Deploy and the Runner | . If you have not already done so, you should follow the instructions to setup your OS dependencies and get a copy of Cloudera-Deploy. New users are encouraged to use the Quickstart, while Power Users who want to immediately understand the inner workings may wish to follow selected parts of the «developers.adoc#_detailed_setup_for_developers,Developers» guide. Regardless of which method you select, complete the setup process until you are in the orange coloured interactive shell. Ensure that /runner/project within the shell has mounted your Projects directory that contains the Definition working directory you created in step one. IMPORTANT: At this point you should ensure that the directory listing in /runner/project within the Runner matches your projects directory on your local machine, we earlier gave the example of ~/Projects for this. When using the Ansible Runner, you should be aware that most changes to the filesystem within the container are not persistent - that is, when you kill the container, most changes will be lost. When we are working on files that we want to persist, such as cluster templates, we can keep these on the local filesystem outside of the container, and mount the local filesystem directory inside the container, which is the purpose of the /runner/project mountpoint. This means that we can read and write to those files both inside and outside of the container and you can safely terminate your container without losing them. Apart from the working dir, the container will mount specific directories out of your local user profile such as ~/.ssh, ~/.config, ~/.aws etc. into the container to pass-through credentials and other requirements. You should assume that any changes outside of these profile directories and /runner/project are not persisted. NOTE: If we do not specify which directory to mount, the quickstart.sh script will mount the parent directory that the current session is in. i.e. if you run quickstart.sh from the path ~/Projects/cloudera-deploy, then ~/Projects in the host filesystem will be mounted to /runner/projects in the Container. NOTE: If you did not stop the container previously, quickstart.sh will ignore any arguments passed and simply give you a new terminal session in the existing container. Therefore, if you want to change the mounted directory you must stop and restart the container with the new path. Executing Playbooks . To run the playbook, we use the ansible-playbook command. The entry point to the playbook is in main.yml in cloudera-deploy. With our mount point in /runner/project, the full path is /runner/project/cloudera-deploy/main.yml. This is passed as the first argument to ansible-playbook. We also need to provide some additional arguments. We do this with the -e flag. The first argument to pass is the definition_path. This is the path to the directory that contains our definition, inventory, etc. NOTE: It is usually a good idea to use Cloudera-Deploy to verify your infrastructure before committing to the full deployment, by using the same main playbook and adding the \u001averify\u001a tag, as below. This is particularly handy for real-world deployments for a quick sanity check. An example command is shown below. Notice that we do not need to specifically provide an inventory list. The playbook will look in the definition_path for an inventory file, which is included in the cloudera-deploy-definitions examples. Of course, you can provide an inventory file using the -i if you want. NOTE: Commands like ansible-playbook should be run from the /runner directory in the Ansible-Runner to pick up the defaults and other useful configurations . ansible-playbook /runner/project/cloudera-deploy/main.yml \\ -e \"definition_path=/runner/project/my-definition/\" \\ -t verify -v . Following this will be a lot of output into the terminal which tracks the stages of our deployment. ----- PLAY [Init Cloudera-Deploy Run] ****************************************************************************************************************************** ************************************************************************************************ TASK [Gathering Facts] *************************************************************************************************************************************** ************************************************************************************************ Thursday 13 May 2021 18:54:13 +0000 (0:00:00.014) 0:00:00.014 ********** ok: [localhost] ----- . To run the actual full deployment against your inventory, the most common tag to use is full_cluster. The complete listing of all tags can be found by reviewing the Plays in https://github.com/cloudera-labs/cloudera-deploy/blob/main/cluster.yml[cluster.yml] in cloudera-deploy. The command would look something like this, with verbosity at level 2 . ansible-playbook /runner/project/cloudera-deploy/main.yml \\ -e \"definition_path=/runner/project/my-definition/\" \\ -t full_cluster -vv . Expect the deployment to take 30 to 90 minutes or longer, assuming you encounter no errors. If you do run into issues, most runs are idempotent so you can re-run with increased verbosity of the terminal output by adding the \u001a-v\u001a flag to the ansible-playbook command. You can scale the verbosity by adding more v\u001as up to \u001a-vvvvv\u001a for maximum verbosity. There is nothing you need to do until the playbook completes, however it can be useful to have a scroll through the output and get a feel for what it is doing. Eventually, you will get to some output that looks like the following. This indicates that Cloudera Manager is being installed, and then a check runs to wait for the server to start responding. When you get past this step, you\u001all be able to access the CM UI in your browser. It will be installed on the host that was under the cloudera_manager title in your inventory, and on port 7180 for HTTP and 7183 for HTTPS. The username is typically ‘admin’, and the password will be the admin_password you set in your Profile or Definition. ----- TASK [cloudera.cluster.server : Install Cloudera Manager Server] ********************************************************************************************* ************************************************************************************************ Thursday 13 May 2021 19:41:50 +0000 (0:00:06.555) 0:47:36.812 ********** \u001a. RUNNING HANDLER [cloudera.cluster.common : wait cloudera-scm-server] ***************************************************************************************** ************************************************************************************************ Thursday 13 May 2021 19:42:45 +0000 (0:00:09.338) 0:48:31.682 ********** ----- . The next important step to watch out for comes right at the end of the playbook. This is the Import Cluster Template step. In this step, the playbook is using the CM API to insert our cluster template, which allows CM to handle the complex logic of deploying the software, pushing out configurations and completing initializations and first runs. During this step, you will not see much useful output in the terminal. Instead, you should go inside the CM web UI and go to the \u001aRunning Commands\u001a page, where you will be able to drill down into the \u001aImport Cluster Template\u001a command and watch the individual steps that CM performs. This is the best place to debug any errors that you might encounter during the Import Cluster Template step. ----- TASK [cloudera.cluster.cluster : Import cluster template] ****************************************************** ----- . NOTE: Deploying parcels can take some time if downloading directly from Cloudera Repos over slow or long-distance connections. Consider using the local repo options if doing multiple builds. After the Template is imported, the First Run is completed, and then a cluster Restart command will run. In the terminal session, our playbook has now completed and we will see the results at the end of the output. We should see a success message and a quick recap of the steps it took. ----- TASK [Deployment results] *************************************************************************************************************** Thursday 13 May 2021 20:59:45 +0000 (0:00:00.287) 2:05:31.793 ********** ok: [localhost] =&gt; { \"msg\": \"Success!\" } PLAY RECAP *************************************************************************************************************** ccycloud-1.cddemo.root.hwx.site : ok=162 changed=49 unreachable=0 failed=0 skipped=151 rescued=0 ignored=0 ccycloud-2.cddemo.root.hwx.site : ok=71 changed=23 unreachable=0 failed=0 skipped=65 rescued=0 ignored=0 ccycloud-3.cddemo.root.hwx.site : ok=71 changed=23 unreachable=0 failed=0 skipped=65 rescued=0 ignored=0 ccycloud-4.cddemo.root.hwx.site : ok=71 changed=23 unreachable=0 failed=0 skipped=65 rescued=0 ignored=0 localhost : ok=173 changed=11 unreachable=0 failed=0 skipped=149 rescued=0 ignored=1 Thursday 13 May 2021 20:59:45 +0000 (0:00:00.064) 2:05:31.857 ********** =============================================================================== cloudera.cluster.cluster : Import cluster template --------------------------------- 4132.25s cloudera.cluster.daemons : Install Cloudera Manager daemons package ---------------- 1415.92s cloudera.cluster.user_accounts : Create local user accounts ------------------------ 294.11s cloudera.cluster.user_accounts : Set home directory permissions -------------------- 254.82s cloudera.cluster.common : wait cloudera-scm-server --------------------------------- 99.33s cloudera.cluster.agent : Install Cloudera Manager agent packages ------------------- 64.32s cloudera.cluster.os : Populate service facts --------------------------------------- 60.19s cloudera.cluster.jdk : Install JDK ------------------------------------------------- 50.16s cloudera.cluster.krb5_server : Install KRB5 server --------------------------------- 39.23s geerlingguy.postgresql : Ensure PostgreSQL packages are installed. ----------------- 38.81s cloudera.cluster.cluster : Restart Cloudera Management Service --------------------- 35.66s cloudera.cluster.mgmt : Start Cloudera Management Service -------------------------- 34.83s cloudera.cluster.krb5_client : Install KRB5 client libraries ----------------------- 34.25s cloudera.cluster.kerberos : Import KDC admin credentials --------------------------- 25.34s Gather facts from connected inventory ---------------------------------------------- 20.92s cloudera.cluster.krb5_server : Start Kerberos KDC ---------------------------------- 19.53s cloudera.cluster.deployment/repometa : Download parcel manifest information -------- 19.00s cloudera.cluster.os : Install rngd ------------------------------------------------- 18.34s cloudera.cluster.rdbms : Copy SQL to change template to UTF-8 ---------------------- 16.64s Gathering Facts -------------------------------------------------------------------- 16.31s ----- . From this, we can see that the build took 2:05:31.793 (2 hours 5 minutes) in total, around 1 hour of this was the Import Cluster Template which includes the parcel downloads. Pre-downloading and hosting a cluster-local parcel repository can speed this up dramatically. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_developers/#cdp-private-cloud-deployments",
    "relUrl": "/docs/content/ansible_developers/#cdp-private-cloud-deployments"
  },"16": {
    "doc": "Goal-oriented Guides",
    "title": "Goal-oriented Guides",
    "content": ". | Detailed Setup for Developers . | Developers setup on OSX | Developer setup on Windows | . | Process Guides . | Main Setup Process | Refresh your Cloudera-Deploy local repo | Manual Ansible Controller setup on Centos7 | Install Homebrew and Git on OSX | Setup Docker | Install Windows Subsystem for Linux (WSL2) | Handle line-endings on Windows | Setup GPG commit signing | Using the Ansible Runner Independent of Cloudera-Deploy | Getting Started with Developing Collections | Install Dependencies without using Runner | Developing within the Runner | Working with AWS SSO | Working with named credential profiles | Using a Jumpbox | . | CDP Private Cloud Deployments . | Ansible Controller options | Inventories | Process Overview | Executing Playbooks | . | . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_developers/",
    "relUrl": "/docs/content/ansible_developers/"
  },"17": {
    "doc": "Information-oriented References",
    "title": "Background",
    "content": "In this section of the documentation we will attempt to collate answers to commonly asked questions about the Why and How things are built the way they are. In general, we try to include comments directly in the code to explain why some particular thing may be done a particular way. Please raise an Issue if there is some step in the tasks which you want more explanation on, or if you want some workflow to have an explanation in these docs. Guiding Principles . | Good Defaults don’t produce Bad Surprises | Least-worst is usually better than best | The needs of the many generally outweigh the needs of the few | It should work out of the box | It should be secure by default | Provide examples and as much freedom as possible | Fix it upstream if you can | . History . This codebase is the 4th or 5th generation tooling for automating the Platform that, in the current incarnation, is called CDP or the Cloudera Data Platform. When Ambari and Cloudera Manager (amongst others) where early orchestrators-of-clusters, there were quickly deployers-of-orchestrators being published, such as Ambari-Bootstrap and many others. These early efforts were simple and effective, and typically in Bash or Perl. Later on, when management of deployment to IaaS was being developed in Director and Cloudbreak, Procedural management tooling like Saltstack, Puppet, Ansible and Chef were battling for adoption and dominance in deployment and configuration management. As cloud infrastructure APIs became more deterministic, and cloud standards more codified, Terraform rose to prominence with the Declarative IaC movement. Now you could combine declarative infrastructure with procedural deployments, and people started to appreciate the combination of Terraform and Ansible as powerful system tools. Around this time Cloudera first published our Ansible Playbooks for deploying Cloudera Clusters. While we still also use other tools within our systems, we settled on Ansible as the most broadly adoptable and adaptable tool to work with our Customers when integrating our Platform with their extremely varied Environments. With the rise of Container Orchestration like Kubernetes, yet another maturity level in systems management became apparent - a stack building up to declarative applications with self-healing and auto-scaling possibilities. Now maturing valuable APIs for determinism and observability became more critical, while still offering suitable integration for users of varying modernity and maturity stages at the edge of the zone of control. So, today, Cloudera offers many entrypoints into integrating with our Hybrid Data Platform, from deploying legacy versions of our traditional cluster management software, to deploying real-time data streaming analytics on managed kubernetes in various cloud-native providers. We continue to provide Ansible as our general purpose Procedural automation framework, as it can generally be adapted to most situations as a good starting point. We wrap in Terraform as an option for Infrastructure, as the widely recognised industry standard, and provide the usual collection of SDKs and API specifications underneath. Declarative vs Procedural . We have taken the approach of implementing a much more Declarative approach to Ansible in this framework. It is not particularly traditional to use Ansible in this way prior to ~v2.10, and you might say why not use a built-for-purpose Declarative tool like Terraform instead. The main answer to this could be summarised as follows: Ansible is better at discovering and modifying deployments, whereas Terraform prefers to own and modify them. The difference is important as, in designing these automations, we cannot know whether a given user has full control of the deployment over time. It is quite easy to get your Terraform state into a bad place if people can unexpectedly edit the deployment directly (as often happens in complex tiered applications with shared-responsibility models), whereas Ansible is comfortable with taking things as it finds them. Secondly, we cannot know or support how 3rd party APIs and software versions may change over time with respect to a users’ deployment. There are too many permutations of possible deployments to support, and so we again need a framework that is more flexible and may be adapted by any given user to their circumstances. Thirdly, we find that the broad set of APIs and systems that CDP is integrated with have better coverage in Ansible Modules and Collections than Terraform Providers, and when that coverage isn’t available, it’s relatively easier to fall back to a CLI or API request implemented in Ansible. That is not to say Terraform is somehow bad or inappropriate - actually we strongly recommend that, if you have good authorization controls over your Infrastructure, Terraform is the best way to manage it. Our point is more that, if you have this you probably already have good DevOps skills and can gracefully adopt this Framework, whereas Ansible is going to be lower friction for everyone earlier in their journey. All that being said however, the more of your deployment you can manage declaratively probably the easier your life will be - the art of the science here is knowing when to switch tools and why. NOTE: Ansible and Terraform - Better Together . Implicit with defaults vs explicit . This framework strives to have a clear and singular default value for as many of the variables in the platform as possible, and those default values should represent the balance between best practice secured deployments and working easily out-of-the-box for new users. The defaults should not produce results that surprise the User with Bad Things, like accidentally deleting data or secrets. All defaults should be able to be overridden without difficulty by users who know what they want. The purpose of this approach is primarily to enable new users to be productive with a minimum of front-loaded learning, through the means of deployments requiring a minimum of configuration, but allowing a maximum of explicit configurability as user skill and confidence grows. So, you can have a deployment from defaults with almost no fore-knowledge of how CDP works (like our Trial), but you can also use a 400 line declarative YAML file to describe your whole deployment (like our QE) and both options are perfectly valid and use exactly the same tooling . Multi-tier abstraction . In order to deliver on this more complex implementation, it is necessary to pay the cost of several layers of abstraction. Some of these are implemented directly here, but where possible we use upstream vendor implementations, e.g. cloud provider maintained Terraform Providers or Ansible Collections. | Base clients (CDPCLI, Azure CLI, AWS CLI, etc.) | Procedural clients (cdpy) | Declarative Modules (cloudera.cloud, amazon.community, etc.) | Sequential Tasklists (cloudera.exe, geerlingguy.postgresql) | Playbooks (cloudera-deploy) | Declarative Definitions (definitions) | . Generally we aim to push some complex task or piece of logic towards the best implementation point, for example, it is much easier to do a complex map or filter operation in Python within an Ansible Module than to express it within an Ansible Jinja template in a task within a Role. Most users will not need to bother themselves with any of this; they will craft Definitions, apply them with the Playbooks and satisfy their requirements. But for Developers, you will need to develop a sense of when something should be upstream in the Base Clients, put into cdpy or the Modules as reusable Python code, or be expressed in some Ansible Task within a particular sequence. Feel free to raise an Issue on the repositories for guidance in these cases. Ansible Collections and the Handling of Variables . Possibly the single most frustrating thing in Ansible is when you can’t figure out where a variable was defined, or why it isn’t the value you expect to be because someone has squashed it somewhere else. In this Framework, unless you can’t for reasons of backwards compatibility, all variable names should be prefixed. If the variable is local, and only not intended to be reused, then we follow the Python standard of a single or double underscore. This is particularly important in naming trivial variables such as those used in loops. They must be unique, and it is lazy not to do this, e.g. __bag_of_holding_item . If the variable is likely to be referenced elsewhere within that specific Role, assign the whole Role some short but obvious prefix followed by a double underscore to indicate ownership, and then uniquely name variables e.g. hg__wingardium_leviosaaaa . If the user defined variable is needed across multiple Roles in a Collection, then make a common Role in the Collection and have the other Roles pull the variables from the defaults part of this common Role. cloudera.exe makes extensive use of this, with the common defaults here, and the infrastructure Role importing them here. If the variable is discovered for a Role, such as checking whether a resource exists on a given cloud provider, then it should be discovered anew for each Role and variables from other Roles should not be assumed to exist. This allows the Roles to be used separately if desired, or skipped using --skip-tags for brevity of Runs. All significant variables in a Role that the end-user is likely to care about should be defined in defaults for that Role, using the prefixed names, and imported from common where necessary. In this way 99.9% of stupidly annoying variable issues may be avoided, and the rest can probably be blamed on Azure consistency errors. Full credit to Webster Mudge for this robust design. Cloud Infrastructure and the Naming of Objects . Naming things is the second-hardest problem in DevOps. In this Framework we rely extensively on procedurally naming objects and complying with the various and varying restrictions on uniqueness, length, character sets, immutability, and other weird requirements that emerge from the world of Hybrid Cloud Infrastructure. In practical terms, we try to stick to a known-good intersection of requirements that work. As such, we have settled on assigning default label strings to most conceptual components, and suffix for most classes of objects to be created. When combined with the name_prefix for that Deployment, and some other sources of uniqueness like the infra_type, we find we can generate meaningful names for almost anything while still allowing the user to modify as they see fit. You can find most of these in the cloudera.exe.common Role defaults. You can also statically assign names to almost anything, but then you are responsible for investigating the impact that might have. Generally shorter names are better, particularly when something might be used to construct a FQDN, which must generally be &lt;63 chars to be safe. Generally avoid punctuation, particularly at the start or end of a name, as we have found that different cloud providers and their subsystems will fail unexpectedly when they encounter a double underscore or double hyphen. In most cases a single hyphen or underscore that is not the first or last character are allowed. Starting a name with a number can also sometimes cause strange subsystem errors. Generally stick to basic UTF8 characters - while some systems will allow you to explore the exciting depths of UTF32, many others will fail terribly. Our guidance in these notes is there to help you avoid such difficult errors, please heed it. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_info/#background",
    "relUrl": "/docs/content/ansible_info/#background"
  },"18": {
    "doc": "Information-oriented References",
    "title": "Terminology",
    "content": "Some terms used herein have specific meaning, and are usually Capitalised in the document to indicate they are being used in that specific context. Playbooks Are referring to Ansible Playbooks, and are generally the main entrypoint into Runs . Run Some execution of a Playbook . NOTE: When using the Runner, Runs are automatically logged back to the user profile and use a collection of default settings known to be good in most situations . Controller The Controller, or Ansible Controller, refers to the machine where the ansible or ansible-playbook etc. commands are being executed, as distinct from the ‘Ansible Inventory’ which are the hosts that Ansible is connecting to. Runner Refers to cldr-runner, a common Execution Environment. This is built from a Dockerfile maintained by the Ansible community, to which the various dependencies for CDP and Hybrid Cloud Architectures are added. Definition A directory containing files expected by the Playbooks which describe the Deployment. The Definition directory also doubles as a working directory for artefacts produced in the Run. The files and details around composability will be explained later. Deployment Definitions Refers to one or more of these Definition directories that may be provided by default with Cloudera-Deploy, created by the user, or some other process . Tags Tags is an overloaded term, and may be referring to: . | ‘Ansible Tags’, which control what actions will be executed within the Run based on the Definition. At the simplest level this is things like ‘deploy’ or ‘teardown’ but can provide a great deal of control with sophisticated use | ‘Tags’ applied to cloud infrastructure, which is strongly recommended for all users | . Profile Profile is an overloaded term and may refer to any of the following depending on context: . | A profile specifically for Cloudera-Deploy which provides the lowest precedence of user defaults for things like Passwords. Usually found in ~/.config/cloudera-deploy/profiles on Linux machines | A profile used with some external API, such as AWS, Azure, GCP or CDP, which usually specifies things like credentials, endpoints, regions, etc. Usually found in ~/.aws or similar on Linux machines | Sometimes used to refer to your user home directory on your machine. When using the Runner, it mounts key User home directories such as your .ssh and .config folders in provide you access to those files with the various tools. Usually /home/&lt;user&gt; on Linux machines. | . Subsystems . Modern Platforms are layers upon layers of different bits of Software and Infrastructure working together. When we refer to Subsystems, we may mean a Cloud Provider in general, or some specific API, or an Apache Project. It is not necessarily Software provided or written by Cloudera, but we will attempt to give you good information about it. Workflows . There are many Processes within the Framework which touch on many levels of automation abstraction and may operate end-to-end over many files and subsystems. We refer to these as Workflows as a convenient label to indicate it’s a bit more complex than a simple Ansible Task list or Playbook, and Workflows have their own section in the Developer’s Guide. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_info/#terminology",
    "relUrl": "/docs/content/ansible_info/#terminology"
  },"19": {
    "doc": "Information-oriented References",
    "title": "Deployments Reference",
    "content": "A ‘Definition’ is a directory containing one or more files, and may contain additional assets required by the user. The intention is that a single directory may contain all the necessary components to describe a Deployment suitable for configuration-as-code practices using version control. This also means that pre-defined Definitions are easy to create, and we include several for new users. See explanations in the sections for CDP Public Cloud and CDP Private Cloud. Definition Structure . The definition_path . Cloudera-Deploy will recognise and compose together several files during Initialization in order to determine what should be done in a run. The files have defaults set in cloudera-deploy/roles/cloudera_deploy/defaults/main.yml and the various Collections, which may be overridden by the Definition or with extra vars at runtime. The Definition directory also acts as a working directory during the run, and is the location where Definition-specific artefacts, such as dynamic inventory files, will be written back to. This differs from persistent records for the Runner like Logs and Profiles which are kept in ~/.config/cloudera-deploy and thus written back to the Filesystem hosting the Runner. As such, the definition_path extra var is one of the few mandatory values required by the Playbook, and one of the first things checked for in a Run. It is expected that the user will have any persistent information from their profile, like ssh keys and cloud credentials and logs, mounted via bash expansion in ~/, and any Code and Projects mounted for persistence to /runner/project. Pretty much all other areas of the Runner are presumed to be ephemeral storage. Explaining the various Definition Files and Profiles . As said above, Cloudera-Deploy will read in and compose the facts for the run from a set of files found in the Definition path, then from an optional Profile file, and then from the Collections which hold defaults for the different kinds of deployments available. The various files and their override values are set during cloudera-deploy initialization. We pull these values into multiple different files to give the user control of composing their deployments if desired - you may wish to recombine different profiles, definitions, and cluster specs according to your needs, but you can also just glob it all into a single definition.yaml file in most cases if you want. The Profile is drawn from the directory ~/.config/cloudera-deploy/profiles. The default file collected is simply called default in this directory, and is automatically created from the profile.yml template in the cloudera-deploy repo. Then the user may choose to put all the rest of their definition details into the definition.yml file in their definition_path, they may also choose to split it across definition.yml and cluster.yml, for backwards compatibility with legacy Cloudera Playbooks. The dynamic_inventory files are explained in a later section. Required Definition Files . There are 2 files that must be present in the definition path: . | A Definition for the deployment, usually cluster.yml or definition.yml | a post-run playbook, with the name application.yml | . You may optionally supply other files recognised by the Definition parser, such as an inventory template or static ini file. cluster.yml or definition.yml, or split between both . This file contains the variables that define: . | The CDP Cluster(s) - which services, service configuration parameters, host templates &amp; Role allocation, external databases, runtime repositories, security | Cloudera Manager - cm repository, management services, CM configs | Host configurations - e.g. values set via CM -&gt; All Hosts -&gt; Configuration | Auth backends - e.g. ldap/ad | Any other vars required for the Plays in-scope (e.g. kdc, haproxy settings) | . Application.yml . Used to describe additional deployment steps. Although this file must be present, it doesn’t have to do anything and you can simply copy the sample no-op Playbook from the examples/sandbox in Cloudera-Deploy. Your Profile file(s) . If you take a look in the Profile.yml Template, you will see it is a collection of keys in YAML format, these are the keys most commonly personalised to a given user, such as passwords and naming prefixes. The main cloudera-deploy https://github.com/cloudera-labs/cloudera-deploy/blob/main/readme.adoc[readme] does a good job of explaining the different values in this file, and note that you can also set these keys in your definition or cluster files if you wish, but be aware of precedence if you have them set in multiple files. definition.yml file . definition.yml is the main file the user is anticipated to modify in order to define what their Deployment should look like. This is then combined with the Ansible tags used at runtime in order to produce the expected result. For example, you could define a CML deployment in your definition.yml but use the ‘infra’ tag with Ansible to only create the Infrastructure dependencies for CML, but not actually deploy CML itself. This gives the user a great deal of flexibility and operational control. If you wish you may set all of your YAML into the definition.yml file and skip profile, cluster, etc. This may be useful for CICD deployments. cluster.yml file . Although cluster.yml is usually provided externally to the Definition, it can also be included in the definition for two main reasons: . Firstly, the v2 edition of the Cloudera Playbooks expects a cluster.yml definition file, so this provides convenient backwards compatibility and a point of comfortable continuity for existing users migrating to the v3 / Collections based approach. Secondarily, certain complex cluster definitions typically used with the v2 playbook depended upon advanced Ansible lazy-evaluation tricks in order to replace values within the configs, such as hostnames and service descriptors mid-deployment. cluster.yml is specifically lazy evaluated (unlike definition.yml, which is evaluated almost immediately) to allow these deployments to continue to work. We do not expect new users to need these tricks, so this feature is primarily for backwards compatibility. There is a basic cluster definition included in the cloudera-deploy https://github.com/cloudera-labs/cloudera-deploy/blob/main/roles/cloudera_deploy/defaults/basic_cluster.yml[defaults], you may instruct it to be included by setting the value use_default_cluster_definition: yes in your definition.yml, or you could copy it into your definition.yml or cluster.yml file in your Definition directory and then use it as a starting point to get going with customising your deployment. Static_inventory.ini . You may also include a typical Ansible static inventory file within the Definition to be automatically loaded and used for the run, this is in addition to any inventory files in the default Ansible Inventory directory (which are typically empty). There is an example of a Static Inventory https://github.com/cloudera-labs/cloudera-deploy/blob/main/examples/sandbox/inventory_static.example[here], it is notable that the Dynamic Inventory option will generate a static inventory file for you as a part of the process, and tear it down along with the rest of the infrastructure if you use the ‘teardown’ tag. You may also wish to use traditional Ansible dynamic inventory implementations, or the built-in dynamic inventory generator for AWS documented xref:cdDynamicInventory[Dynamic Inventory] . Crafting your Definition . In all simple cases, Cloudera-Deploy aims to have defaults for practically everything but your administrator password, allowing you to then only override any parameters that provide a specific change you want. We recommend, but do not require, that you also set the values in your default user profile as explained in the readme, because otherwise your infrastructure is unlikely to be uniquely tagged and named within the cloud infrastructure account (assuming you are using these features). However, once you have set those defaults, you will likely want to describe the actual deployment you require. There are several examples given within cloudera-deploy, such as the Sandbox, a CML Workshop, or all the Cloudera DataFlow services. These are all primarily public cloud examples, although the Sandbox does include a basic private cloud cluster as well. Generally, CDP Public cloud parameters follow a simple structure of some top-level key triggering a particular component to be deployed, along with whatever dependencies it needs, and then any child keys under that top-level key controlling some override of some default. These keys are explained in xref:cdSchemaReference[CDP Public Definition Keys]. Generally, CDP Private Cloud is a more haphazard structure, simply because it is extremely configurable with nearly a decade of history and backwards compatibility, and therefore trying to constrain it to a pretty structure causes more problems than it fixes within the automation. CDP Public Definitions . Summary . We aim to keep a full dictionary of all the YAML structure in the cloudera.exe docs, there are literally hundreds of parameters that are defaulted and may be overridden, so in this section we will attempt to explain the key values to achieve common goals. Generally the structure is in dot notation within a collected set of default files, one to each Role. If you can find the parameter you are interested in, do a global search within that Role (preferably in your IDE, but also in the repo), and you will likely be able to quickly backtrace how it was generated and find the default. Defaults are usually in the Role which requires them, and will otherwise be moved up to cloudera.exe.common if more than one Role needs to use the same values. For example, we create the subnets in cloudera.exe.infra, but need to supply them to the cloudera.exe.platform and cloudera.exe.runtime Roles as well, so they are sourced from the common defaults. Defaults in cloudera.exe.common may also be overridden by values from whatever called it, via the globals mechanism. This can be observed within Cloudera-Deploy, which contains its own defaults, then reads in the User supplied Definition, then passes the values set down to the Collections. As another example, the key infra.teardown.delete_data occurs within the Infrastructure defaults in the cloudera.exe.infrastructure Role, and it is defaulted to False. If you set it to True, when you run a teardown cloudera-deploy will also clean the targeted storage. You include it in your definition by converting it to YAML: . infra: teardown: delete_data: yes . The second structure we typically use is nested dictionaries keyed by some common variable like Cloud Infrastructure Type, and contained in Ansible vars within Roles. This allows us to set defaults for each infrastructure provider and various types of things we might need, such as virtual machine sizes and disk types. A good example of this may be found in the cloudera.exe.infrastructure Role. Again, global searching for the parameters will show you how they are used within the deployment structure. What is important to note here is that we attempt to allow the user to define the least amount of information necessary to produce the target Deployment, something sometimes refered to as Goal Based Declarative Deployment. You can set as many overrides as you like, but you then take on responsibility for knowing how they will interact, whereas if you use the defaults they are already known to be best-practice. Infrastructure . Controlled under the ‘infra’ key in all the underlying cloud infrastructure services you could need, most defaults are either in the cloudera.exe.common or cloudera.exe.infra defaults files. It is worth examining these defaults if you are interested in which subnet CIDRs, storage types, and other values are used in deployment. If you include the infra key in your definition without anything else, cloudera-deploy will create a base set of infrastructure (network, storage, security groups, etc.) on your chosen provider when you use one of the deployment triggering tags.e.g. To create Infrastructure for CDP Public on Azure: . infra_type: azure infra: . CDP Public Environment &amp; Datalake . This is the core CDP Public ‘platform’ to which the plat Ansible Tag refers, and is controlled by the top-level YAML key env. Apart from the common defaults mentioned earlier, there are platform specific defaults as well in https://github.com/cloudera-labs/cloudera.exe/blob/main/roles/platform/defaults/main.yml[cloudera.exe.platform]. As with other top level keys, if you include env: on its own, then cloudera-deploy will generate an infrastructure suitable for CDP Public and Private based on your Definition, Profile, and other defaults. There are a large number of defaults in this Role, you are advised to scroll through the defaults file above - we resist listing them all here to avoid additional maintenance burden.e.g. To create a CDP Public Infrastructure on GCP with a Datalake pinned to a particular version: . infra_type: gcp env: datalake: version: 7.2.12 . Datahubs . datahub: is the next key we look at, and the first that requires some child keys to work. As with env, if you have datahub in your definition file cloudera-deploy will ensure that the requisite Infrastructure and CDP Public Platform are deployed. For datahubs, you are required to submit a list of configurations, one for each datahub you want to be present. You have three options: . | You may give the name of a definition to deploy it from defaults | You may refer to a jinja template | You may explicitly set the necessary values | .e.g. To Create an OpDB Datahub, and a Kafka Datahub on AWS (default): . datahub: definitions: - definition: ‘Operational Database with SQL for AWS’ suffix: cod-dhub - include: \"datahub_streams_messaging_light.j2\" . dw You can include the top-level key dw: in your definition to deploy a default CDW cluster . More functionality for this service is coming soon . ml CML has a nice example definition, which is again a top level key ml: with a child key ‘definitions’ which is a list of workspaces to create, either from defaults or with specific parameters. If you include ‘ml’ without any child keys a single default workspace will be created. df Like CML, CDF also has an example definition which works in a similar fashion. de Same as above, here is the example definition. opdb Same as above, the example is coming soon. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_info/#deployments-reference",
    "relUrl": "/docs/content/ansible_info/#deployments-reference"
  },"20": {
    "doc": "Information-oriented References",
    "title": "Workflow Reference Guide",
    "content": "Many of these Architecture notes explain the sequence of steps in various parts of the codebase. It is not necessary to understand these processes in order to use Cloudera-Deploy for Deployments, but if you want to make your own processes they may be quite informative. quickstart.sh . Quickstart.sh is provided in the root of the Cloudera-Deploy repository, itself being the reference entrypoint to the tooling. It is intended to prepare and launch you into a shell running inside the Docker execution container where all the necessary software is already prepared for. You’ll know you are in this custom shell by the Orange coloured cldr &lt;version&gt; #&gt; prompt. When you invoke quickstart.sh in Cloudera-Deploy, here is the general sequence of steps . | It determines the parent directory of quickstart.sh, and sets that as the Project directory unless the user has passed in a variable for this purpose | Checks if Docker is running | Runs ‘docker pull’ against the image if there is an updated image available | Creates the default local User Profile mount paths on the local machine if not present | Creates a default Cloudera Deploy Profile in the default path if not present | Checks if various development helper options are set: . | If CLDR_COLLECTION_PATH is set, that path is put into the ANSIBLE_COLLECTIONS_PATH, and the user is instructed to use /runner/project as the base path instead of /opt/ | If CLDR_PYTHON_PATH is set, then that path is added to the PYTHON_PATH in the internal container | . | Checks if ssh-agent and ssh-auth-sock are running / available | Then: . | If the Runner container is present and running, a new bash session is started | If the Runner container is present but stopped, it is removed | If the Runner container is not present or removed by the previous step, a new one is instantiated and a bash session started | . | The current release version of cloudera-deploy is cloned into /opt/cloudera-deploy alongside the other Ansible Collection pieces | The user is given several notices and dropped into an orange coloured bash prompt in the /runner directory | . IMPORTANT: The only way to change the /runner/project directory mounted into the container is to stop the container and rerun the quickstart with a different path as the argument. NOTE: The Ansible Log is pointed at ~/.config/cloudera-deploy/log by the quickstart container launcher . Main Playbook in Cloudera-Deploy . The main.yml Playbook is also in the root of Cloudera-Deploy and is intended as the entrypoint that you would call with the ansible-playbook command in the orange bash prompt we provide you in the Quickstart. IMPORTANT: If you invoke Ansible in a directory other than /runner within cldr-runner, behavior will change as Ansible checks for particular directories relative to the execution path and uses that to find some defaults. Yes this is often annoying, they seem resistant to changing it. The main Playbook is generally invoked from one of two locations: . /opt/cloudera-deploy/main.yml . This is the current release version of Cloudera-Deploy baked into the Runner itself . /runner/project/cloudera-deploy/main.yml . If you have launched quickstart.sh following the usual process, then this is the version of Cloudera-Deploy you have checked out from Github on your local machine, as your Project directory is mounted to /runner/project, and thus cloudera-deploy will be in that project directory. If you are editing the Playbooks or Collections, you should launch from /runner/project to pick up your changes. Here is the general sequence of steps: . | Ansible initialisation . | /runner/env/envvars are loaded | Any Ansible Inventory in /runner/inventory is read | . | The init Role is run . | Run Metadata is immediately logged | The Definition Path and Files are validated | The User Profile is included, making it the lowest in Definition precedence | We validate that an Admin Password has been supplied | We examine the presence and controls around definition.yml and cluster.yml | definition.yml is included, making it the second in precedence | The selected cluster.yml is included (if present), making it third and highest in precedence for Definition files | ‘globals’ is then generated from general top level facts found in files included thus far | ‘globals’ is then updated by any values explicitly set as ‘globals’ from definition.yml, making this the highest definition authority only trumped by extra vars following the usual Ansible hierarchy | The Namespace is then validated and set . | Note that this is not a namespace in the sense of Kubernetes, but the name_prefix that is used for all resources generated by Cloudera-Deploy | . | SSH keys are then validated, if necessary created, and then set | If a Dynamic Inventory is found, it is then validated and parsed | If the Download Mirror has been requested and requirements are met, the prepare_download_mirror Role is then included | Temp directories, various Environment Variables for the Run, and other details are finally set before continuing | The Cloud Playbook is then imported | If Cloud actions are indicated in this Run, the Role cloudera.exe.sequence is imported | Any Dynamic Inventory result is persisted to Static Inventory artefact for later use | If the Download Mirror is in Play, it is then populated and the cached files then persisted to the Download Mirror cache file for later use | If a Teardown has been requested, the clean_dynamic_inventory Role is run | Then the preparation steps for a Cluster deployment are run if a Cluster definition was found during init’s parsing of the Definition . | The static inventory artefact is loaded, if found (including when just created) | The Download Mirror URLs are injected from the cache artefact, if requested | Necessary facts for the Cluster deployment are distributed to the Inventory based on the Run thus far | . | If a Cluster is defined, the Cluster Playbook is then imported | The Application Playbook is imported from the Definition directory for any post-run tasks | . | . Cloud Deployment Sequencing . The cloud.yml Playbook in Cloudera-Deploy acts as a wrapper for steps that are taken against Public Cloud Infrastructure. NOTE: cloud.yml is expected be called within the main.yml workflow, and therefore expects that certain Ansible Variables are available. If you wish to modify or re-use behavior, you are advised to trace the Tasks from the start of main.yml to understand the process. The following steps are included: . | The cloudera.exe.sequence Role is imported if Cloud Infra or CDP Public Cloud included in the Definition | If Dynamic Inventory has been created in the cloud steps, the details about it are persisted | If the Download Mirror is in the Definition, it is prepared and the target files are downloaded to it | The Download Mirror cache is then updated if any new downloads have been completed | . Cluster Deployment Sequencing . The cluster.yml Playbook controls the sequence of Plays which deploy a CDP Base cluster, or legacy CDH cluster. NOTE: cluster.yml is expected be called within the main.yml workflow, and therefore expects that certain Ansible Variables are available. If you wish to modify or re-use behavior, you are advised to trace the Tasks from the start of main.yml to understand the process. It is a long Playbook at ~580 lines, and has many tags to activate or skip plays. There are Tags which control full sequences of Plays, specifically those which trigger a full_cluster build, a default_cluster build, or teardown_all to remove the deployed Clusters (and differs from the very top level teardown tag for Cloudera Deploy) . Tags to trigger individual Plays are best sourced from reading the individual Plays in cluster.yml . The main reason it is broken up into so many plays is that different tasks must happen in a particular sequence on a particular host or set of hosts at any given point. You are advised not to reorder the Plays unless you are quite familiar with the process. There are a number of ‘blocks’ of Plays in the Playbook to help the user orient themselves, they are annotated in comments. The general Deployment sequence is: . | Init Cluster Deployment Tasks . | Prepare any local directories or other items needed on the Controller | Generally required where files from the nodes need to come back to the Controller for the user to do something with, like handling certificate signing requests | . | Check Inventory Connectivity . | Validate that the Inventory given to the Ansible Controller can actually be connected to | Then collects the Ansible Facts from this inventory for use later | . | Verify Inventory and Definition . | We run various validation steps on the Inventory and the Cluster Definition to attempt to fail early if known misconfigurations are present | If a node is designated to host a custom_repo, then the Play Install custom parcel repository is run at this point so that the Repo may be populated with the various files and then validated against the provided Definition | . | Prepare Nodes . | Having validated our Definition and prepared our Run, we now move onto preparing the nodes that will form the cluster | We first apply the OS prerequisites such as huge pages and reconfiguring selinux | We then create any local user accounts needed on the nodes | The JDK is installed on the relevant nodes | Depending on which Database is Defined for Cloudera Manager, the clients and other prequisites for that Database will be prepared | . | Create Cluster Service Infrastructure . | If you’ve designated nodes for KDC, Certificate Authority or HAProxy, those nodes will be prepared | . | Prepare TLS . | Having prepared the ca_server for hosting the Certificate Authority above, this section then prepares the TLS keystores and truststores from the certificates if necessary | . | Install Cluster Service Infrastructure . | Having completed the necessary OS and other prereqs, we are now ready to start installing services | First the selected RDBMS is installed on the db_server node if required by the Definition | Then we setup the specific TLS setup for NiFi if it is required | . | Install Cloudera Manager . | First the Cloudera Manager Daemons are installed | Then the Cloudera Manager Server is installed | If a Cloudera License is included in the Definition, it will be applied to the server, otherwise it will remain in Trial mode | Then the Cloudera Manager agents are installed | If TLS is enabled, Cloudera Manager is configured for TLS using the previously prepared configs | Finally in the CM install, the various agent, server and auth configurations are layered in | . | Prepare Security . | At this stage, if Auto-TLS is enabled in the Definition, it is applied to Cloudera Manager | We then complete the Kerberos configuration for Cloudera Manager and the Cluster if necessary | . | Install Cluster . | Next we move onto installing the Cluster within Cloudera Manager | Firstly, we restart the CM Agents and ensure they are up and heartbeating | We then install and start the Cloudera Manager Management Service | If a custom_repo is in the Inventory, we then check for any parcels which could be preloaded to Cloudera Manager to speed up launch time | Finally in this section, we deploy the Clusters listed in the Definition | This involves importing the Cluster Template, which can take a long time | . | Setup HDFS Encryption . | If KTS and or KMS are in the Inventory ahd Definition, they will now be configured | Finally Client Configs are refreshed to fix up any stale entries | . | . The general Teardown sequence is: * Note that the Teardown Plays follow the Deployment plays in the Playbook, so you’ll see a lot of skipped plays when calling these specific teardowns ** If you have Tagged a ca_server teardown, it is the first to be run ** Then, if TLS is in your Definition, it will be cleaned up ** Finally, if you have Tagged to teardown one or more or all of the Clusters in Cloudera Manager, they will be removed . NOTE: There are no Teardown options for the stages preceding cluster deployment, as it is assumed that you would simply reinitialise the nodes and rerun a deployment to ensure a clean build. Dynamic Inventory . The Dynamic Inventory implementation in cloudera-deploy is primarily intended as an aid to automated testing or trial deployments. It allows the user to provide a template inventory file, which is then converted into a number of VMs on the infra platform, which are in turn used within the same run as inventory for the CDP Private Cloud Base deployment, without any further interaction from the user. Note that this is not necessarily a typical approach, where a user may prefer to create infrastructure using Terraform and then supply Ansible with a static inventory file. Or a user may wish to use a traditional Ansible dynamic inventory plugin. This functionality within Cloudera-Deploy is designed to make it abundantly easy for users new to these technologies to get going with minimal initial learning. Presently this feature is only implemented for AWS, though extending each of the code sections for any other infrastructure provider is quite possible if the process below is understood. It is one of the more complex workflows in cloudera-deploy simply because it touches on several Roles and collections in order to avoid repetition and each segment of the work involved being executed with other similar workflows. The process is deliberately broken up into sections with artefact files at key points to allow the user to substitute their own process or inventory at any given stage. It also makes testing changes a lot easier, and it may then be replaced using Terraform or some other provider at your convenience. NOTE: Dynamic Inventory uses AMIs from the AWS Marketplace, it is necessary for someone to have subscribed to the selected image at least once within the account for it to work. If you are the first person to run this in an account you will get an error and a link to activate the subscription, then you can re-run to complete the deployment. Yay idempotence, as this can only be done in the GUI. The meta structure specific to this feature is as follows: . | Include an inventory_template.ini in your Definition path, which describes the shape of the infrastructure required for your CDP Base Cluster. | Use a supported infra provider, currently this is only AWS | When the main.yml playbook is run, the cloudera-deploy/init Role will pick up and process the template to determine validity and the number of hosts needed for the deployment. | Note that the name of the template file may be modified | Then existence of the template file is checked | We then check if a static inventory has been supplied with the -i switch, and do not process the dynamic inventory if it is set | If the above condition is satisfied, we then use the Ansible ‘refresh inventory’ command with the template included in the inventory dir, this is a tasklist called ‘refresh_inventory’. | Note that this is a minor hack as traditional Ansible usage would recommend against dynamic inventory refresh mid run, but we use it here in the name of NUX. | We then check some simple compliance details within the template, and if it passes we count the number of hosts required in the template and add it to the ‘globals’ variables as ‘vm.count’, which is in turn used later to generate the required host infrastructure. | We then remove the dynamic inventory template from the Ansible Inventory, and refresh the inventory again. | Note: We use this process because Ansible already has the logic to process the inventory template and give us a host count from it. | . | Next, the main.yml playbook calls on the cloud.yml playbook, which in turn calls the sequence Role in the cloudera.exe Collection to run all of the Public Cloud tasks for creating the necessary infrastructure for the deployment. | The defaults used by cloudera.exe for creating the Dynamic Inventory are here in the role defaults | Note: that globals.vm.count from earlier is consumed here, along with naming, ssh, storage, etc. pieces. | Additional defaults are set from a vars file in this Role, where we hold pickers for things like storage, vm, OS type, etc. particular to that Infrastructure provider. All of which may be overridden. | Initialization of necessary information is done in initialize_setup_aws in the same Role, here we get the correct AMI to use for VM deployment, and then the host connectivity information Ansible needs for inventory population | Then in the main setup.yml file within this infrastructure Role, we run the infra provider-specific ‘compute’ task list . | The compute task list first ensures the correct number of VMs is created with the given parameters. This task does a lot of work for us using the Infra provider’s own Ansible modules. | It then prepares the $$infra__dynamic_inventory_host_entries$$ variable with a list of the necessary connection information for each new host in a format that Ansible understands, which is a combination of the generic connector information in $$infra__dynamic_inventory_connectors$$ (ansible_user, ssh private key file) and the VM-specific information (private FQDN as node name, Public FQDN as ansible_host) | . NOTE: This connectivity information is very important for Ansible to build clusters correctly on AWS because AWS uses a reverse-proxy for DNS. Other infra providers would probably only require a much simpler set of connectivity information. | . | Next, the cloud.yml playbook will run a post-processing check if dynamic inventory VMs have been created by checking the $$infra__dynamic_inventory_host_entries$$ variable, and if it and the template are present it will merge them into a static_inventory.ini artefact in the Definition directory using the cloudera-deploy/persist_dynamic_inventory tasklist. This artifact is later used by the cluster.yml playbook. NOTE: This essentially picks up the connection entry for each host and replaces the template entry on a 1:1 basis, look at the example above to see how it works. | Back in the initial main.yml, we check if a static inventory artefact is presented. If the user created their own, or used the preceding dynamic inventory process to generate a static inventory artefact, this process is then triggered, thus allowing both options. This once again uses the ‘refesh_inventory’ tasklist to inject the static_inventory artefact (regardless of how it was created) into the Ansible Inventory ready for a Cluster deployment. | The cluster.yml playbook is then called, which will use whatever inventory is present to run the cluster deployment according to the rest of the configuration and tags presented. In this case it will pick up the static inventory artefact, produced by thi dynamic inventory process, and deploy the cluster. | . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_info/#workflow-reference-guide",
    "relUrl": "/docs/content/ansible_info/#workflow-reference-guide"
  },"21": {
    "doc": "Information-oriented References",
    "title": "Definition Reference Guide",
    "content": "In this section we will break down all the available Definition options by section and explain their usages. The general structure of each section will start with guidance on the minimally required information for that type of Deployment, followed by guidance on the various options. For an introduction to the basics of Definition creation and the most common options, please see the [Deployments] Section. Profile . The default Cloudera-Deploy user profile is instantiated by the quickstart.sh script from a template file profile.yml in the repository, and written to ~/.config/cloudera-deploy/profiles/default . The values in the profile are among the most common to be set for a Deployment, as such the User has several options where they can be set: . | Values in the default profile will be picked up at the lowest priority | if the user provides the path to an alternative profile.yml via the extra-vars option abs_profile, that will be used instead of the default profile | The User may set the values from the Profile in definition.yml, which will override the previous stated options | The User may pass in the values as extra-vars at runtime, which overrides almost anything else. | . IMPORTANT: Many Profile values use bash expansion of the user’s home directory (~/)to refer to files, this allows the same path to work on your local machine AND in the Runner. Using absolute paths (/home/user) in the Profile is strongly advised against, but should be fine elsewhere in the Framework. Default Password . admin_password: Mysecretpword1! . IMPORTANT: You must provide an admin_password to the Framework in some manner, it is the only mandatory value which we do not set a default for. This password will be propagated to your CDP Public Workload User, the admin user in CDP Base, and anywhere else a security value may be required such as in a database required by the Definition you supply. We recommend you set individual passwords for separate services in serious deployments using the overrides available for each of these values in the Definition. For Demo and Development a common password is a lot easier. Password Complexity . Recommended 12-16 chars, 1 Cap, 1 num, 1 special (avoid ‘#’, ‘?’ and ‘/’ for some subsystems). This satisfies password requirements for most subsystems. What we mean by this is that the Framework automates many, many subsystems for you depending on what you put in the Definition, and each of those subsystems has different security standards. We strive to balance having security out-of-the-box with an easy to set default value. Namespace . name_prefix: cldr . Where names are procedurally generated by the Framework, rather than explicitly provided by the user, this will be prefixed. Sometimes it will be joined with other sources of uniqueness, such as the first two characters of the cloud provider, to generate a unique name for a given subsystem. All deployment names should be prefixed to allow differentiation and accurate discovery or filtering. You are recommended to combine this with unique default Tags when there are a lot of Deployments sharing some Tenant to avoid clashes. NOTE: Many subsystems require that deployments within a given Tenant have a unique name, as such you should avoid using the default name prefix cldr if multiple people may be using this automation Framework in your Deployments. Ideally it should not exceed 6 chars, and must start with a letter. your personal Initials are an excellent option in most cases. WARNING: Do not use punctuation in your name_prefix, particularly ‘-‘ and ‘_’ can cause extremely weird behavior in some subsystems, particularly security subsystems. WARNING: If you are deploying to Azure, we strongly recommend it be &lt;= 4 chars, as Azure has quite short field character count limits in some cases, particularly storage. It will default to the name_prefix cldr as set in roles/cloudera_deploy/defaults if not provided by the User. Default Tags . These tags are applied by default to all deployed infrastructure, particularly chargeable infrastructure on cloud providers. You can also set specific Tags on specific deployments, such as a Datahub, in their sections of the Definition. Some subsystems require certain tags to self-identify different functions, this is particularly common with certain cloud Kubernetes implementations. These will be automatically applied for you where appropriate. You should use tags to identify your services to make it easy to track and remove them when no longer needed. tags: owner: dchaffelson@cloudera.com enddate: \"01312022\" . IMPORTANT: The two-space indent of the tag key: value pairs is critical for correct YAML structure . NOTE: For backwards compatibility reasons, we give the example date above in American or ‘Freedom’ format of MMDDYYYY. We recommend most people use ISO8601 or YYYYMMDD format. Cloud Infrastructure . Cloud Provider . Specifies the Cloud Infrastructure provider, CDP presently supports GCP, AWS and Azure . infra_type: aws # gcp, azure . NOTE: Only necessary when using Public Cloud Infrastructure and when you do not specify an Ansible Inventory . Cloud Region . Specify the default region you prefer for your infrastructure provider . It must be valid for the selected infra_type . infra_region: us-east-1 . IMPORTANT: Generally it is not a problem if the region you set in Definition (or these defaults) differs from the region you have set in the CLI Config. The exception is where you wish to combine other tools with this Framework and they start looking in a different region. Be mindful of mixed configurations. The automation service attempts to validate that the requested build will work in the given region, so some may be rejected as not all regions provide all cloud services. NOTE: You are advised to build a reference deployment, including all services you intend to use, before investing into data and application development in a particular Cloud provider or region. You can immediately remove it once this validation is complete. Listing of AWS regions. Listing of Azure regions (Note: not microsoft.com). Or you can run this in the runner: . az account list-locations --query '[*].name' . Listing of GCP regions. Cloud Credentials . Path to Google Cloud Credentials file, if using Google Cloud . Should be in your local profile . WARNING: We recommend they should not be located anywhere near a version controlled directory like git to avoid accidental inclusion! . If using Azure or AWS the credentials will be automatically collected from your local user profile; these credentials are required because Deployments require a GCP Service Account which is handled differently. gcloud_credential_file: '~/.config/gcloud/mycreds.json' . SSH . NOTE: Your SSH keys should be in your local profile (typically ~/.ssh), or the Definition Path, or some other persistent directory available to the Runner. You are advised to be careful of not inadvertently committing your private SSH keys to version control. NOTE: If you have set the necessary SSH Information into your Ansible Inventory for deploying CDP Base, you can leave these fields commented out of your profile. They are only really necessary for Public Cloud Deployments. Public Key File . A Public Key file is required if using Azure or GCP as Cloud Infrastructure, or deploying Private Cloud . If not supplied, one will be generated using the supplied name_prefix, along with a matching Private Key file, and ignoring the private_key_file setting below. The default location is the usual ssh path in the User’s Home Directory ~/.ssh . public_key_file: '~/.ssh/cldr.pub' . Private key file . Required if deploying Dynamic Inventory to set the Ansible Connection Parameters. NOTE: Must be set if public_key_file is set as Ansible validates that your keys match as a convenience . private_key_file: '~/.ssh/cldr.pem' . Key Name . Required for AWS Cloud Infrastructure . Defaults to the Namespace if not set . Must be set if public_key_file is set, even if not using AWS. public_key_id: cldr . Cloudera License . Path to your Cloudera License file, if you want to supply one. Required if deploying a CDP Cluster for Private Cloud in a mode other than Trial, or where files are required to be downloaded from behind authentication on archive.cloudera.com . Should be in your local profile using bash expansion, or the definition directory. license_file: \"~/.cdp/my_cloudera_license.txt . NOTE: Putting the license file in ~/.cdp/ is a convenient convention, but not required. Full Profile YAML . admin_password: Mysecretpword1! name_prefix: cldr tags: owner: dchaffelson@cloudera.com enddate: \"01312022\" infra_type: aws infra_region: us-east-1 gcloud_credential_file: '~/.config/gcloud/mycreds.json' public_key_file: '~/.ssh/cldr.pub' private_key_file: '~/.ssh/cldr.pem' public_key_id: cldr license_file: \"~/.cdp/my_cloudera_license.txt\" . Globals . Globals is a structure to allow particular variables to be propagated across the entire Framework in a simple dictionary, and they are usually set by the top level playbook and then inherited by the Collections. Globals are usually taken from some default or User set value, as such you are advised to not set the globals dictionary directly, as you may squash other variables the Framework expects to be there. If you want to change something you have found in globals during debugging, look to where it is set upstream in the Playbook or other locations. Generally only add new values to globals when developing new features if they are needed in multiple Collections, such as cloudera.exe and cloudera.cluster. General . The Globals in this section are explained elsewhere, as most come from the User Profile in Cloudera-Deploy, or are derived from its initialization process. globals: admin_password: MySuperSecret1! artifacts: create_deployment_details: yes directory: ~/.config/cloudera-deploy/artifacts cloudera_license_file: ~/.cdp/my_cloudera_license.txt create_utility_service: yes dynamic_inventory: vm: count: 6 os: centos8 gcloud_credential_file: ~/.config/gcp/credentials.json infra_type: aws name_prefix: cldr namespace_cdp: cldr-aw region: eu-west-1 tags: mykey: myvalue utility_bucket_name: cldr-0123456789-uk-west-1 . Object Storage Name . globals: storage: name: us-west-1-default . This particular global is to allow the User to set a unique value to be used when constructing the name for a cloud object store, such as an S3 bucket or Azure storage account. It defaults to the region + aws profile name for AWS, and just the name of the region for the other clouds. When combined with the namespace, this is usually sufficiently unique for most users. This odd construction is necessary because, unlike most other cloud resources, these names much be globally unique to all accounts. This is not always the case, but as it usually is we opt for a solution which is less likely to surprise the user with an unexpected error. SSH . Unfortunately the different cloud providers, and OS types that you may use, tend to have slightly different requirements when it comes to remote access via SSH. As such, you may observe there are several options for providing your SSH credentials to the Framework. In most cases you are advised to supply the path to your public_key_file and private_key_file in your profile. If you are using AWS, you should set the public_key_id to match. You may supply the public_key_text directly for some Azure use cases, but we will look it up from the file if you supply public_key_file instead, and this latter option is preferred security practice. globals: ssh: key_path: ~/.ssh private_key_file: ~/.ssh/mykey.pem public_key_file: ~/.ssh/mykey.pub public_key_id: mykey public_key_text: . Labels . The global labels are the short strings used as identifiers when constructing larger labels for uniqueness. It allows us to procedurally generate the different names from standard pieces. They are deliberately short in some cases due to restricted character counts on some fields. globals: labels: admin: admin app: app cml: cml cde: cde credential: cred cross_account: xaccount data: data datalake: dl datalake_admin: dladmin default: default env: env group: group idbroker: idbroker identity: identity internet_gateway: igw knox: know logs: logs policy: policy private: pvt public: pub ranger_audit: audit role: role service_network: svcnet storage: storage subnet: sbnt table: table user: user vpc: vpc vpce: vpce . Infrastructure . AWS . infra: aws: profile: region: arn_partition: vpc: az_count: internet_gateway: name: suffix: labels: public_route_table: private_route_table: public_route_table_suffix: private_route_table_suffix: existing: vpc_id: public_subnet_ids: private_subnet_ids: role: tags: policy: tags: storage: tags: private_endpoints: . Azure . infra: azure: metagroup: name: suffix: netapp: account: name: suffix: pool: name: size: suffix: type: suffix: volume: name: size: suffix: type: region: sp_login_from_env: storage: class: name: type: . Dynamic Inventory . infra: dynamic_inventory: storage: delete: size: type: tag: tag_key: tag_value: vm: suffix: type: . GCP . infra: gcp: project: region: storage: path: data: logs: . Security Groups . infra: security_group: default: name: suffix: knox: name: suffix: vpce: name: suffix: . Storage . infra: storage: name: path: data: de: logs: ml: ranger_audit: . Teardown . infra: teardown: delete_data: delete_mirror: delete_network: delete_ssh_key: . VPC Networking . infra: vpc: cidr: extra_cidr: extra_ports: name: private_subnets: private_subnets_suffix: public_subnets: public_subnets_suffix: service_network: name: subnet: user_cidr: user_ports: tunneled_cidr: . Environment . The Environment definition is one of the largest, because the Framework takes the position that it will only work with one Environment per run for CDP Public. This gives us the convenience of using it as encapsulating object for all configuration at the Platform level. We’ll break the env key down into sections around the direct keys and then the complex sub-keys, and then provide the full schema at the end of this section as usual. Top Level Environment Keys . These keys are also at the top level under env, but do not have complex substructures and therefore do not need breaking out into a separate explanation . env: name: cldr-aw-env suffix: env workload_analytics: yes # no tunnel: yes # no public_endpoint_access: yes # no . Name . The name may be supplied here, or it will be procedurally generated by concatenating the namespace, cloud provider, and suffix. We include the cloud provider in the generated name as the user may wish to make a group of environments within a namespace but across multiple clouds, and the names must be distinct. IMPORTANT: The first eight characters of the name of the Environment must be unique within the CDP Tenant, as they are used in generating various DNS entries for some cloud subsystems which you probably do not want to clash. Suffix . The suffix used when generating the name procedurally, if the name is not directly provided. Note that the suffix can be set here within the Environment definition, or at the globals level if you are composing your configs. Workload Analytics . This simple true or false value enables workload analytics for this particular environment. It defaults to false. L0, L1, and L2 network architectures . One of the main consequential decisions when deploying a CDP Public Environment is the Cloud Networking Architecture you wish to deploy. While this has more complex implications in terms of the Network topology created, we abstract the typical cases into top level flags here. We generally refer to network setups for CDP Public as falling into three categories. If all nodes have public IPs and internet access, we call this L0 or Public - set tunnel &amp; endpoints to false If gateway nodes have public IPs, but other nodes do not, we call this L1 or semi-public - set tunnel &amp; endpoints to true If no nodes have public IPs, we call this L2 or private - set tunnel to true and endpoints to false . Note that L0 and L1 will work out of the box as the Framework will put the IP of the Ansible Controller on the network security allow-list by default. This is a practical consideration because the Controller usually needs to connect to hosts to do more configuration work, and the User also usually wants to access those machines from their workstation which is usually running the Controller. For an L2 configuration, the User will need to have some other arrangements to access to the private IPs within the deployed network. Perhaps a jumpbox, VPN, VPC Peering, or one of many such possibilities. These deployments are typical of Production cloud networking in enterprise customers and setup of them is outside the scope of this Framework. Tunnel . Setting tunnel option to true enables CCM gateway which removes the need for the environment hosts to have a public IP address. The default is false. Public Endpoint Access . Setting public_endpoint_access to true enables public workload endpoint access gateway which lets users access workload from the internet. The default is false. Needed when tunneling is enabled, but you don’t have the direct connectivity with the VPC via a VPN or similar. Environment AWS sub-structure . As the actual definition of Infrastructure to be deployed on AWS lives under the infra tag, this section under env is primarily concerned with handling the naming and deployment of the necessary Policies and Roles. While the Roles and Policies are created on AWS, and therefore you would reasonably think they should be part of the Infrastructure section, we moved them into the Platform section because the selection of Roles and Policies is closely tied to the shape of the CDP Public Environment and Datalake to be created, especially the cross-account access. It’s not a perfect demarcation, but we have found this setup to be the least-worst of the available options. Policy Naming . Every value in this section has a practical default. You can override the suffix used in policy name generation with the suffix key, or directly set the literal name used for the policy object with the appropriate name subkey. You may set tags to be applied to Policies here. We plan to support directly supplying the policy documents from local files in the future, presently the Framework uses the official Cloudera policies directly from Cloudera’s codebase. env: aws: policy: name: bucket_access: cldr-bucket-access-pol cross_account: cldr-xaccount-pol datalake_admin_s3: cldr-dl-admin-pol idbroker: cldr-idbroke-pol log: cldr-log-pol ranger_audit_s3: cldr-ranger-s3-audit-pol suffix: pol tags: pol_key: pol_val . Roles . Similar to Policies above, here you can set the label used when generating names for Roles. The label simply specifies the short descriptive string for that individual component type, and the ‘suffix’ is the string appended for this particular class of object. If you do not set them here in the Environment configuration, they are usually set to one of the global suffix or label defaults. As such, you do not need to set any of these values in most cases. You can also set the names for the Roles directly. env: aws: role: label: cross_account: xaccount datalake_admin: dladmin idbroker: idbroker log: log ranger_audit: audit name: cross_account: cldr-xaccount-rl datalake_admin: cldr-dladmin-rl idbroker: cldr-idbroker-rl log: cldr-log-rl ranger_audit: . Storage . Here you can simply override the default suffix used for naming policies and Roles for storage. Not to be confused with the naming of buckets or storage accounts in the Infrastructure definition. env: aws: storage: suffix: . Environment Azure sub-structure . Azure Application Name . Explicitly set the name, or just the suffix to use when procedurally generating the name. env: azure: app: name: cldr-xaccount-app suffix: app . Azure Custom Policy for Cross Account Role . The Policy is stored in version control in Cloudera Labs and set to the minimum necessary policies for all CDP Public deployments to function. You use your own policy document if you wish, but we recommend consultation with Cloudera Support first. You may also override the suffix used when naming the Policy during creation . env: azure: policy: suffix: policy url: https://raw.githubusercontent.com/cloudera-labs/snippets/main/policies/azure/cloudbreak_minimal_multiple_rgs_v1.json . Azure Roles . env: azure: role: assignment: cross_account: contributor: role: datalake_admin: data: storageowner: logs: storageowner: idbroker: mgdidentop: vmcontributor: log: storagecontr: ranger_audit: storagecontr: label: data: datalake_admin: idbroker: identity: log: ranger_audit: xaccount: name: cross_account: datalake_admin: idbroker: log: ranger_audit: name_suffix: admin: assignment: contributor: operator: owner: user: suffix: . Azure Storage . env: azure: storage: path: data: logs: suffix: . Environment GCP sub-structure . env: gcp: bindings: cross_account: logs: role: label: cross_account: datalake_admin: idbroker: identity: log: ranger_audit: name: cross_account: datalake_admin: idbroker: identity: log: ranger_audit: suffix: storage: path: data: logs: suffix: . Environment CDP sub-structure . env: cdp: admin_group: name: resource_roles: roles: suffix: control_plane: cidr: ports: credential: name: name_suffix: suffix: cross_account: account_id: external_id: group_suffix: user_group: name: resource_roles: roles: suffix: . Environment Datalake sub-structure . env: datalake: name: suffix: user_sync: version: scale: . Environment Teardown sub-structure . env: teardown: delete_admin_group: delete_credential: delete_cross_account: delete_policies: delete_roles: delete_user_group: . Datahubs . When the datahub key is included at the top level, you are required to provide an array of definitions providing at enough information for the Framework to know which one you want deployed. Datahub Deployment configurations are prepared in the Prepare for CDP Datahub clusters task within the cloudera.exe.runtime.initialize_base Role within the Cloudera.exe Collection. They are deployed in the Request CDP Datahub deployments Task within the cloudera.exe.runtime.setup_base Role, and leverage the cloudera.cloud.datahub_clusters module. Minimum Datahub Definition . The minimal definition to create a Datahub is to provide the name of a predefined Datahub Definition in the definition key within the array of definitions under the datahub key, e.g. datahub: definitions: definition: Streams Messaging Light Duty for AWS . NOTE: A listing of available Datahub Definitions can be found in the CDP UI by navigating to: Management Console &gt; Environments &gt; Your Environment &gt; Cluster Definitions . You may also use a Jinja Template via the include key, there is an example here . You may also specify a CDP Datahub Template (sometimes called a Cluster Blueprint) using the template key, which will be paired with the instance_groups (either from defaults, or supplied by you) in order to produce a Definition to be deployed. NOTE: Available Cluster Templates, including Custom Templates, can be found in the CDP UI by navigating to: Management Console &gt; Shared Resources &gt; Cluster Templates . So, in summary, a Datahub Definition is a combination of a Template and Instance Groups. You may either use predefined Datahub Definitions from the CDP Control Plane, or pass in various methods of constructing one yourself. Naming your Datahubs . If you do not supply a name key in your Datahub Definition, the Framework will attempt to create a name for you. Datahub names must be unique within a Tenant, so you have several options: . | You are advised to supply your own unique name as the best option | Or you can set the suffix key which will be concatenated with the namespace and cloud provider in order to generate a fairly unique but deterministic name, e.g. the suffix dhub01 with the namespace cldr on AWS would produce cldr-aw-dhub01 | . Instance Groups . You may provide a detailed specification of your own instance groups, either on a per-Datahub basis in the definitions array, or by supplying a replacement instance_group_base key. In most cases this is not recommended, as they predefined Datahub Definitions have best-practice configurations in place. Of particular note for instance_groups are the look-up tables in cloudera.exe.runtime.vars which specify defaults for compute and storage used in various cases. You may wish to change the types and sizes of these values to suit your own scale. NOTE: If you use a pre-defined Datahub Definition using the definition key, the instance_groups here are ignored, as the Definition has them baked in. You need to use a template or one of the other methods to override the instance_groups. Image Catalog . When constructing a Datahub Template, the appropriate image for deployment is selected from the CDP Control Plane Image Catalog. The default Image Catalog for CDP is used, but you may supply the name and URL for a custom image catalog if you wish. Preparing a custom image catalog is outside the scope of the Automation Framework. Full Datahub Definition . This is the full specification with the most common default values included for your convenience. In most cases you would not supply most of these values in your own Definition, and actually doing so is likely to cause a maintenance burden. datahub: definitions: - name: streams-messaging-dhub-01 include: datahub_streams_messaging_light.j2 template: Streams Messaging Light Duty: Apache Kafka definition: Streams Messaging Light Duty for AWS suffix: streams-dhub-01 instance_groups: - nodeCount: 1 instanceGroupName: master instanceGroupType: GATEWAY instanceType: \"\" rootVolumeSize: 100 recoveryMode: MANUAL recipeNames: - some_recipe_name attachedVolumeConfiguration: - volumeSize: 100 volumeCount: 1 volumeType: \"\" tags: key: value compute: aws: std_gp: 'm5.2xlarge' lrg_gp: 'm5.4xlarge' std_mem: 'r5.4xlarge' dsk_mem: 'r5d.4xlarge' std_gpu: \"p2.8xlarge\" azure: std_gp: 'Standard_D8_v3' lrg_gp: 'Standard_D16_v3' std_mem: 'Standard_D16_v3' dsk_mem: 'Standard_D8_v3' std_gpu: 'Standard_D8_v3' gcp: std_gp: 'e2-standard-8' lrg_gp: 'e2-standard-8' std_mem: 'e2-standard-8' dsk_mem: 'e2-standard-8' std_gpu: 'e2-standard-8' image_catalog: name: cdp-default url: https://cloudbreak-imagecatalog.s3.amazonaws.com/v3-prod-cb-image-catalog.json instance_group_base: nodeCount: 1 instanceGroupName: master instanceGroupType: GATEWAY instanceType: \"\" rootVolumeSize: 100 recoveryMode: MANUAL recipeNames: - some_recipe_name attachedVolumeConfiguration: - volumeSize: 100 volumeCount: 1 volumeType: \"\" storage: aws: std: 'standard' fast: 'st1' eph: 'ephemeral' azure: std: 'StandardSSD_LRS' fast: 'StandardSSD_LRS' eph: 'StandardSSD_LRS' gcp: std: 'pd-standard' fast: 'pd-standard' eph: 'pd-standard' . Data Engineering . de: definitions: - name: cde-01 instance_type: 'm5.2xlarge' minimum_instances: 1 maximum_instances: 4 minimum_spot_instances: 0 maximum_spot_instances: 0 enable_public_endpoint: yes enable_workload_analytics: yes initial_instances: 1 initial_spot_instances: 0 root_volume_size: 100 chart_value_overrides: - chartName: dex-app - overrides: dexapp.api.gangScheduling.enabled:true skip_validation: yes tags: definition-tag: value use_ssd: yes virtual_clusters: - name: cloudera-deployed-vc-1 cpu_requests: 32 memory_requests: '128Gi' spark_version: 'SPARK2' acl_users: '*' runtime_spot_component: 'NONE' chart_value_overrides: - chartName: dex-app - overrides: pipelines.enabled:true suffix: de-svc tags: default_tag: value force_delete: no vc_suffix: de-vc . Data Flow . df: suffix: min_k8s_nodes: max_k8s_nodes: public_loadbalancer: loadbalancer_ip_ranges: kube_ip_ranges: cluster_subnets: loadbalancer_subnets: teardown: persist: force_delete: terminate_deployments: . Data Warehouse . dw: definitions: suffix: . Machine Learning . ml: definitions: k8s_request_base: suffix: tags: public_loadbalancer: . Operational Database . opdb: definitions: suffix: . Data Management . data: storage: # A list of lists of locations (read/[only|write]) defined in a policy and assigned to a Role - read_only: bool locations: [] policy: name: suffix: delete: bool role: datalake_admin: bool name: suffix: delete: bool policy: suffix: aws: suffix: read_only: suffix: url: read_write: suffix: url: role: suffix: aws: suffix: teardown: delete_policies: delete_roles: . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_info/#definition-reference-guide",
    "relUrl": "/docs/content/ansible_info/#definition-reference-guide"
  },"22": {
    "doc": "Information-oriented References",
    "title": "Information-oriented References",
    "content": ". | Background . | Guiding Principles | History | Declarative vs Procedural | Implicit with defaults vs explicit | Multi-tier abstraction | Ansible Collections and the Handling of Variables | Cloud Infrastructure and the Naming of Objects | . | Terminology | Deployments Reference . | Definition Structure | Crafting your Definition | CDP Public Definitions | . | Workflow Reference Guide . | quickstart.sh | Main Playbook in Cloudera-Deploy | Cloud Deployment Sequencing | Cluster Deployment Sequencing | Dynamic Inventory | . | Definition Reference Guide . | Profile | Globals | Infrastructure | Environment | Datahubs | Data Engineering | Data Flow | Data Warehouse | Machine Learning | Operational Database | Data Management | . | . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_info/",
    "relUrl": "/docs/content/ansible_info/"
  },"23": {
    "doc": "Introduction and User-orientation",
    "title": "Introduction",
    "content": "Ansible Automation for Cloudera Products AKA: Cloudera Deploy | Cloudera AutoProv | Cloudera Ansible Foundry | Cloudera Playbooks . What to Expect . These docs are for new users and power users alike to leverage Cloudera’s Ansible implementation for deployment and automation of Cloudera Products within the wider Hybrid Data Cloud ecosystem. It is generally broken into several sections: . For all users: . | These introductory notes to orient new users . | A discussion of Use Cases that the Framework supports . | A Getting Started guide based around user goals and skill levels . | An Overview of preparing Definitions for Deployment . | . For Developers: . | A History and guiding principles . | A breakdown of Framework Components . | An in-depth Setup Guide . | Logical breakdowns of key Workflows within the Playbooks . | A Reference guide to all Definition options . | . What is Cloudera Ansible Foundry . This framework is generally called the Cloudera Ansible Foundry, and it wraps software dependencies, automation code, examples, and documentation into an integrated family. It further provides a consistent and portable toolkit for working in Python, Bash, Terraform, various CLIs, language clients, ad-hoc commands, and other commonly used tools in the Hybrid Cloud ecosystem. What is Cloudera Deploy . Cloudera Deploy is the name of the reference implementation in Ansible Playbooks for automating Cloudera Products. For most users, Cloudera-Deploy is the entrypoint and only component they need interact with, but underneath it there is an extensible framework of components for handling various scenarios for power-users. To that end, you can use Cloudera-Deploy itself directly, or use it as a starting point for your own implementation, or simply use the components within for your own purposes. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#introduction",
    "relUrl": "/docs/content/ansible_intro/#introduction"
  },"24": {
    "doc": "Introduction and User-orientation",
    "title": "Licensing, Warranties and Restrictions",
    "content": "Cloudera-Deploy and the underlying framework are all open source, mostly under the Apache 2.0 license or compatible licenses. The software is provided without Warranty or Guarantee of Support - it therefore differs from, and is complementary to, Software provided by Cloudera or other Vendors under commercial agreements. However, while we do not guarantee support, Cloudera use this software along with our partners and customers, and thus strive to maintain it to the same high standards of our other products in the best spirit of community and partnership. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#licensing-warranties-and-restrictions",
    "relUrl": "/docs/content/ansible_intro/#licensing-warranties-and-restrictions"
  },"25": {
    "doc": "Introduction and User-orientation",
    "title": "Use Cases",
    "content": "Simple Use Cases . These simple use cases can be run from the Cloudera-Deploy Quickstart without requiring additional skills in advance beyond use of cmdline, docker, and cloud credentials . | Deploy Reference Architectures for CDP Public on AWS, GCP, or Azure . | Deploying from OSX, Windows10, or Linux machines . | Leverage Ansible or Terraform for Cloud Infrastructure . | Deploy CDP Base clusters to most supported OS targets . | Deploy Applications on various CDP implementations . | Teardown and other lifecycling task . | . Intermediate Use Cases . In these cases, we expect that you are a more experienced DevOps user with particular goals in mind and at a good working knowledge of the technologies in play . | Deploy CDP Private Cloud on a Kubernetes Variant . | Deploy Applications across multiple CDP Hybrid Cloud Environments . | Integrate a 3rd party component into the Framework . | Create a mirror of archive.cloudera.com or custom offline repo of parcels . | Use the framework within Ansible Tower, or via CICD integration . | Handle partial deployments or partial teardowns using run levels . | Automate deployment to adopted infrastructure . | Extend the Ansible Collections to handle additional cases . | Force remove cloud infrastructure within a given namespace . | . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#use-cases",
    "relUrl": "/docs/content/ansible_intro/#use-cases"
  },"26": {
    "doc": "Introduction and User-orientation",
    "title": "Framework Components",
    "content": "Here we will give more of a sense of what is in each of the repositories within the framework . cloudera-deploy . This is the standard entry-point for most users, most of the time. It contains the main Ansible Playbooks, default User Profile, Definition handling, and Run initialization. It also usually contains other ease-of-use features for new users, like the current Dynamic Inventory implementation, until such time as they are matured into their own separate feature. cldr-runner . A Container image as a common Runner with all dependencies suitable for use locally, with Ansible Tower, against an IDE, or various other circumstances . It is based in a centos8 image produced by the Ansible Community, known as ‘Ansible Runner’. We then resolve the difficulty of conflicting dependencies to layer in the various clients, python modules, utilities, and other bits and pieces that make it a useful shell or remote execution environment for working with Hybrid Cloud. You can also use this as a template for your own specific implementation, but we ask Users to adopt it as their default if they don’t have a reason not too for the simple benefit of not reinventing something that works well and reduces the burden of reproducing errors. cdpy . A Python client for the CDP Control Plane, both Public and Private. This is essentially a convenience wrapper for CDPCLI, which itself is based in a fork of AWSCLI and fully written in Python. cdpy contains a large number of helper methods which are reused throughout the cloudera.cloud modules, as well as well-structured Ansible-friendly error handling. Cloudera Ansible Collections . | cloudera.cloud | . This Collection is primarily un-opinionated modules for the CDP Control Plane in Public or Private Cloud. The point of keeping the modules unopinionated and solely covering the CDP Control Plane interactions is to minimise the dependencies and attack surface for users who aren’t using Public Cloud. | cloudera.exe | . This Collection is highly opinionated Roles for most task sequences around achieving some run-level or dependency satisfaction. The concept of run-level deployments in cloudera.exe is explained later in the Architecture. | cloudera.cluster | . This Collection is focused on Deploying &amp; Configuring Clusters via Cloudera Manager - usually traditional Cloudera clusters. It is backwards compatible with the Cloudera Playbooks written for Ansible &lt;2.9, whereas this Collection is for &gt;2.10 . Example Deployment Definitions . Example Definitions for use with Cloudera-Deploy, presently bundled here . We plan on publishing more examples soon. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#framework-components",
    "relUrl": "/docs/content/ansible_intro/#framework-components"
  },"27": {
    "doc": "Introduction and User-orientation",
    "title": "Getting Started",
    "content": "Cloudera-Deploy is designed for use both internally and externally by Cloudera Engineering, Testing, Education, Support, Field, Sales, Marketing and other staff, but also the same code and artefacts are used by Customers, Partners, Resellers, and our Community. As such, we have set it up so a new user can get started in as little as 3 steps, while not limiting Power Users from leveraging the full suite of capabilities. | Prerequisites | . While Cloudera-Deploy aims to remove as many complexities for the new user, there are still a few dependencies like authentication and runtimes required for it to work. The quickstart guide contains extra prerequisite steps that you are advised to follow if this is your first time using new credentials or tools such as AWS CLI. | Understanding the Skills Gap | . We recognise there is a significant jump in the skills and knowledge required to go from using the pre-configured push-button examples in the ‘Simple’ section to more complex cases the ‘Intermediate’ section and beyond. There are several good introductory courses online for Ansible and Terraform use in Hybrid Cloud Architectures. Cloudera also offers a training program tailored to these tools to interested Customers and Partners. You can also reach out to your Cloudera contacts for assistance in these deployments at any time. Personas . | New to Automation | . Using Cloudera-Deploy without modifying it only requires a couple of steps and some cloud credentials. It is designed to be extremely accessible to new users. If this is your first time, we suggest running the Cloudera-Deploy Quickstart using one of the prebuilt Definitions before diving in further. Skills required: commandline, editing text files, cloud credential management . | Create your own Definitions | . Crafting Definitions is just editing text files to declare what the Deployment should look like, and allowing Cloudera-Deploy to interpret and produce what you have described. The files are written in YAML with a simple structure. Once you have worked out how to run a basic Deployment via the Quickstart, you may wish to customise it to meet your requirements. A summary of editing Definitions is included in the Getting Started sections for CDP Public Cloud and CDP Private Cloud. There is also a detailed explanation of how Definitions work in the Deployments Reference in the Developer’s Guide, though reading the rest of these Docs and examining the other Definition examples may also be helpful. Additional skills required: YAML editing and formatting, Cloudera-Deploy Definitions management . | Create your own Playbooks | . Creating your own Playbooks requires a working understanding of authoring Ansible tasks and how to use Modules and Roles from Ansible Collections. It goes a step beyond editing YAML declarations into understanding the sequences of steps necessary to achieve a given outcome in a hopefully robust manner. We will soon be publishing more examples of Application Playbooks on top of Cloudera-Deploy, but you can review the Playbooks already included as a starting point. We suggest you start by fully reading this documentation including the Developer sections and how the existing Workflows are structured. Additional skills required: Ansible Task development, use of Ansible Collections, advanced use of Ansible Tags, experience with multi-tier automation abstractions . | Extend the Framework | . If you are familiar with Ansible and Cloudera Products, and are considering adding Ansible modules or Roles to the Collections, then you may wish to follow the Developer Setup to fork and checkout the Framework components so you can make (and perhaps contribute back) your own changes. Additional skills required: Python development, Ansible test frameworks and debugging, Docker Containers . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#getting-started",
    "relUrl": "/docs/content/ansible_intro/#getting-started"
  },"28": {
    "doc": "Introduction and User-orientation",
    "title": "Introduction and User-orientation",
    "content": ". | Introduction . | What to Expect | What is Cloudera Ansible Foundry | What is Cloudera Deploy | . | Licensing, Warranties and Restrictions | Use Cases . | Simple Use Cases | Intermediate Use Cases | . | Framework Components . | cloudera-deploy | cldr-runner | cdpy | Cloudera Ansible Collections | Example Deployment Definitions | . | Getting Started . | Personas | . | . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/",
    "relUrl": "/docs/content/ansible_intro/"
  },"29": {
    "doc": "Operations & Troubleshooting",
    "title": "Ansible vs cloudera.cluster",
    "content": "Can’t detect the required Python library . This is most commonly caused by having more than the usual number of versions of Python installed. Very possibly you’ve ended up with 2.7, 3.6 and 3.8, or something like that, and the library you are looking for is in the wrong version. Remove extraneous versions, figure out what is installing them and prevent it. cannot dnf install with python2 . This usually happens when Ansible decides it simply must use /usr/bin/python even though a perfectly good python3 or other is preferred. Most commonly this is caused by legacy Ansible python version detection where it will always use /usr/bin/python if present. This is fixed by adding ansible_python_interpreter=auto to all your target hosts in your inventory, such as in the [deployment:vars] group. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_troubleshoot/#ansible-vs-clouderacluster",
    "relUrl": "/docs/content/ansible_troubleshoot/#ansible-vs-clouderacluster"
  },"30": {
    "doc": "Operations & Troubleshooting",
    "title": "Cloud Credentials",
    "content": "403 error from AWS . Could be a credential refreshing, retry to see if it’s isolated . Error like: Profile given for AWS was not found. Please fix and retry. This looks to be caused when an underlying call uses authentication from boto, rather than boto3 - and boto doesn’t support the new access scheme. Find the failing Task and report it to us. The SSO session associated with this profile has expired or is otherwise invalid . You need to relogin . no valid credential sources for Terraform AWS Provider found . Using Terraform for Cloudera Deploy, this typically means your SSO token has expired, relogin to continue . WSL2 - Browser does not launch . The browser may not open during the aws configure sso command if you have the DISPLAY environment variable set in WSL2. This is likely to happen if you run Linux GUI applications in WSL and don’t have a browser installed in the WSL distribution. The quick fix in this case is to unset DISPLAY and re-run the SSO configure command. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_troubleshoot/#cloud-credentials",
    "relUrl": "/docs/content/ansible_troubleshoot/#cloud-credentials"
  },"31": {
    "doc": "Operations & Troubleshooting",
    "title": "Docker Usage",
    "content": "docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? . You need to enable WSL integration in the Docker Desktop settings for Resources &gt; WSL Integration . SSH_AUTH_SOCK is empty or not set, unable to proceed. Exiting . Run the following command to activate the ssh-agent: . ---- eval `ssh-agent -s` ---- . If that command doesn’t work, try . ---- eval \"$(ssh-agent)\" ---- . docker: Error response from daemon: invalid mount config for type “bind”: field Source must not be empty. Probably your SSH_AUTH_SOCK is not populated from the ssh-agent. Same fix as above. The path /private/tmp/com.apple.launchd.[some random sting]/Listeners is not shared from the host and is not known to Docker. You need to add you /private directory to the file sharing in your Docker settings for Resources &gt; File Sharing . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_troubleshoot/#docker-usage",
    "relUrl": "/docs/content/ansible_troubleshoot/#docker-usage"
  },"32": {
    "doc": "Operations & Troubleshooting",
    "title": "Code signing for Cloudera Labs",
    "content": "GPG Signing is failing for my git commits . Could be lots of reasons, start by doing a commit on cmdline with GIT_TRACE=1 set to see the specific error. A common cause is your GPG Key expiring, to fix this on OSX: . | comment out no-tty in ~/.gnupg/gpg.conf | Follow https://stackoverflow.com/a/43728576/4717963[this process] on SO to reset your key expiry | Re-enable no-tty | . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_troubleshoot/#code-signing-for-cloudera-labs",
    "relUrl": "/docs/content/ansible_troubleshoot/#code-signing-for-cloudera-labs"
  },"33": {
    "doc": "Operations & Troubleshooting",
    "title": "Operations & Troubleshooting",
    "content": ". | Ansible vs cloudera.cluster | Cloud Credentials | Docker Usage | Code signing for Cloudera Labs | . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_troubleshoot/",
    "relUrl": "/docs/content/ansible_troubleshoot/"
  },"34": {
    "doc": "Coming Soon",
    "title": "Coming Soon",
    "content": ". ",
    "url": "http://10.15.4.152:9090/docs/development/",
    "relUrl": "/docs/development/"
  },"35": {
    "doc": "Setup ECS Client",
    "title": "Setup ECS Client",
    "content": "This article explains the step to setup the ECS environment so that user can administer the CDP PvC ECS cluster after successful installation. | Configure the ECS environment in the ECS master/server node. # cp /etc/rancher/rke2/rke2.yaml .kube/config # export PATH=$PATH:/var/lib/rancher/rke2/bin # kubectl get nodes NAME STATUS ROLES AGE VERSION ecsmaster1.cdpkvm.cldr Ready control-plane,etcd,master 120m v1.21.8+rke2r2 ecsworker1.cdpkvm.cldr Ready &lt;none&gt; 117m v1.21.8+rke2r2 ecsworker2.cdpkvm.cldr Ready &lt;none&gt; 117m v1.21.8+rke2r2 . | Amend the ~/.bash_profile login shell to include export PATH=$PATH:/var/lib/rancher/rke2/bin parameter to persist the environment setting. | . ",
    "url": "http://10.15.4.152:9090/docs/content/ecs_env/",
    "relUrl": "/docs/content/ecs_env/"
  },"36": {
    "doc": "Deploy Nvidia GPU in ECS",
    "title": "Install Nvidia Driver and Nvidia-container-runtime",
    "content": ". | Based on the Nvidia GPU card specification, browse the Nvidia site in order to check which software driver version to use. This demo uses Nvidia A100 GPU card and a check at the Nvidia site shows that version 515.65.01 is recommended. | Cordon the GPU worker node. # kubectl cordon ecsgpu.cdpkvm.cldr node/ecsgpu.cdpkvm.cldr cordoned . | In the ECS host/node installed with Nvidia GPU card, install the necessary OS software packages as described below and subsequently reboot the node. In this demo, the OS of the node is Centos7.9 and the hostname of the node with GPU card installed is ecsgpu.cdpkvm.cldr. # yum update -y # yum install -y tar bzip2 make automake gcc gcc-c++ pciutils elfutils-libelf-devel libglvnd-devel vim bind-utils wget # yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # yum -y group install \"Development Tools\" # yum install -y kernel-devel-$(uname -r) kernel-headers-$(uname -r) # reboot . | Subsequently, install the Nvidia driver and nvidia-container-runtime software by executing the following commands. # BASE_URL=https://us.download.nvidia.com/tesla # DRIVER_VERSION=515.65.01 # curl -fSsl -O $BASE_URL/$DRIVER_VERSION/NVIDIA-Linux-x86_64-$DRIVER_VERSION.run # sh NVIDIA-Linux-x86_64-$DRIVER_VERSION.run . | After successful installation, run the nvidia-smi tool and ensure the driver is deployed successfully by verifying the similar output as shown in the following example. [root@ecsgpu ~]# nvidia-smi Wed Aug 24 13:03:46 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 515.65.01 Driver Version: 515.65.01 CUDA Version: 11.7 |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | MIG M. |===============================+======================+======================| 0 NVIDIA A100-PCI... Off | 00000000:08:00.0 Off | 0 | N/A 32C P0 37W / 250W | 0MiB / 40960MiB | 3% Default | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | GPU GI CI PID Type Process name GPU Memory | ID ID Usage |=============================================================================| No running processes found | +-----------------------------------------------------------------------------+ [root@ecsgpu ~]# lsmod | grep nvidia nvidia_drm 53212 0 nvidia_modeset 1142094 1 nvidia_drm nvidia 40761292 1 nvidia_modeset drm_kms_helper 186531 3 qxl,nouveau,nvidia_drm drm 468454 7 qxl,ttm,drm_kms_helper,nvidia,nouveau,nvidia_drm [root@ecsgpu ~]# dmesg | grep nvidia [ 123.588172] nvidia: loading out-of-tree module taints kernel. [ 123.588182] nvidia: module license 'NVIDIA' taints kernel. [ 123.704411] nvidia: module verification failed: signature and/or required key missing - tainting kernel [ 123.802826] nvidia-nvlink: Nvlink Core is being initialized, major device number 239 [ 123.925577] nvidia-uvm: Loaded the UVM driver, major device number 237. [ 123.934813] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms 515.65.01 Wed Jul 20 13:43:59 UTC 2022 [ 123.940999] [drm] [nvidia-drm] [GPU ID 0x00000800] Loading driver [ 123.941018] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:08:00.0 on minor 1 [ 123.958317] [drm] [nvidia-drm] [GPU ID 0x00000800] Unloading driver [ 123.968642] nvidia-modeset: Unloading [ 123.978362] nvidia-uvm: Unloaded the UVM driver. [ 123.993831] nvidia-nvlink: Unregistered Nvlink Core, major device number 239 [ 137.450679] nvidia-nvlink: Nvlink Core is being initialized, major device number 240 [ 137.503657] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms 515.65.01 Wed Jul 20 13:43:59 UTC 2022 [ 137.508187] [drm] [nvidia-drm] [GPU ID 0x00000800] Loading driver [ 137.508190] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:08:00.0 on minor 1 [ 149.717193] nvidia 0000:08:00.0: irq 48 for MSI/MSI-X [ 149.717222] nvidia 0000:08:00.0: irq 49 for MSI/MSI-X [ 149.717248] nvidia 0000:08:00.0: irq 50 for MSI/MSI-X [ 149.717275] nvidia 0000:08:00.0: irq 51 for MSI/MSI-X [ 149.717301] nvidia 0000:08:00.0: irq 52 for MSI/MSI-X [ 149.717330] nvidia 0000:08:00.0: irq 53 for MSI/MSI-X . | Install the nvidia-container-runtime software package. Reboot the server. # curl -s -L https://nvidia.github.io/nvidia-container-runtime/$(. /etc/os-release;echo $ID$VERSION_ID)/nvidia-container-runtime.repo | sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo # yum -y install nvidia-container-runtime # rpm -qa | grep nvidia libnvidia-container-tools-1.11.0-1.x86_64 libnvidia-container1-1.11.0-1.x86_64 nvidia-container-toolkit-base-1.11.0-1.x86_64 nvidia-container-runtime-3.11.0-1.noarch nvidia-container-toolkit-1.11.0-1.x86_64 # nvidia-container-toolkit -version NVIDIA Container Runtime Hook version 1.11.0 commit: d9de4a0 # reboot . | Uncordon the GPU worker node. # kubectl uncordon ecsgpu.cdpkvm.cldr node/ecsgpu.cdpkvm.cldr cordoned . | . ",
    "url": "http://10.15.4.152:9090/docs/content/ecs_gpu/#install-nvidia-driver-and-nvidia-container-runtime",
    "relUrl": "/docs/content/ecs_gpu/#install-nvidia-driver-and-nvidia-container-runtime"
  },"37": {
    "doc": "Deploy Nvidia GPU in ECS",
    "title": "Nvidia GPU Card Testing and Verification in CML",
    "content": ". | Assuming the CDP PvC Data Services with ECS platform is already installed, SSH into the ECS master node and run the following command to ensure that ecsgpu.cdpkvm.cldr host has nvidia.com/gpu: field in the node specification. Host ecsgpu.cdpkvm.cldr is a typical ECS worker node without Nvidia GPU card installed. [root@ecsmaster1 ~]# kubectl describe node ecsgpu.cdpkvm.cldr | grep -A15 Capacity: Capacity: cpu: 16 ephemeral-storage: 209703916Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 263975200Ki nvidia.com/gpu: 1 pods: 110 Allocatable: cpu: 16 ephemeral-storage: 203999969325 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 263975200Ki nvidia.com/gpu: 1 pods: 110 [root@ecsmaster1 ~]# kubectl describe node ecsworker1.cdpkvm.cldr | grep -A13 Capacity: Capacity: cpu: 16 ephemeral-storage: 103797740Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 263974872Ki pods: 110 Allocatable: cpu: 16 ephemeral-storage: 100974441393 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 263974872Ki pods: 110 . | Assuming a CML workspace is already provisioned in the CDP PvC Data Services platform, navigate to Site Administration &gt; Runtime/Engine. Select the number for Maximum GPUs per Session/GPU. This procedure effectively allows the CML session to consume the GPU card. | Create a CML project and start a new session by selecting the Workbench editor with Python kernel alongside Nvidia GPU edition. Choose the number of GPU to use - in this demo, the quantity is 1. | Create a new Python file and run the following script. Also, open the terminal session and run nvidia-smi tool. Note that the output shows the Nvidia GPU card details. !pip3 install torch import torch torch.cuda.is_available() torch.cuda.device_count() torch.cuda.get_device_name(0) . | Navigate to the CML project main page and a check at the user resources dashboard displays the GPU card availability. | SSH into the ECS master node and run the following command to verify the node that hosting the above CML project session pod is ecsgpu.cdpkvm.cldr. [root@ecsmaster1 ~]# oc -n workspace1-user-1 describe pod wifz6t8mvxv5ghwy | grep Node: Node: ecsgpu.cdpkvm.cldr/10.15.4.185 [root@ecsmaster1 ~]# oc -n workspace1-user-1 describe pod wifz6t8mvxv5ghwy | grep -B2 -i nvidia Limits: memory: 7714196Ki nvidia.com/gpu: 1 -- cpu: 1960m memory: 7714196Ki nvidia.com/gpu: 1 -- . | When a process is consuming the Nvidia GPU, the output of nvidia-smi tool will show the PID of that process (in this case, the CML session pod). [root@ecsgpu ~]# nvidia-smi Thu Aug 25 13:58:40 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 515.65.01 Driver Version: 515.65.01 CUDA Version: 11.7 |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | MIG M. |===============================+======================+======================| 0 NVIDIA A100-PCI... Off | 00000000:08:00.0 Off | 0 | N/A 29C P0 35W / 250W | 39185MiB / 40960MiB | 0% Default | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | GPU GI CI PID Type Process name GPU Memory | ID ID Usage |=============================================================================| 0 N/A N/A 29990 C /usr/local/bin/python3.9 39183MiB | +-----------------------------------------------------------------------------+ . | In the event the ECS platform has no available worker node with GPU card, provisioning a session with GPU will result in Pending state as the system is looking for a worker node installed with at least one Nvidia GPU card. | . ",
    "url": "http://10.15.4.152:9090/docs/content/ecs_gpu/#nvidia-gpu-card-testing-and-verification-in-cml",
    "relUrl": "/docs/content/ecs_gpu/#nvidia-gpu-card-testing-and-verification-in-cml"
  },"38": {
    "doc": "Deploy Nvidia GPU in ECS",
    "title": "Deploy Nvidia GPU in ECS",
    "content": "This article describes the steps to install the Nvidia GPU software driver and its associated software in the CDP PvC Data Services platform with ECS solution. These implementation steps must be carried out after the node (with Nvidia GPU card) is added into the ECS platform/cluster. This article also describes the steps to test the GPU card in the CML workspace. | Install Nvidia Driver and Nvidia-container-runtime | Nvidia GPU Card Testing and Verification in CML | . ",
    "url": "http://10.15.4.152:9090/docs/content/ecs_gpu/",
    "relUrl": "/docs/content/ecs_gpu/"
  },"39": {
    "doc": "CDP PvC Data Services",
    "title": "Initialize Config",
    "content": "Add a _config.yml file to your docs directory. That can either be the repository root, or /docs. Specify this theme as the remote_theme. remote_theme: zendesk/jekyll-theme-zendesk-garden@main . Note, the @main version pin is required because GitHub pages currently only looks for the latest remote_theme on the remote repositories master branch. That’s it! This will setup the minimal theme to be active once you enable GitHub pages in your repo settings. Version Pinning . You can also specify a specific version if you don’t want a rebuild to change your theme without you knowing about it. remote_theme: zendesk/jekyll-theme-zendesk-garden@v0.3.0 . ",
    "url": "http://10.15.4.152:9090/docs/getting_started/#initialize-config",
    "relUrl": "/docs/getting_started/#initialize-config"
  },"40": {
    "doc": "CDP PvC Data Services",
    "title": "Additional Features",
    "content": "Sidebar Navigation and Search can be enabled through additional site configuration. ",
    "url": "http://10.15.4.152:9090/docs/getting_started/#additional-features",
    "relUrl": "/docs/getting_started/#additional-features"
  },"41": {
    "doc": "CDP PvC Data Services",
    "title": "CDP PvC Data Services",
    "content": " ",
    "url": "http://10.15.4.152:9090/docs/getting_started/",
    "relUrl": "/docs/getting_started/"
  },"42": {
    "doc": "Cloudera Labs",
    "title": "Cloudera Labs",
    "content": "Welcome to Cloudera Labs . ",
    "url": "http://10.15.4.152:9090/docs/",
    "relUrl": "/docs/"
  },"43": {
    "doc": "Search",
    "title": "Search",
    "content": "Site search can be configured in your _config.yml by setting search_enabled. search_enabled: true . Indexing Pages . For a page to be indexed, it must meet two criteria: . | It must produce HTML content. This means markdown/text/etc… files are included while things like asset files are not. | The pages front matter must include a non-empty title property. Example . --- id: my-doc title: My Searchable Documenet --- ## My Document . | . ",
    "url": "http://10.15.4.152:9090/docs/content/search/",
    "relUrl": "/docs/content/search/"
  }}

