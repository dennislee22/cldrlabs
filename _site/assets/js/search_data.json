{"0": {
    "doc": "Additional Config",
    "title": "Version",
    "content": "The layout includes an updated timestamp in the footer by default, but can also include a version string. This will automatically pull the tag_name from the latest release of the repository on GitHub. If there is no latest release or tag name, then it will fall back to the build_revision, i.e. the git SHA, of the commit that triggered the page build. If you don’t want this behavior, you can explicitly set the version attribute in your _config.yml file and it’s value will be displayed in the footer next to the updated timestamp. version: v1.2.3 . If you don’t want a version to be displayed at all, you can opt-out by setting version to `false. version: false . ",
    "url": "http://10.15.4.152:9090/docs/content/additional/#version",
    "relUrl": "/docs/content/additional/#version"
  },"1": {
    "doc": "Additional Config",
    "title": "Automatic Page Titles",
    "content": "The layout can automatically add a &lt;h1&gt; element at the top of the content containing the page’s title if it is present. To enable this, set auto_page_title to true in your _config.yml file. auto_page_title: true . Per-page Opt-out . If you don’t want this behavior on a specific page, you can add set auto_title to false in the front matter of the page. --- id: my-page title: My Title auto_title: false --- # My explicit title here . ",
    "url": "http://10.15.4.152:9090/docs/content/additional/#automatic-page-titles",
    "relUrl": "/docs/content/additional/#automatic-page-titles"
  },"2": {
    "doc": "Additional Config",
    "title": "Mermaid",
    "content": "This theme supports rendering of MermaidJS diagrams. It does so in a way that maintains compatibility with the native Mermaid diagram rendering in the Github UI. For example: . graph TD; A--&gt;B; A--&gt;C; B--&gt;D; C--&gt;D; . ",
    "url": "http://10.15.4.152:9090/docs/content/additional/#mermaid",
    "relUrl": "/docs/content/additional/#mermaid"
  },"3": {
    "doc": "Additional Config",
    "title": "Disabling Mermaid Support",
    "content": "If you don’t want to use Mermaid diagrams, you can set the mermaid_enabled option in your _config.yml file to false. ",
    "url": "http://10.15.4.152:9090/docs/content/additional/#mermaid-enabled",
    "relUrl": "/docs/content/additional/#mermaid-enabled"
  },"4": {
    "doc": "Additional Config",
    "title": "Customising the Mermaid theme",
    "content": "Mermaid supports a number of default themes. This can be configured using the mermaid_theme option in your _config.yml file. Note that this theme must be supported in the Mermaid configuration. ",
    "url": "http://10.15.4.152:9090/docs/content/additional/#mermaid-theme",
    "relUrl": "/docs/content/additional/#mermaid-theme"
  },"5": {
    "doc": "Additional Config",
    "title": "Additional Config",
    "content": "| Property | Default | . | version | site.github.latest_release.tag_name | . | auto_page_title | false | . | mermaid_enabled | true | . | meraid_theme | forest | . ",
    "url": "http://10.15.4.152:9090/docs/content/additional/",
    "relUrl": "/docs/content/additional/"
  },"6": {
    "doc": "Goal-oriented Guides",
    "title": "Enable notty for IDE signing",
    "content": "no-tty . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_developers/#enable-notty-for-ide-signing",
    "relUrl": "/docs/content/ansible_developers/#enable-notty-for-ide-signing"
  },"7": {
    "doc": "Goal-oriented Guides",
    "title": "Enable gpg to use the gpg-agent",
    "content": "use-agent . Configure IntelliJ to sign your commits .. This should be as simple as restarting your IDE once this process is complete .. Then when you make a new commit you can tick the box to ‘Sign-off commit’ in the dialog box . Configure vsCode to sign commits .. Find the following flag in the config and enable it git.enableCommitSigning . ==== Using the Ansible Runner Independent of Cloudera-Deploy . In order to minimise time spent on dependency management and troubleshooting issues arising from users on different systems, we provide a standardised container image. The image is prepared in such a way that you can use it as a shell, a python environment, a container on Kubernetes, within other CICD Frameworks, Ansible Tower, or simply as an ansible-runner.Testing: . | OSX Catalina 10.15.7 on MacBook Pro 2019 | Windows 10.0.19042 on Intel gaming rig (Tasteful RGB Edition) | .Manual Process: . To run this process on Windows you are expected to be within your xref:_install_windows_subsystem_for_linux_wsl2[WSL2] subsystem . Clone the Cloudera Labs Ansible Runner implementation repo into your preferred local Projects directory [source, bash] git clone https://github.com/cloudera-labs/cldr-runner.git &amp;&amp; cd cldr-runner . Linux only: Mark the run_project.sh script and build.sh script as executable [source, bash] chmod +x ./run_project.sh chmod +x ./build.sh . Ensure xref:_setup_docker[Docker] is running on your host machine . Copy the absolute path to the root of your code projects directory that contains the projects you want to execute within the container environment, e.g. /Users/dchaffelson/Projects . Launch the runner targeting the project you want to execute by passing the absolute path as the argument to the run_project.sh script, e.g./run_project.sh /Users/dchaffelson/Projects ** The script will build the container image from the latest release bits, this will take a few minutes the first time, the resulting image will be ~2GB ** You will then be dropped into a shell session in directory /runner in the container environment. Your Project will be mounted at /runner/project. You will have all the currently known dependencies for working with CDP pre-installed with conflicts resolved ** Note that the container must be stopped for a new project directory to be mounted to a new build, if there is already a container of the same name running you will just get a new shell session in it . At this point you may wish to install additional dependencies to the container, particularly those which may be behind VPN or on your corporate VCS. [source, bash] ansible-galaxy install -r project/deps/ansible-deps.yml pip install -r project/deps/python-deps.txt . NOTE: By default, the container is recreated if stopped, but it will not stop if you close your shell session as it is held open by a background tty. Try not to kill that. ==== Getting Started with Developing Collections . NOTE: You can skip this step if you only want to use the Collections to create your own playbooks. + This step is setting up the to Develop the Collections themselves. This will guide you through setting up a directory structure convenient for developing and executing the Collections within the Runner, or other execution environments. You only need to do this if you want to contribute directly to the Collections or Python clients underlying the interactions with Cloudera products - you do not need to go through this setup process if you simply wish to use cloudera-deploy with your own YAML Definitions, as the Collections and Clients should not need to be modified in those cases and are already pre-installed in the Runner.Why do it this way: Ansible expects to find collections within a path ‘collections/ansible_collections’ on a series of predefined or default paths within your environment. By default, the Runner has this Path variable prepopulated in a helpful fashion to the pre-installed Collections, this process guides you through modifying that to point at your own versions which you have to maintain yourself. For development purposes, creating this path in your favourite coding Projects directory, and then checking out the collections under it and renaming them to match the expected namespace may seem slightly arcane but it is the lowest-friction method for ongoing development we have found over many years of doing this.Process: . Make the directory tree Ansible expects in the same parent code directory that cloudera-deploy is in, e.g. [source, bash] cd cloudera-deploy &amp;&amp; mkdir -p ../ansible_dev/collections/ansible_collections/cloudera . ** cloudera is the base namespace of our collection ** Your Projects directory should also have your Ansible Playbooks and other codebase in it, so that you can mount the root of it to the Runner and have access to all your codebase, e.g. ~/Projects/cloudera-deploy should be where Cloudera-Deploy is located . Fork each of the sub-collections and cdpy into your personal github, and replace with your actual github account below . Checkout each of the sub-collections into this folder, e.g.: [source, bash] cd ~/Projects git clone -b devel https://github.com//cdpy.git cdpy cd ansible_dev/collections/ansible_collections/cloudera git clone -b devel https://github.com//cloudera.exe.git exe git clone -b devel https://github.com//cloudera.cloud.git cloud git clone -b devel https://github.com//cloudera.cluster.git cluster + NOTE: The cloned directories above must be named \"exe\", \"cloud\" and \"cluster\", respectively. Ensure you specify the directory name as the last parameter in the command line, as shown above. Each of the subcollections should be on the ‘devel’ branch so you can PR them back them with your changes + . Your Code Project directory should now look something like this: [source,bash] /Projects/ansible_dev/collections/ansible_collections/cloudera /Projects/ansible_dev/collections/ansible_collections/cloudera/exe /Projects/ansible_dev/collections/ansible_collections/cloudera/cloud /Projects/ansible_dev/collections/ansible_collections/cloudera/cluster /Projects/cdpy /Projects/cloudera-deploy . Before you invoke quickstart.sh, set the environment variable below to tell Ansible where to find your code inside your execution environment once it is mounted in the Container at /runner/project: [source, bash] export CLDR_COLLECTION_PATH=\"ansible_dev/collections\" export CLDR_PYTHON_PATH=/runner/project/cdpy/src + NOTE: You might want to set this in your bash or zsh profile on your local machine so it is persistent . Then, when you run quickstart.sh in cloudera-deploy, it will pick up this extra Collection location and add cdpy to the PYTHONPATH, and use these instead of the release versions basked into the Container . You can confirm this is working by running this inside the Runner [source, bash] ansible-galaxy collection list . It should look something like: [source, bash] —- . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_developers/#enable-gpg-to-use-the-gpg-agent",
    "relUrl": "/docs/content/ansible_developers/#enable-gpg-to-use-the-gpg-agent"
  },"8": {
    "doc": "Goal-oriented Guides",
    "title": "/runner/project/ansible_dev/collections/ansible_collections",
    "content": "Collection Version —————- ——- cloudera.cloud 0.1.0 cloudera.cluster 2.0.0 cloudera.exe 0.0.1 Cloudera.runtime 0.0.1 . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_developers/#runnerprojectansible_devcollectionsansible_collections",
    "relUrl": "/docs/content/ansible_developers/#runnerprojectansible_devcollectionsansible_collections"
  },"9": {
    "doc": "Goal-oriented Guides",
    "title": "/home/runner/.ansible/collections/ansible_collections",
    "content": "Collection Version ——————– ——- amazon.aws 1.4.0 ansible.netcommon 1.5.0 ansible.posix 1.1.1 azure.azcollection 1.4.0 community.aws 1.4.0 community.crypto 1.4.0 community.general 2.1.1 community.mysql 1.2.0 community.postgresql 1.1.1 google.cloud 1.0.2 netapp.azure 21.3.0 —- . If you see duplication of collections because you are using the runner AND mounting your own versions, you probably have not activated the CLDR_COLLECTION_PATH variable correctly, and thus quickstart.sh is not picking it up. As another test, you should also be able to invoke python inside the container and use cdpy [source,python] from cdpy.cdpy import Cdpy c = Cdpy() c.iam.get_user() . To test that cdpy is present and you can access your account as on cmdline . You may now edit the collections or cdpy and have the changes immediately available for use within the Runner, which is an awful lot easier than having to compile and crossload them after every change. ==== Install Dependencies without using Runner We prefer that you use the Runner, because it sets many defaults to avoid common issues and thus save you and us a lot of issue reproduction time. However, we understand that there are many situations where it may not be appropriate, such as air-gapped environments, or when you want to run the install locally on the hardware and not have a separate ansible controller. Create a new virtualenv, or activate an existing one, we do not recommend installing dependencies in system python on most OS. Install dependencies for your hosting infrastructure version following the pathway laid out in the https://github.com/cloudera-labs/cldr-runner/blob/main/Dockerfile[Dockerfile] in ansible-runner Install any additional dependencies you may have . NOTE: THe Dockerfile resolves combined dependencies for all our Hybrid Cloud Deployments, you probably only need a subset for your environment. ==== Developing within the Runner While we recommend using the Runner as your execution environment when doing development, actually developing directly against the Runner using something like Visual Studio may not be a great idea due to the file system latency commonly encountered. Generally when the maintainers work with this system we are editing the files directly on our host system using our IDE, and those files are RW mounted into the container for execution via the /runner/project mechanism, which then does not noticeably incur any performance degradation. ==== Working with AWS SSO .Why: Traditionally AWS users would use static keys, but more recently using temporary credentials via SSO is more commonplace. The upside of AWS SSO is better credential management, the downside being additional complexity, reliance on the fairly awful AWSCLIv2, and a lack of OOTB automation integration.Setup AWS SSO: . Install awscli v2 on your local machine [source,bash] brew install awscli . Check you have version 2 (currently 2.5.4) [source,bash] aws –version . Login to AWS SSO via Okta (or however), examine the list of accounts and select the one you want to set up, copy the name to use as your AWS Profile name . Setup and login to AWS SSO for your selected account [source,bash] aws configure sso –profile .. Enter your Start URL, e.g. https://&lt;app-name&gt;.awsapps.com/start .. SSO Region, e.g. us-east-1 .. It’ll launch your browser (which is why we do it on your local machine) .. Complete the login process to authenticate the session .. Select which AWS account you wish to set up .. Set your default region for this profile, e.g. us-east-1 .. Set your default output for this profile, e.g. json . Now whenever you run a deployment you must set the profile name to match the one you have setup, e.g. in definition.yml [source,yaml] aws_profile: my-aws-profile . NOTE: You will likely have to re-login to SSO before each run . ==== Working with named credential profiles . For CDP, AWS, and other credential-based access controls it is common to have multiple accounts that may be used from time to time. As such, it is useful to be able to easily, safely, and precisely switch between these accounts. This is typically achieved through use of multiple named profiles, a good introductory guide to this is https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html[here] for AWS. To set up a named profile for CDP, follow https://docs.cloudera.com/cdp/latest/cli/topics/mc-cli-generating-an-api-access-key.html[this] guide to create an API access key, and then https://docs.cloudera.com/cdp/latest/cli/topics/mc-configuring-cdp-client-with-the-api-access-key.html[this] guide to configure your local credentials by simply adding the --profile &lt;profile name&gt; flag to the commands . Credentials are typically stored in a dot-prefix directory in the user profile on Linux, and in the user home directory on Windows, e.g. ~/.aws/credentials, ~/.cdp/credentials etc. They may be directly edited in these files if you prefer instead of using the configure CLI commands. By default, Cloudera-Deploy will use the default profiles for each account touched, you can set the Definition yaml keys cdp_profile: &lt;profile name&gt; and/or aws_profile: &lt;profile name&gt; to use a specific named profile instead.Things to know: . | There should always be a default credential which points to a ‘safe’ account for when you deliberately or accidentally run commands without specifying a profile. This is generally a dev account, or even an expired credential, to ensure you don’t accidentally delete prod. | It is typical with CDP Public Cloud to pair a CDP Tenant with specific Cloud Infrastructure accounts to avoid confusion, therefore if you have a dev and prod CDP tenant with profiles of the same name, we recommend the paired cloud infrastructure accounts are also named dev and prod. | . ==== Using a Jumpbox . A common deployment scenario is to funnel all cluster access through a jump/bastion host. In this case, there are three possibilities: . To run the Ansible Runner from the jump host itself . To deploy the dependencies within the boundary of the Jump Host . To run the Ansible Runner locally and tunnel connections through the jump host. In scenario 3, the following will be necessary to tunnel both SSH connection and HTTP calls through the jump host.HTTP In the runner, edit /runner/env/envvars and add http_proxy=&lt;proxy&gt; where is the name:port of your http proxy (e.g. a SOCKs proxy running on localhost). Alternatively, edit quickstart.sh to pass this value through from your local machine if it is available.SSH In your inventory file, under the [deployment:vars] group, add the following variable to set additional arguments on the SSH command used by Ansible. [source,bash] ansible_ssh_common_args=’-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=”ssh -W %h:%p -q \"' . Optionally, in your SSH config file (e.g. ~/.ssh/config) you can configure an alias with predefined parameters for the jump host. This makes it easier to manage between different deployments and makes the argument string easier to read. [source,bash] Host myJump IdentityFile ~/.ssh/myKey StrictHostKeyChecking = no User myUser HostName jump.host.name . With this SSH config the proxy string would look like this: [source,bash] ansible_ssh_common_args=’-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ProxyCommand=”ssh -W %h:%p -q myJump”’ . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_developers/#homerunneransiblecollectionsansible_collections",
    "relUrl": "/docs/content/ansible_developers/#homerunneransiblecollectionsansible_collections"
  },"10": {
    "doc": "Goal-oriented Guides",
    "title": "Goal-oriented Guides",
    "content": ". | Enable notty for IDE signing | Enable gpg to use the gpg-agent | /runner/project/ansible_dev/collections/ansible_collections | /home/runner/.ansible/collections/ansible_collections | . === Detailed Setup for Developers . This document is structured with suggested sequences of steps at the top, and the various processes you need underneath. Use the suggested sequences or create your own as appropriate. Many of the Processes in here have been come to over years of trying different approaches, in the main our philosophy has been to adopt the least-worst option, minimising user surprise in the least number of the simplest possible steps, even if that results in a more complex implementation for us in code where few users ever tread. We welcome all suggestions and discussions for improvement, but if you’re going to tell us we’re ‘using Docker Wrong’ then we’d greatly appreciate an explanation on how you’d improve it. ==== Developers setup on OSX . The Developer onboarding is aimed at those who want to modify the automation tooling itself to change behaviors.The following chapters of this documentation provide the necessary process: . Prepare your xref:_install_homebrew_and_git_on_osx[OSX Deps] . Follow the xref:_main_setup_process[Main Setup] . (Optional) Setup xref:_setup_gpg_commit_signing[Commit Signing] for Contributions . (Optional) Get setup to Develop the xref:_getting_started_with_developing_collections[Collections] themselves . ==== Developer setup on Windows . Because Windows already needs to set up Linux compatibility to make containers work, we’re just going to leverage that across the board for simplicity. IMPORTANT: You must use WSL2 or you are going to get some really weird errors. We assume you have a recent version of 64bit Windows 10 with Administrator access and virtualization enabled, if you are restricted by your org from doing this then they should have a replacement process as this is pretty standard stuff for developers. Install xref:_install_windows_subsystem_for_linux_wsl2[WSL2] . Handle xref:_handle_line_endings_on_windows[Line Endings] on Windows . Follow the xref:_main_setup_process[Main Setup] . (Optional) Setup xref:_setup_gpg_commit_signing[Commit Signing] for Contributions . (Optional) Get setup to Develop the xref:_getting_started_with_developing_collections[Collections] themselves . === Process Guides . ==== Main Setup Process . This Setup Process is simply the https://github.com/cloudera-labs/cloudera-deploy/blob/main/readme.adoc#2-quickstart[Quickstart] from the Cloudera-Deploy Readme with more steps and explanations. ===== Prerequisites . NOTE: It is easiest to test your credentials within the Runner, as the dependencies are already installed for you . Cloud Credentials (CDP Public) and/or Host Infrastructure (CDP Private) ** If you are deploying to a public cloud, you will need an active credential for that provider with rights to deploy resources for the Deployment you specify. ** Unless otherwise instructed your ‘default’ credential will be used for the cloud provider. ** You can check this with aws sts get-caller-identity and comparing outputs to the AWS UI, if using AWS ** For Azure, consider az account list, or if you need to refresh your credentials, az login. ** For GCP, check that your Service Account credential is being picked up in the Cloudera Deploy Profile, and then test with gcloud auth list . CDP Credentials (CDP Public) ** If you are deploying CDP Public Cloud, you will need an active credential within a CDP Tenant. ** Unless otherwise instructed, your ‘default’ credential and tenant will be used. ** You can check this with cdp iam get-user and comparing outputs to the CDP UI . Code Deps: .. Docker and Git installed locally (quick option)(«_setup_docker,See Setup Docker») .. or - Ability to install Python and CLI dependencies on the machine of your choice (slow option)(See xref:_manual_ansible_controller_setup_on_centos7[Centos7 Script] for an example) . ===== Get the Repo . Open a terminal and switch to your preferred projects directory [source,bash] cd ~/Projects . Clone or update the repo [source,bash] git clone https://github.com/cloudera-labs/cloudera-deploy.git &amp;&amp; cd cloudera-deploy &amp;&amp; chmod +x ./quickstart.sh . NOTE: If you already have the repo cloned, don’t forget to xref:_refresh_your_cloudera_deploy_local_repo[update it] . ===== Get the Runner . Have xref:_setup_docker[Docker] Installed and running . (Optional) Set your provider to download a reduced image and save disk space .. AWS [source,bash] export provider=aws .. Azure [source,bash] export provider=azure .. GCP [source,bash] export provider=gcp .. Default is all of them, but it’s a ~2.3GB image download . Run the quickstart script in the Repo you just cloned above ** Option A: Run the quickstart bare, it will mount the parent directory of the cloudera-deploy repo, which will be your code directory if you followed the directions above, e.g. ~/Projects [source,bash] ./quickstart.sh . ** Option B: Include an explicit path on your local machine, it will be mounted in the /runner/project folder in the container. If your code or Definitions are not in this path you may not be able to access them in the container. ** Don’t worry if you get it wrong, just stop the container and rerun quickstart and it’ll make a new one with the new mount. [source,bash] ./quickstart.sh ~/code . This will drop you into a new shell in the container with all dependencies resolved for working with CDP and Cloud Infrastructure, you’ll know it has worked because the shell prompt is Orange with cldr &lt;version&gt; #&gt; ** It will automatically load your local machine user profile so you have access to your credentials, ssh keys, etc. ** If you run the quickstart script again, it’ll simply create another bash session on the container, providing useful parallelism ** If you stop the container, the next time you run quickstart it will be updated and recreated, so any changes within the container filesystem and not persisted back to your Project directory or mounted user profile will be lost ** As long as you run commands from within the /runner path, it will log your Ansible back to ~/.config/cloudera-deploy/log . If you already have a CDP Credential in your local user profile, you can test it with [source,bash] cdp iam get-user .. It will use the default CDP credential, or you can use a different profile by setting the CDP_PROFILE environment variable, or setting cdp_profile in your cloudera-deploy Definition files.. You should compare the UUID of your user returned by this command in the terminal with the UUID of your user reported in the User Profile in the CDP UI so you are certain that you are deploying to the expected Tenant . Check you have a credential for your chosen Cloud Infrastructure Provider. The default is AWS, and again you can provide a specific profile or use your default. You can check it by running [source,bash] aws sts get-caller-identity .. You should likewise compare the Account ID reported here with the Account ID in the AWS IAM UI to ensure you are targeting the expected Account. This is similar for other providers. ===== Prepare your Profiles to run a Deployment NOTE: that you should execute any Ansible commands from /runner in the Runner, as it has all the defaults set for you and it may fail to find dependencies otherwise. NOTE: If you have different settings for different deployments you can create additional profile files under the directory above to store the different configurations. To use an alternative cloudera-deploy profile, specify the -e profile=&lt;profile_name&gt; option when running the ansible-playbook command. Edit the default user Profile to personalise the Password, Namespace, SSH Keys, etc. for your Deployments. Note that this file is created the first time you run the quickstart. [source,bash] vi ~/.config/cloudera-deploy/profiles/default . You will need CDP Credentials, and Credentials for your target Infrastructure of choice. Fortunately the Runner has most of these dependencies available to you .. CDP with pre-issued Keys and optional profile [source,bash] cdp configure –profile default .. AWS with pre-issued Keys [source,bash] aws configure –profile default . ** AWS SSO requires awscliv2 which is not installed in the Runner by default .. Azure via interactive login [source,bash] az login .. Google Cloud via init [source,bash] gcloud init . ===== Deployment Run Commands . Provided you have completed the prerequisites to set up a cloud provider credential, and CDP Public Cloud credential, and your Cloudera-Deploy Profile, the following command creates a default Sandbox with CDP Public &amp; Private Cloud in your default CDP Tenant and Cloud Infra Provider with no further interaction from the user: [source,bash] ansible-playbook project/cloudera-deploy/main.yml -e “definition_path=examples/sandbox” -t run,default_cluster . NOTE: So that is three dependencies, and two commands, and you have a complete Hybrid Cloud Data Platform . The command is structured typically for an ansible-playbook. | If you have used quickstart.sh to mount a local project directory with your definitions.yml and application.yml into the runner container (as explained in the steps in Get the Runner above), then you won’t have your own main.yml close to hand. You can instead use the default main.yml in /opt/cloudera-deploy/, and reference your project dir at /runner/project: [source,bash] ansible-playbook /opt/cloudera-deploy/main.yml -e “definition_path=/runner/project/\" -t . | Note that we pass in some ansible ‘extra variables’ using the -e flag. The only required variable points to the definition_path which contains the files that describe the deployment you want. You can provide many more extra vars, or even files directly on the command-line per usual Ansible options. | Note that we pass in Ansible Tags with -t, in this case the tags instruct Cloudera-Deploy to build CDP Public Cloud to the ‘Runtimes’ level, and also deploy a ‘default’ or basic CDP Base Cluster on EC2 machines in the same VPC. There are many other tags that may be used to control behavior, and are explained elsewhere. | .Here are additional commands which will come in handy: . | Teardown and delete everything related to this definition: [source,bash] ansible-playbook project/cloudera-deploy/main.yml -e “definition_path=examples/sandbox” -t teardown | . WARNING: This will teardown everything related to this definition and name_prefix, make sure that is actually what you want to be doing before running it. | Just deploy a CDP Public Datalake: [source,bash] ansible-playbook project/cloudera-deploy/main.yml -e “definition_path=examples/sandbox” -t plat | . NOTE: This uses the same definition, but then uses a different Ansible Tag to only deploy part of it, more explanation of the Definitions and Tags will follow elsewhere in the Architecture Docs. | Just deploy CDP Private Cluster Trial on Public Cloud Infra: [source,bash] ansible-playbook project/cloudera-deploy/main.yml -e “definition_path=examples/sandbox” -t infra,default_cluster | . NOTE: This leverages the dynamic inventory feature to make a simple cluster on EC2 instances on AWS without the user needing to learn how first, and is very handy for trials and platform testing . ==== Refresh your Cloudera-Deploy local repo If you have previously used Cloudera-Deploy but haven’t touched it in a while, here is a guide to refreshing your setup and getting results quickly . [source,bash] cd cloudera-deploy git fetch –all git pull docker stop $(docker ps -qa) ./quickstart.sh cdp iam get-user aws iam get-user . ==== Manual Ansible Controller setup on Centos7 . We provide an example script for initialising an Ansible Controller on a Centos7 box https://github.com/cloudera-labs/cloudera-deploy/blob/main/centos7-init.sh[here] . ==== Install Homebrew and Git on OSX .Install XCode command line tools [source,bash] xcode-select –install .Install Homebrew [source,bash] /bin/bash -c “$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)” .Install git (through Homebrew) [source,bash] brew install git . If you are going to use AWS SSO, you may also with to install awscliv2 on your local machine [source, bash] brew install awscli@2 . ==== Setup Docker .Guide for Windows Follow the instructions provided by https://docs.docker.com/docker-for-windows/install/[Docker] . NOTE: You want to be using WSL2, make sure Docker can see the underlying Ubuntu (or similar) for execution. You are advised to stick to the guide here, as we have found Windows throws some intractable filesystem and networking errors with creative linux-on-Windows setups for Docker and SSH.Guide for OSX Follow the instructions provided by https://docs.docker.com/docker-for-mac/install/[Docker] . ==== Install Windows Subsystem for Linux (WSL2) .There’s a lot of guides on how to do this, here’s a summary: . Enable Developer mode (on older versions of Win10) .. Windows Settings &gt; Update &amp; Security .. Select the Developer Mode radio button .. You may not need to do this, don’t worry if it’s not there and the rest of the process works, as the latest releases don’t require you to do this to install Linux .. If you did have to enable it, you may have to reboot (yay Windows) . Enable Windows Subsystem for Linux v2 .. Control Panel &gt; Programs &amp; Features &gt; Turn Windows Features on and off .. Tick the box for ‘Windows Subsystem for Linux’ .. Make sure you either setup WSL2 from the start, or do the upgrade process. WSL1 has some strange behaviors .. You’ll probably have to reboot. Yay Windows! . Install Ubuntu 18 (other distros untested, including Ubuntu 20) .. Try to do it from the Microsoft Store … Open the store (search store in launch bar) … Search for ‘Linux’ in the store … Select and install Ubuntu .. If you can’t do it from the store, try this in the cmd.exe prompt lxrun /install .. It’ll ask you to set a username and password, keep it short and simple, doesn’t have to match your actual Windows user . You may have to set this as the default bash environment as Docker for Windows likes to steal it away after updates .. List the currently installed distros by opening cmd.exe .. wsl --list --all .. Set ubuntu as default if it is not .. wsl -s Ubuntu . ==== Handle line-endings on Windows Windows defaults to a different line ending standard than Linux/OSX. NOTE: You only need to follow this step if you plan on editing the code on Windows. The Cloudera Labs repos standardise on linux line endings, the easiest way to handle this on Windows involves basically two steps. Set git to always not use Windows line endings, so it doesn’t rewrite your files when you checkout .. git config --global core.autocrlf false .. Set your IDE to use Linux line endings, if not for everything then at least the Cloudera Labs projects .. In Pycharm, this is in File &gt; Settings &gt; Editor &gt; Code Style &gt; General &gt; Line Separator: set to ‘Unix and macOS (\\n) . ==== Setup GPG commit signing . NOTE: You can skip this step if you are not planning on contributing code .DCO: Cloudera-Labs uses the Developer Certificate of Origin (DCO) approach to open source contribution, by signing your commits you are attesting that you are allowed to submit them by whoever owns your work (you, or your employer). We also require commit signing to validate the supply chain on our code.Background: There is a good explanation https://nifi.apache.org/gpg.html[here], it also covers setup for machines with other OS. This subguide assumes that you want to set up your development machine so that it automatically signs all your commits without bothering you too much. If you are just checking out the code to inspect it and not for contributing back to the community, then you can skip this step. You may want to modify this process to ask you for your passphrase or manually sign commits each time at your preference.References: https://nifi.apache.org/gpg.html + https://stackoverflow.com/a/46884134 + https://superuser.com/a/954536 + https://withblue.ink/2020/05/17/how-and-why-to-sign-git-commits.html + .Testing: OSX Catalina 10.15.7 on MacBook Pro 2019 .Process: . Update or install xref:_install_homebrew_and_git_on_osx[Homebrew] . Install dependencies [source,bash] brew install gpg2 pinentry-mac &amp;&amp; brew install –cask gpg-suite . ** gpg-suite is not strictly necessary, but it makes it easier to integrate signing with IDEs like IntelliJ as it helps you manage the passphrase in your OSX keychain ** Pinentry-mac makes it easy to sign commits within your IDE, like Pycharm, without having to always commit via terminal . Create a directory for gnupg to store details [source,bash] mkdir ~/.gnupg . Put the following in ~/.gnupg/gpg-agent.conf [source,bash] default-cache-ttl 600 max-cache-ttl 7200 default-cache-ttl-ssh 600 max-cache-ttl-ssh 7200 pinentry-program /usr/local/bin/pinentry-mac . Enable it in your user profile such as ~/.bash_profile or ~/.zprofile [source,bash] export GPG_TTY=$(tty) gpgconf –launch gpg-agent . Set correct permissions on your gnupg user directory [source,bash] chown -R $(whoami) ~/.gnupg/ find ~/.gnupg -type f -exec chmod 600 {} \\; find ~/.gnupg -type d -exec chmod 700 {} \\; . Generate yourself a key [source,bash] gpg –full-gen-key .. Key type 4 .. Keysize 4096 .. Expiration 1y or 2y or whatever .. Your real name, or github username .. Your real email address, or the one github recognises as yours in your Settings .. A Passphrase - don’t forget it, and make sure it is strong . Verify your key is created and stored [source,bash] gpg2 –list-secret-keys –keyid-format SHORT . ** Copy your key ID, it’ll look something like rsa4096/674CB45A ** You want the second bit, 674CB45A . Test your key can be used [source,bash] echo “hello world” | gpg2 –clearsign . ** You’ll have to enter your passphrase to sign it, then it’ll print the encrypted message . You may want to add multiple email addresses to the key signing, such as your open source email and/or your employer email and/or your personal email .. Open your key for editing [source,bash] gpg2 –edit-key .. Then use the adduid command [source, bash] adduid .. Enter the identity Name and Email as before .. Then update the trust for the new identity [source, bash] uid 2 trust . ** You probably want trust 5 .. Save to exit [source, bash] save . Configure Github to recognise your signed commits .. Set your git email for making commits [source, bash] git config –global user.email your@email.com . *** This email must be one of those in your GPG key set earlier .. Export your public key for uploading to Github [source, bash] gpg2 –armor –export .. Copy everything including the following lines into your paste buffer [source, bash] —–BEGIN PGP PUBLIC KEY BLOCK—– … —–END PGP PUBLIC KEY BLOCK—– . Open github and go to your key settings https://github.com/settings/keys . Add new GPG key, paste in your PGP block from your buffer . Configure git to autosign your commits [source, bash] git config –global gpg.program gpg git config –global user.signingkey git config --global commit.gpgSign true git config --global tag.gpgSign true . Put the following in ~/.gnupg/gpg.conf [source, bash] . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_developers/",
    "relUrl": "/docs/content/ansible_developers/"
  },"11": {
    "doc": "Introduction and User-orientation",
    "title": "Ansible Automation for Cloudera Products",
    "content": "AKA: Cloudera Deploy | Cloudera AutoProv | Cloudera Ansible Foundry | Cloudera Playbooks . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#ansible-automation-for-cloudera-products",
    "relUrl": "/docs/content/ansible_intro/#ansible-automation-for-cloudera-products"
  },"12": {
    "doc": "Introduction and User-orientation",
    "title": "What to Expect",
    "content": "These docs are for new users and power users alike to leverage Cloudera’s Ansible implementation for deployment and automation of Cloudera Products within the wider Hybrid Data Cloud ecosystem. It is generally broken into several sections: . For all users: These introductory notes to orient new users . – A discussion of Use Cases that the Framework supports . – A Getting Started guide based around user goals and skill levels . – An Overview of preparing Definitions for Deployment . For Developers: – A History and guiding principles . – A breakdown of Framework Components . – An in-depth Setup Guide . – Logical breakdowns of key Workflows within the Playbooks . – A Reference guide to all Definition options . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#what-to-expect",
    "relUrl": "/docs/content/ansible_intro/#what-to-expect"
  },"13": {
    "doc": "Introduction and User-orientation",
    "title": "What is Cloudera Ansible Foundry",
    "content": "This framework is generally called the Cloudera Ansible Foundry, and it wraps software dependencies, automation code, examples, and documentation into an integrated family. It further provides a consistent and portable toolkit for working in Python, Bash, Terraform, various CLIs, language clients, ad-hoc commands, and other commonly used tools in the Hybrid Cloud ecosystem. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#what-is-cloudera-ansible-foundry",
    "relUrl": "/docs/content/ansible_intro/#what-is-cloudera-ansible-foundry"
  },"14": {
    "doc": "Introduction and User-orientation",
    "title": "What is Cloudera Deploy",
    "content": "Cloudera Deploy is the name of the reference implementation in Ansible Playbooks for automating Cloudera Products. For most users, Cloudera-Deploy is the entrypoint and only component they need interact with, but underneath it there is an extensible framework of components for handling various scenarios for power-users. To that end, you can use Cloudera-Deploy itself directly, or use it as a starting point for your own implementation, or simply use the components within for your own purposes. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#what-is-cloudera-deploy",
    "relUrl": "/docs/content/ansible_intro/#what-is-cloudera-deploy"
  },"15": {
    "doc": "Introduction and User-orientation",
    "title": "Licensing, Warranties and Restrictions",
    "content": "Cloudera-Deploy and the underlying framework are all open source, mostly under the Apache 2.0 license or compatible licenses. The software is provided without Warranty or Guarantee of Support - it therefore differs from, and is complementary to, Software provided by Cloudera or other Vendors under commercial agreements. However, while we do not guarantee support, Cloudera use this software along with our partners and customers, and thus strive to maintain it to the same high standards of our other products in the best spirit of community and partnership. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#licensing-warranties-and-restrictions",
    "relUrl": "/docs/content/ansible_intro/#licensing-warranties-and-restrictions"
  },"16": {
    "doc": "Introduction and User-orientation",
    "title": "Use Cases",
    "content": "Simple Use Cases . These simple use cases can be run from the Cloudera-Deploy Quickstart without requiring additional skills in advance beyond use of cmdline, docker, and cloud credentials . – Deploy Reference Architectures for CDP Public on AWS, GCP, or Azure . – Deploying from OSX, Windows10, or Linux machines . – Leverage Ansible or Terraform for Cloud Infrastructure . – Deploy CDP Base clusters to most supported OS targets . – Deploy Applications on various CDP implementations . – Teardown and other lifecycling task . Intermediate Use Cases . In these cases, we expect that you are a more experienced DevOps user with particular goals in mind and at a good working knowledge of the technologies in play . – Deploy CDP Private Cloud on a Kubernetes Variant . – Deploy Applications across multiple CDP Hybrid Cloud Environments . – Integrate a 3rd party component into the Framework . – Create a mirror of archive.cloudera.com or custom offline repo of parcels . – Use the framework within Ansible Tower, or via CICD integration . – Handle partial deployments or partial teardowns using run levels . – Automate deployment to adopted infrastructure . – Extend the Ansible Collections to handle additional cases . – Force remove cloud infrastructure within a given namespace . Framework Components . Here we will give more of a sense of what is in each of the repositories within the framework . cloudera-deploy . This is the standard entry-point for most users, most of the time. It contains the main Ansible Playbooks, default User Profile, Definition handling, and Run initialization. It also usually contains other ease-of-use features for new users, like the current Dynamic Inventory implementation, until such time as they are matured into their own separate feature. cldr-runner . A Container image as a common Runner with all dependencies suitable for use locally, with Ansible Tower, against an IDE, or various other circumstances . It is based in a centos8 image produced by the Ansible Community, known as ‘Ansible Runner’. We then resolve the difficulty of conflicting dependencies to layer in the various clients, python modules, utilities, and other bits and pieces that make it a useful shell or remote execution environment for working with Hybrid Cloud. You can also use this as a template for your own specific implementation, but we ask Users to adopt it as their default if they don’t have a reason not too for the simple benefit of not reinventing something that works well and reduces the burden of reproducing errors. cdpy . A Python client for the CDP Control Plane, both Public and Private. This is essentially a convenience wrapper for CDPCLI, which itself is based in a fork of AWSCLI and fully written in Python. cdpy contains a large number of helper methods which are reused throughout the cloudera.cloud modules, as well as well-structured Ansible-friendly error handling. Cloudera Ansible Collections . cloudera.cloud This Collection is primarily un-opinionated modules for the CDP Control Plane in Public or Private Cloud. The point of keeping the modules unopinionated and solely covering the CDP Control Plane interactions is to minimise the dependencies and attack surface for users who aren’t using Public Cloud. cloudera.exe This Collection is highly opinionated Roles for most task sequences around achieving some run-level or dependency satisfaction. The concept of run-level deployments in cloudera.exe is explained later in the Architecture. cloudera.cluster This Collection is focused on Deploying &amp; Configuring Clusters via Cloudera Manager - usually traditional Cloudera clusters. It is backwards compatible with the Cloudera Playbooks written for Ansible &lt;2.9, whereas this Collection is for &gt;2.10 . Example Deployment Definitions . Example Definitions for use with Cloudera-Deploy, presently bundled here . We plan on publishing more examples soon. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#use-cases",
    "relUrl": "/docs/content/ansible_intro/#use-cases"
  },"17": {
    "doc": "Introduction and User-orientation",
    "title": "Getting Started",
    "content": "Cloudera-Deploy is designed for use both internally and externally by Cloudera Engineering, Testing, Education, Support, Field, Sales, Marketing and other staff, but also the same code and artefacts are used by Customers, Partners, Resellers, and our Community. As such, we have set it up so a new user can get started in as little as 3 steps, while not limiting Power Users from leveraging the full suite of capabilities. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#getting-started",
    "relUrl": "/docs/content/ansible_intro/#getting-started"
  },"18": {
    "doc": "Introduction and User-orientation",
    "title": "Prerequisites",
    "content": "While Cloudera-Deploy aims to remove as many complexities for the new user, there are still a few dependencies like authentication and runtimes required for it to work. The quickstart guide contains extra prerequisite steps that you are advised to follow if this is your first time using new credentials or tools such as AWS CLI. ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#prerequisites",
    "relUrl": "/docs/content/ansible_intro/#prerequisites"
  },"19": {
    "doc": "Introduction and User-orientation",
    "title": "Understanding the Skills Gap",
    "content": "We recognise there is a significant jump in the skills and knowledge required to go from using the pre-configured push-button examples in the ‘Simple’ section to more complex cases the ‘Intermediate’ section and beyond. There are several good introductory courses online for Ansible and Terraform use in Hybrid Cloud Architectures. Cloudera also offers a training program tailored to these tools to interested Customers and Partners. You can also reach out to your Cloudera contacts for assistance in these deployments at any time. Personas . New to Automation Using Cloudera-Deploy without modifying it only requires a couple of steps and some cloud credentials. It is designed to be extremely accessible to new users. If this is your first time, we suggest running the Cloudera-Deploy Quickstart using one of the prebuilt Definitions before diving in further. Skills required: commandline, editing text files, cloud credential management . Create your own Definitions Crafting Definitions is just editing text files to declare what the Deployment should look like, and allowing Cloudera-Deploy to interpret and produce what you have described. The files are written in YAML with a simple structure. Once you have worked out how to run a basic Deployment via the Quickstart, you may wish to customise it to meet your requirements. A summary of editing Definitions is included in the Getting Started sections for CDP Public Cloud and CDP Private Cloud. There is also a detailed explanation of how Definitions work in the Deployments Reference in the Developer’s Guide, though reading the rest of these Docs and examining the other Definition examples may also be helpful. Additional skills required: YAML editing and formatting, Cloudera-Deploy Definitions management . Create your own Playbooks Creating your own Playbooks requires a working understanding of authoring Ansible tasks and how to use Modules and Roles from Ansible Collections. It goes a step beyond editing YAML declarations into understanding the sequences of steps necessary to achieve a given outcome in a hopefully robust manner. We will soon be publishing more examples of Application Playbooks on top of Cloudera-Deploy, but you can review the Playbooks already included as a starting point. We suggest you start by fully reading this documentation including the Developer sections and how the existing Workflows are structured. Additional skills required: Ansible Task development, use of Ansible Collections, advanced use of Ansible Tags, experience with multi-tier automation abstractions . Extend the Framework If you are familiar with Ansible and Cloudera Products, and are considering adding Ansible modules or Roles to the Collections, then you may wish to follow the Developer Setup to fork and checkout the Framework components so you can make (and perhaps contribute back) your own changes. Additional skills required: Python development, Ansible test frameworks and debugging, Docker Containers . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/#understanding-the-skills-gap",
    "relUrl": "/docs/content/ansible_intro/#understanding-the-skills-gap"
  },"20": {
    "doc": "Introduction and User-orientation",
    "title": "Introduction and User-orientation",
    "content": ". | Ansible Automation for Cloudera Products | What to Expect | What is Cloudera Ansible Foundry | What is Cloudera Deploy | Licensing, Warranties and Restrictions | Use Cases . | Simple Use Cases | Intermediate Use Cases | Framework Components | cloudera-deploy | cldr-runner | cdpy | Cloudera Ansible Collections | Example Deployment Definitions | . | Getting Started | Prerequisites | Understanding the Skills Gap . | Personas | . | . ",
    "url": "http://10.15.4.152:9090/docs/content/ansible_intro/",
    "relUrl": "/docs/content/ansible_intro/"
  },"21": {
    "doc": "Coming Soon",
    "title": "Coming Soon",
    "content": ". ",
    "url": "http://10.15.4.152:9090/docs/development/",
    "relUrl": "/docs/development/"
  },"22": {
    "doc": "Setup ECS Client",
    "title": "Setup ECS Client",
    "content": "This article explains the step to setup the ECS environment so that user can administer the CDP PvC ECS cluster after successful installation. | Configure the ECS environment in the ECS master/server node. # cp /etc/rancher/rke2/rke2.yaml .kube/config # export PATH=$PATH:/var/lib/rancher/rke2/bin # kubectl get nodes NAME STATUS ROLES AGE VERSION ecsmaster1.cdpkvm.cldr Ready control-plane,etcd,master 120m v1.21.8+rke2r2 ecsworker1.cdpkvm.cldr Ready &lt;none&gt; 117m v1.21.8+rke2r2 ecsworker2.cdpkvm.cldr Ready &lt;none&gt; 117m v1.21.8+rke2r2 . | Amend the ~/.bash_profile login shell to include export PATH=$PATH:/var/lib/rancher/rke2/bin parameter to persist the environment setting. | . ",
    "url": "http://10.15.4.152:9090/docs/content/ecs_env/",
    "relUrl": "/docs/content/ecs_env/"
  },"23": {
    "doc": "Deploy Nvidia GPU in ECS",
    "title": "Install Nvidia Driver and Nvidia-container-runtime",
    "content": ". | Based on the Nvidia GPU card specification, browse the Nvidia site in order to check which software driver version to use. This demo uses Nvidia A100 GPU card and a check at the Nvidia site shows that version 515.65.01 is recommended. | Cordon the GPU worker node. # kubectl cordon ecsgpu.cdpkvm.cldr node/ecsgpu.cdpkvm.cldr cordoned . | In the ECS host/node installed with Nvidia GPU card, install the necessary OS software packages as described below and subsequently reboot the node. In this demo, the OS of the node is Centos7.9 and the hostname of the node with GPU card installed is ecsgpu.cdpkvm.cldr. # yum update -y # yum install -y tar bzip2 make automake gcc gcc-c++ pciutils elfutils-libelf-devel libglvnd-devel vim bind-utils wget # yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm # yum -y group install \"Development Tools\" # yum install -y kernel-devel-$(uname -r) kernel-headers-$(uname -r) # reboot . | Subsequently, install the Nvidia driver and nvidia-container-runtime software by executing the following commands. # BASE_URL=https://us.download.nvidia.com/tesla # DRIVER_VERSION=515.65.01 # curl -fSsl -O $BASE_URL/$DRIVER_VERSION/NVIDIA-Linux-x86_64-$DRIVER_VERSION.run # sh NVIDIA-Linux-x86_64-$DRIVER_VERSION.run . | After successful installation, run the nvidia-smi tool and ensure the driver is deployed successfully by verifying the similar output as shown in the following example. [root@ecsgpu ~]# nvidia-smi Wed Aug 24 13:03:46 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 515.65.01 Driver Version: 515.65.01 CUDA Version: 11.7 |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | MIG M. |===============================+======================+======================| 0 NVIDIA A100-PCI... Off | 00000000:08:00.0 Off | 0 | N/A 32C P0 37W / 250W | 0MiB / 40960MiB | 3% Default | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | GPU GI CI PID Type Process name GPU Memory | ID ID Usage |=============================================================================| No running processes found | +-----------------------------------------------------------------------------+ [root@ecsgpu ~]# lsmod | grep nvidia nvidia_drm 53212 0 nvidia_modeset 1142094 1 nvidia_drm nvidia 40761292 1 nvidia_modeset drm_kms_helper 186531 3 qxl,nouveau,nvidia_drm drm 468454 7 qxl,ttm,drm_kms_helper,nvidia,nouveau,nvidia_drm [root@ecsgpu ~]# dmesg | grep nvidia [ 123.588172] nvidia: loading out-of-tree module taints kernel. [ 123.588182] nvidia: module license 'NVIDIA' taints kernel. [ 123.704411] nvidia: module verification failed: signature and/or required key missing - tainting kernel [ 123.802826] nvidia-nvlink: Nvlink Core is being initialized, major device number 239 [ 123.925577] nvidia-uvm: Loaded the UVM driver, major device number 237. [ 123.934813] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms 515.65.01 Wed Jul 20 13:43:59 UTC 2022 [ 123.940999] [drm] [nvidia-drm] [GPU ID 0x00000800] Loading driver [ 123.941018] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:08:00.0 on minor 1 [ 123.958317] [drm] [nvidia-drm] [GPU ID 0x00000800] Unloading driver [ 123.968642] nvidia-modeset: Unloading [ 123.978362] nvidia-uvm: Unloaded the UVM driver. [ 123.993831] nvidia-nvlink: Unregistered Nvlink Core, major device number 239 [ 137.450679] nvidia-nvlink: Nvlink Core is being initialized, major device number 240 [ 137.503657] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver for UNIX platforms 515.65.01 Wed Jul 20 13:43:59 UTC 2022 [ 137.508187] [drm] [nvidia-drm] [GPU ID 0x00000800] Loading driver [ 137.508190] [drm] Initialized nvidia-drm 0.0.0 20160202 for 0000:08:00.0 on minor 1 [ 149.717193] nvidia 0000:08:00.0: irq 48 for MSI/MSI-X [ 149.717222] nvidia 0000:08:00.0: irq 49 for MSI/MSI-X [ 149.717248] nvidia 0000:08:00.0: irq 50 for MSI/MSI-X [ 149.717275] nvidia 0000:08:00.0: irq 51 for MSI/MSI-X [ 149.717301] nvidia 0000:08:00.0: irq 52 for MSI/MSI-X [ 149.717330] nvidia 0000:08:00.0: irq 53 for MSI/MSI-X . | Install the nvidia-container-runtime software package. Reboot the server. # curl -s -L https://nvidia.github.io/nvidia-container-runtime/$(. /etc/os-release;echo $ID$VERSION_ID)/nvidia-container-runtime.repo | sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo # yum -y install nvidia-container-runtime # rpm -qa | grep nvidia libnvidia-container-tools-1.11.0-1.x86_64 libnvidia-container1-1.11.0-1.x86_64 nvidia-container-toolkit-base-1.11.0-1.x86_64 nvidia-container-runtime-3.11.0-1.noarch nvidia-container-toolkit-1.11.0-1.x86_64 # nvidia-container-toolkit -version NVIDIA Container Runtime Hook version 1.11.0 commit: d9de4a0 # reboot . | Uncordon the GPU worker node. # kubectl uncordon ecsgpu.cdpkvm.cldr node/ecsgpu.cdpkvm.cldr cordoned . | . ",
    "url": "http://10.15.4.152:9090/docs/content/ecs_gpu/#install-nvidia-driver-and-nvidia-container-runtime",
    "relUrl": "/docs/content/ecs_gpu/#install-nvidia-driver-and-nvidia-container-runtime"
  },"24": {
    "doc": "Deploy Nvidia GPU in ECS",
    "title": "Nvidia GPU Card Testing and Verification in CML",
    "content": ". | Assuming the CDP PvC Data Services with ECS platform is already installed, SSH into the ECS master node and run the following command to ensure that ecsgpu.cdpkvm.cldr host has nvidia.com/gpu: field in the node specification. Host ecsgpu.cdpkvm.cldr is a typical ECS worker node without Nvidia GPU card installed. [root@ecsmaster1 ~]# kubectl describe node ecsgpu.cdpkvm.cldr | grep -A15 Capacity: Capacity: cpu: 16 ephemeral-storage: 209703916Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 263975200Ki nvidia.com/gpu: 1 pods: 110 Allocatable: cpu: 16 ephemeral-storage: 203999969325 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 263975200Ki nvidia.com/gpu: 1 pods: 110 [root@ecsmaster1 ~]# kubectl describe node ecsworker1.cdpkvm.cldr | grep -A13 Capacity: Capacity: cpu: 16 ephemeral-storage: 103797740Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 263974872Ki pods: 110 Allocatable: cpu: 16 ephemeral-storage: 100974441393 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 263974872Ki pods: 110 . | Assuming a CML workspace is already provisioned in the CDP PvC Data Services platform, navigate to Site Administration &gt; Runtime/Engine. Select the number for Maximum GPUs per Session/GPU. This procedure effectively allows the CML session to consume the GPU card. | Create a CML project and start a new session by selecting the Workbench editor with Python kernel alongside Nvidia GPU edition. Choose the number of GPU to use - in this demo, the quantity is 1. | Create a new Python file and run the following script. Also, open the terminal session and run nvidia-smi tool. Note that the output shows the Nvidia GPU card details. !pip3 install torch import torch torch.cuda.is_available() torch.cuda.device_count() torch.cuda.get_device_name(0) . | Navigate to the CML project main page and a check at the user resources dashboard displays the GPU card availability. | SSH into the ECS master node and run the following command to verify the node that hosting the above CML project session pod is ecsgpu.cdpkvm.cldr. [root@ecsmaster1 ~]# oc -n workspace1-user-1 describe pod wifz6t8mvxv5ghwy | grep Node: Node: ecsgpu.cdpkvm.cldr/10.15.4.185 [root@ecsmaster1 ~]# oc -n workspace1-user-1 describe pod wifz6t8mvxv5ghwy | grep -B2 -i nvidia Limits: memory: 7714196Ki nvidia.com/gpu: 1 -- cpu: 1960m memory: 7714196Ki nvidia.com/gpu: 1 -- . | When a process is consuming the Nvidia GPU, the output of nvidia-smi tool will show the PID of that process (in this case, the CML session pod). [root@ecsgpu ~]# nvidia-smi Thu Aug 25 13:58:40 2022 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 515.65.01 Driver Version: 515.65.01 CUDA Version: 11.7 |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | MIG M. |===============================+======================+======================| 0 NVIDIA A100-PCI... Off | 00000000:08:00.0 Off | 0 | N/A 29C P0 35W / 250W | 39185MiB / 40960MiB | 0% Default | | Disabled | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | GPU GI CI PID Type Process name GPU Memory | ID ID Usage |=============================================================================| 0 N/A N/A 29990 C /usr/local/bin/python3.9 39183MiB | +-----------------------------------------------------------------------------+ . | In the event the ECS platform has no available worker node with GPU card, provisioning a session with GPU will result in Pending state as the system is looking for a worker node installed with at least one Nvidia GPU card. | . ",
    "url": "http://10.15.4.152:9090/docs/content/ecs_gpu/#nvidia-gpu-card-testing-and-verification-in-cml",
    "relUrl": "/docs/content/ecs_gpu/#nvidia-gpu-card-testing-and-verification-in-cml"
  },"25": {
    "doc": "Deploy Nvidia GPU in ECS",
    "title": "Deploy Nvidia GPU in ECS",
    "content": "This article describes the steps to install the Nvidia GPU software driver and its associated software in the CDP PvC Data Services platform with ECS solution. These implementation steps must be carried out after the node (with Nvidia GPU card) is added into the ECS platform/cluster. This article also describes the steps to test the GPU card in the CML workspace. | Install Nvidia Driver and Nvidia-container-runtime | Nvidia GPU Card Testing and Verification in CML | . ",
    "url": "http://10.15.4.152:9090/docs/content/ecs_gpu/",
    "relUrl": "/docs/content/ecs_gpu/"
  },"26": {
    "doc": "CDP PvC Data Services",
    "title": "Initialize Config",
    "content": "Add a _config.yml file to your docs directory. That can either be the repository root, or /docs. Specify this theme as the remote_theme. remote_theme: zendesk/jekyll-theme-zendesk-garden@main . Note, the @main version pin is required because GitHub pages currently only looks for the latest remote_theme on the remote repositories master branch. That’s it! This will setup the minimal theme to be active once you enable GitHub pages in your repo settings. Version Pinning . You can also specify a specific version if you don’t want a rebuild to change your theme without you knowing about it. remote_theme: zendesk/jekyll-theme-zendesk-garden@v0.3.0 . ",
    "url": "http://10.15.4.152:9090/docs/getting_started/#initialize-config",
    "relUrl": "/docs/getting_started/#initialize-config"
  },"27": {
    "doc": "CDP PvC Data Services",
    "title": "Additional Features",
    "content": "Sidebar Navigation and Search can be enabled through additional site configuration. ",
    "url": "http://10.15.4.152:9090/docs/getting_started/#additional-features",
    "relUrl": "/docs/getting_started/#additional-features"
  },"28": {
    "doc": "CDP PvC Data Services",
    "title": "CDP PvC Data Services",
    "content": " ",
    "url": "http://10.15.4.152:9090/docs/getting_started/",
    "relUrl": "/docs/getting_started/"
  },"29": {
    "doc": "Cloudera Labs",
    "title": "Cloudera Labs",
    "content": "Welcome to Cloudera Labs . ",
    "url": "http://10.15.4.152:9090/docs/",
    "relUrl": "/docs/"
  },"30": {
    "doc": "Relative Navigation",
    "title": "Relative Navigation",
    "content": "Relative navigation links can be added at the end of the content for any page. In the front matter of any page, set previous_page or next_page to the id of the page to link to. The label will behave similar to the sidebar nav label. If sidebar_label is set, that will be used otherwise the title will be used. Example . --- id: page-2 title: Page 2 previous_page: page-1 next_page: page-3 --- . ",
    "url": "http://10.15.4.152:9090/docs/content/relative_nav/",
    "relUrl": "/docs/content/relative_nav/"
  },"31": {
    "doc": "Search",
    "title": "Search",
    "content": "Site search can be configured in your _config.yml by setting search_enabled. search_enabled: true . Indexing Pages . For a page to be indexed, it must meet two criteria: . | It must produce HTML content. This means markdown/text/etc… files are included while things like asset files are not. | The pages front matter must include a non-empty title property. Example . --- id: my-doc title: My Searchable Documenet --- ## My Document . | . ",
    "url": "http://10.15.4.152:9090/docs/content/search/",
    "relUrl": "/docs/content/search/"
  },"32": {
    "doc": "Sidebar Navigation",
    "title": "Sidebar Navigation",
    "content": "A navigation sidebar can be configured in your `_config.yml` by adding the `sidebar` property. Sidebar navigation supports direct links or a single level of nesting with collapsible categories. To support including pages in the sidebar navigation panel, the page must have [front matter](https://docs.github.com/en/github/working-with-github-pages/about-github-pages-and-jekyll#front-matter) that contains an `id`. ### Root Sidebar Items To reference a page at the sidebar root, directly use its `id` in the list of sidebar items. #### Example ##### `getting_started.md` ``` --- id: getting-started --- ## Some Instructions ... ``` ##### `_config.yml` ```yaml sidebar: - getting-started ``` ### Collapsible Sidebar Items To create a collapsible list of pages in the sidebar, create an entry with `label` and `children`. The `children` property must then contain a list of document `id`s to be included in that section. #### Example ```yaml sidebar: - label: Section One children: - document-one - document-two - document-three ``` ### Mixed Sidebar Types You can mix both root level links and collapsible items in a sidebar. #### Example ```yaml sidebar: - getting-started - label: Dev Docs children: - doc-1 - doc-2 - deployment ``` ### Label Customization By default, the `title` of a page will be used as the label in the sidebar. If you do not specify a `title` in your documents [front matter](https://docs.github.com/en/github/working-with-github-pages/about-github-pages-and-jekyll#front-matter) then the first heading in the document will be extracted using the [`jekyll-titles-from-headings`](https://github.com/benbalter/jekyll-titles-from-headings) plugin (which GitHub automatically includes). If you want to customize the sidebar label further, you can do so by setting a `sidebar_label` attribute in the [front matter](https://docs.github.com/en/github/working-with-github-pages/about-github-pages-and-jekyll#front-matter). ``` --- id: my-document title: This is a really long title that is bad for a sidebar sidebar_label: Long Title --- ## Some header that is also a bad sidebar title ``` ",
    "url": "http://10.15.4.152:9090/docs/content/sidebar/",
    "relUrl": "/docs/content/sidebar/"
  }}

