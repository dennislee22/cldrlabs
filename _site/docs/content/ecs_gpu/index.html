<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Deploy Nvidia GPU in ECS</title>
    <link
      rel="stylesheet"
      href="/assets/css/style.css"
    />

    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">

    <link
      rel="stylesheet"
      href="/assets/css/rouge.css"
    />
    <script src="/assets/js/accordian.js"></script>
    <script src="/assets/js/heading_links.js"></script>
    
    <script src="/assets/js/vendor/lunr.min.js"></script>
    <script src="/assets/js/search.js"></script>
    
  </head>

  <body>
    <div class="chrome">
      <div class="chrome:nav">
      <img src="/assets/images/cloudera_logo_WHITE.png" alt="Link to GitHub Repository">
  
</div>

      <div class="chrome:body">
        <header class="chrome:header">
  <div class="chrome:header_title">
    <a href="/" class="text-grey-800 no-underline hover:no-underline focus:no-underline">
      <span class="text-3xl">Cloudera Labs</span>
    </a>
  </div>
   <div class="chrome:header_item">
  <svg xmlns="http://www.w3.org/2000/svg" width="26" height="26" focusable="false" viewBox="0 0 26 26">
  <g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2">
    <circle cx="11" cy="11" r="7"/>
    <path d="M16 16l6 6"/>
  </g>
</svg>

  <form id="search">
    <input
      type="text"
      id="search_input"
      name="search_query"
      class="search_input"
      placeholder="Search Cloudera Labs"
      autocomplete="off"
    />
  </form>
</div>
 
</header>

        <div class="chrome:content">
          <div class="chrome:main">
            
            
            <h1 id="deploy-nvidia-gpu-in-ecs">Deploy Nvidia GPU in ECS</h1>
            
            
            <p>This article describes the steps to install the Nvidia GPU software driver and its associated software in the CDP PvC Data Services platform with ECS solution. These implementation steps must be carried out after the node (with Nvidia GPU card) is added into the ECS platform/cluster. This article also describes the steps to test the GPU card in the CML workspace.</p>

<ul id="markdown-toc">
  <li><a href="#install-nvidia-driver-and-nvidia-container-runtime" id="markdown-toc-install-nvidia-driver-and-nvidia-container-runtime">Install Nvidia Driver and Nvidia-container-runtime</a></li>
  <li><a href="#nvidia-gpu-card-testing-and-verification-in-cml" id="markdown-toc-nvidia-gpu-card-testing-and-verification-in-cml">Nvidia GPU Card Testing and Verification in CML</a></li>
</ul>

<hr />

<h2 id="install-nvidia-driver-and-nvidia-container-runtime">Install Nvidia Driver and Nvidia-container-runtime</h2>

<ol>
  <li>
    <p>Based on the Nvidia GPU card specification, browse the Nvidia <a href="https://www.nvidia.com/Download/index.aspx?lang=en-us">site</a> in order to check which software driver version to use. This demo uses Nvidia A100 GPU card and a check at the Nvidia site shows that version <code class="language-plaintext highlighter-rouge">515.65.01</code> is recommended.</p>

    <p><img src="../../assets/images/gpu/nvidiaecs1.png" alt="" /></p>

    <p><img src="../../assets/images/gpu/nvidiaecs2.png" alt="" /></p>
  </li>
  <li>
    <p>Cordon the GPU worker node.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># kubectl cordon ecsgpu.cdpkvm.cldr </span>
 node/ecsgpu.cdpkvm.cldr cordoned
</code></pre></div>    </div>
  </li>
  <li>
    <p>In the ECS host/node installed with Nvidia GPU card, install the necessary OS software packages as described below and subsequently reboot the node. In this demo, the OS of the node is Centos7.9 and the hostname of the node with GPU card installed is <code class="language-plaintext highlighter-rouge">ecsgpu.cdpkvm.cldr</code>.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># yum update -y</span>

 <span class="c"># yum install -y tar bzip2 make automake gcc gcc-c++ pciutils elfutils-libelf-devel libglvnd-devel vim bind-utils wget</span>

 <span class="c"># yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm</span>

 <span class="c"># yum -y group install "Development Tools"</span>

 <span class="c"># yum install -y kernel-devel-$(uname -r) kernel-headers-$(uname -r)</span>
    
 <span class="c"># reboot</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Subsequently, install the Nvidia driver and <code class="language-plaintext highlighter-rouge">nvidia-container-runtime</code> software by executing the following commands.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># BASE_URL=https://us.download.nvidia.com/tesla</span>
 <span class="c"># DRIVER_VERSION=515.65.01</span>
 <span class="c"># curl -fSsl -O $BASE_URL/$DRIVER_VERSION/NVIDIA-Linux-x86_64-$DRIVER_VERSION.run</span>
 <span class="c"># sh NVIDIA-Linux-x86_64-$DRIVER_VERSION.run</span>
</code></pre></div>    </div>

    <p><img src="../../assets/images/gpu/nvidiaecs3.png" alt="" /></p>

    <p><img src="../../assets/images/gpu/nvidiaecs4.png" alt="" /></p>

    <p><img src="../../assets/images/gpu/nvidiaecs5.png" alt="" /></p>
  </li>
  <li>
    <p>After successful installation, run the <code class="language-plaintext highlighter-rouge">nvidia-smi</code> tool and ensure the driver is deployed successfully by verifying the similar output as shown in the following example.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">[</span>root@ecsgpu ~]# nvidia-smi    
 Wed Aug 24 13:03:46 2022       
 +-----------------------------------------------------------------------------+
 | NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
 |-------------------------------+----------------------+----------------------+
 | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
 | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
 |                               |                      |               MIG M. |
 |<span class="o">===============================</span>+<span class="o">======================</span>+<span class="o">======================</span>|
 |   0  NVIDIA A100-PCI...  Off  | 00000000:08:00.0 Off |                    0 |
 | N/A   32C    P0    37W / 250W |      0MiB / 40960MiB |      3%      Default |
 |                               |                      |             Disabled |
 +-------------------------------+----------------------+----------------------+
                                                                               
 +-----------------------------------------------------------------------------+
 | Processes:                                                                  |
 |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
 |        ID   ID                                                   Usage      |
 |<span class="o">=============================================================================</span>|
 |  No running processes found                                                 |
 +-----------------------------------------------------------------------------+
    
 <span class="o">[</span>root@ecsgpu ~]# lsmod | <span class="nb">grep </span>nvidia
 nvidia_drm             53212  0 
 nvidia_modeset       1142094  1 nvidia_drm
 nvidia              40761292  1 nvidia_modeset
 drm_kms_helper        186531  3 qxl,nouveau,nvidia_drm
 drm                   468454  7 qxl,ttm,drm_kms_helper,nvidia,nouveau,nvidia_drm
    
 <span class="o">[</span>root@ecsgpu ~]# dmesg | <span class="nb">grep </span>nvidia
 <span class="o">[</span>  123.588172] nvidia: loading out-of-tree module taints kernel.
 <span class="o">[</span>  123.588182] nvidia: module license <span class="s1">'NVIDIA'</span> taints kernel.
 <span class="o">[</span>  123.704411] nvidia: module verification failed: signature and/or required key missing - tainting kernel
 <span class="o">[</span>  123.802826] nvidia-nvlink: Nvlink Core is being initialized, major device number 239
 <span class="o">[</span>  123.925577] nvidia-uvm: Loaded the UVM driver, major device number 237.
 <span class="o">[</span>  123.934813] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver <span class="k">for </span>UNIX platforms  515.65.01  Wed Jul 20 13:43:59 UTC 2022
 <span class="o">[</span>  123.940999] <span class="o">[</span>drm] <span class="o">[</span>nvidia-drm] <span class="o">[</span>GPU ID 0x00000800] Loading driver
 <span class="o">[</span>  123.941018] <span class="o">[</span>drm] Initialized nvidia-drm 0.0.0 20160202 <span class="k">for </span>0000:08:00.0 on minor 1
 <span class="o">[</span>  123.958317] <span class="o">[</span>drm] <span class="o">[</span>nvidia-drm] <span class="o">[</span>GPU ID 0x00000800] Unloading driver
 <span class="o">[</span>  123.968642] nvidia-modeset: Unloading
 <span class="o">[</span>  123.978362] nvidia-uvm: Unloaded the UVM driver.
 <span class="o">[</span>  123.993831] nvidia-nvlink: Unregistered Nvlink Core, major device number 239
 <span class="o">[</span>  137.450679] nvidia-nvlink: Nvlink Core is being initialized, major device number 240
 <span class="o">[</span>  137.503657] nvidia-modeset: Loading NVIDIA Kernel Mode Setting Driver <span class="k">for </span>UNIX platforms  515.65.01  Wed Jul 20 13:43:59 UTC 2022
 <span class="o">[</span>  137.508187] <span class="o">[</span>drm] <span class="o">[</span>nvidia-drm] <span class="o">[</span>GPU ID 0x00000800] Loading driver
 <span class="o">[</span>  137.508190] <span class="o">[</span>drm] Initialized nvidia-drm 0.0.0 20160202 <span class="k">for </span>0000:08:00.0 on minor 1
 <span class="o">[</span>  149.717193] nvidia 0000:08:00.0: irq 48 <span class="k">for </span>MSI/MSI-X
 <span class="o">[</span>  149.717222] nvidia 0000:08:00.0: irq 49 <span class="k">for </span>MSI/MSI-X
 <span class="o">[</span>  149.717248] nvidia 0000:08:00.0: irq 50 <span class="k">for </span>MSI/MSI-X
 <span class="o">[</span>  149.717275] nvidia 0000:08:00.0: irq 51 <span class="k">for </span>MSI/MSI-X
 <span class="o">[</span>  149.717301] nvidia 0000:08:00.0: irq 52 <span class="k">for </span>MSI/MSI-X
 <span class="o">[</span>  149.717330] nvidia 0000:08:00.0: irq 53 <span class="k">for </span>MSI/MSI-X
</code></pre></div>    </div>
  </li>
  <li>
    <p>Install the <code class="language-plaintext highlighter-rouge">nvidia-container-runtime</code> software package. Reboot the server.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># curl -s -L https://nvidia.github.io/nvidia-container-runtime/$(. /etc/os-release;echo $ID$VERSION_ID)/nvidia-container-runtime.repo |  sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo</span>
    
 <span class="c"># yum -y install nvidia-container-runtime</span>
    
 <span class="c"># rpm -qa | grep nvidia</span>
 libnvidia-container-tools-1.11.0-1.x86_64
 libnvidia-container1-1.11.0-1.x86_64
 nvidia-container-toolkit-base-1.11.0-1.x86_64
 nvidia-container-runtime-3.11.0-1.noarch
 nvidia-container-toolkit-1.11.0-1.x86_64
    
 <span class="c"># nvidia-container-toolkit -version</span>
 NVIDIA Container Runtime Hook version 1.11.0
 commit: d9de4a0

 <span class="c"># reboot </span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Uncordon the GPU worker node.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># kubectl uncordon ecsgpu.cdpkvm.cldr </span>
 node/ecsgpu.cdpkvm.cldr cordoned
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="nvidia-gpu-card-testing-and-verification-in-cml">Nvidia GPU Card Testing and Verification in CML</h2>

<ol>
  <li>
    <p>Assuming the CDP PvC Data Services with ECS platform is already installed, SSH into the ECS master node and run the following command to ensure that <code class="language-plaintext highlighter-rouge">ecsgpu.cdpkvm.cldr</code> host has <code class="language-plaintext highlighter-rouge">nvidia.com/gpu:</code> field in the node specification. Host <code class="language-plaintext highlighter-rouge">ecsgpu.cdpkvm.cldr</code> is a typical ECS worker node without Nvidia GPU card installed.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">[</span>root@ecsmaster1 ~]# kubectl describe node ecsgpu.cdpkvm.cldr | <span class="nb">grep</span> <span class="nt">-A15</span> Capacity:
 Capacity:
     cpu:                16
     ephemeral-storage:  209703916Ki
     hugepages-1Gi:      0
     hugepages-2Mi:      0
     memory:             263975200Ki
     nvidia.com/gpu:     1
     pods:               110
 Allocatable:
     cpu:                16
     ephemeral-storage:  203999969325
     hugepages-1Gi:      0
     hugepages-2Mi:      0
     memory:             263975200Ki
     nvidia.com/gpu:     1
     pods:               110
  
 <span class="o">[</span>root@ecsmaster1 ~]# kubectl describe node ecsworker1.cdpkvm.cldr | <span class="nb">grep</span> <span class="nt">-A13</span> Capacity:
 Capacity:
     cpu:                16
     ephemeral-storage:  103797740Ki
     hugepages-1Gi:      0
     hugepages-2Mi:      0
     memory:             263974872Ki
     pods:               110
 Allocatable:
     cpu:                16
     ephemeral-storage:  100974441393
     hugepages-1Gi:      0
     hugepages-2Mi:      0
     memory:             263974872Ki
     pods:               110
</code></pre></div>    </div>
  </li>
  <li>
    <p>Assuming a CML workspace is already provisioned in the CDP PvC Data Services platform, navigate to <code class="language-plaintext highlighter-rouge">Site Administration</code> &gt; <code class="language-plaintext highlighter-rouge">Runtime/Engine</code>. Select the number for <code class="language-plaintext highlighter-rouge">Maximum GPUs per Session/GPU</code>. This procedure effectively allows the CML session to consume the GPU card.</p>

    <p><img src="../../assets/images/gpu/cmlgpu1.png" alt="" /></p>
  </li>
  <li>
    <p>Create a CML project and start a new session by selecting the <code class="language-plaintext highlighter-rouge">Workbench</code> editor with Python kernel alongside <code class="language-plaintext highlighter-rouge">Nvidia GPU</code> edition. Choose the number of GPU to use - in this demo, the quantity is 1.</p>

    <p><img src="../../assets/images/gpu/gpuecssession1.png" alt="" /></p>
  </li>
  <li>
    <p>Create a new Python file and run the following script. Also, open the terminal session and run <code class="language-plaintext highlighter-rouge">nvidia-smi</code> tool. Note that the output shows the Nvidia GPU card details.</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="kt">!pip3</span> <span class="s">install torch</span>
 <span class="s">import torch</span>
    
 <span class="s">torch.cuda.is_available()</span>
 <span class="s">torch.cuda.device_count()</span>
 <span class="s">torch.cuda.get_device_name(0)</span>
</code></pre></div>    </div>

    <p><img src="../../assets/images/gpu/gpuecssession2.png" alt="" /></p>
  </li>
  <li>
    <p>Navigate to the CML project main page and a check at the user resources dashboard displays the GPU card availability.</p>

    <p><img src="../../assets/images/gpu/gpuecssession3.png" alt="" /></p>

    <p><img src="../../assets/images/gpu/gpuecssession4.png" alt="" /></p>
  </li>
  <li>
    <p>SSH into the ECS master node and run the following command to verify the node that hosting the above CML project session pod is <code class="language-plaintext highlighter-rouge">ecsgpu.cdpkvm.cldr</code>.</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">[</span>root@ecsmaster1 ~]# oc <span class="nt">-n</span> workspace1-user-1 describe pod wifz6t8mvxv5ghwy | <span class="nb">grep </span>Node:
 Node:         ecsgpu.cdpkvm.cldr/10.15.4.185
    
 <span class="o">[</span>root@ecsmaster1 ~]# oc <span class="nt">-n</span> workspace1-user-1 describe pod wifz6t8mvxv5ghwy | <span class="nb">grep</span> <span class="nt">-B2</span> <span class="nt">-i</span> nvidia
     Limits:
         memory:          7714196Ki
         nvidia.com/gpu:  1
     <span class="nt">--</span>
         cpu:             1960m
         memory:          7714196Ki
         nvidia.com/gpu:  1
     <span class="nt">--</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>When a process is consuming the Nvidia GPU, the output of <code class="language-plaintext highlighter-rouge">nvidia-smi</code> tool will show the PID of that process (in this case, the CML session pod).</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="o">[</span>root@ecsgpu ~]# nvidia-smi
 Thu Aug 25 13:58:40 2022       
 +-----------------------------------------------------------------------------+
 | NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |
 |-------------------------------+----------------------+----------------------+
 | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
 | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
 |                               |                      |               MIG M. |
 |<span class="o">===============================</span>+<span class="o">======================</span>+<span class="o">======================</span>|
 |   0  NVIDIA A100-PCI...  Off  | 00000000:08:00.0 Off |                    0 |
 | N/A   29C    P0    35W / 250W |  39185MiB / 40960MiB |      0%      Default |
 |                               |                      |             Disabled |
 +-------------------------------+----------------------+----------------------+
                                                                   
 +-----------------------------------------------------------------------------+
 | Processes:                                                                  |
 |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
 |        ID   ID                                                   Usage      |
 |<span class="o">=============================================================================</span>|
 |   0   N/A  N/A     29990      C   /usr/local/bin/python3.9       39183MiB   |
 +-----------------------------------------------------------------------------+
</code></pre></div>    </div>
  </li>
  <li>
    <p>In the event the ECS platform has no available worker node with GPU card, provisioning a session with GPU will result in <code class="language-plaintext highlighter-rouge">Pending</code> state as the system is looking for a worker node installed with at least one Nvidia GPU card.</p>

    <p><img src="../../assets/images/gpu/gpuecssession5.png" alt="" /></p>
  </li>
</ol>


            
          </div>
           <div class="chrome:sidebar">
  
    
      
      
      
      <button
        id="sidebar_item:ansible foundry"
        class="chrome:sidebar_item chrome:sidebar_item_collapsible chrome:sidebar_item_collapsed"
        onclick="toggleAccordian('sidebar_item:ansible foundry')"
      >
        <span class="chrome:sidebar_item_text">Ansible Foundry</span>
        <div class="chrome:collapsible_sub_nav_item_icon">
  <svg
    xmlns="http://www.w3.org/2000/svg"
    width="12"
    height="12"
    viewBox="0 0 12 12"
    focusable="false"
    role="presentation"
  >
    <path
      fill="currentColor"
      d="M1.646 3.646a.5.5 0 01.638-.057l.07.057L6 7.293l3.646-3.647a.5.5 0 01.638-.057l.07.057a.5.5 0 01.057.638l-.057.07-4 4a.5.5 0 01-.638.057l-.07-.057-4-4a.5.5 0 010-.708z"
    ></path>
  </svg>
</div>

      </button>
      <div id="sidebar_item:ansible foundry:accordian"
        class="chrome:sidebar_item_accordian chrome:sidebar_item_accordian_collapsed">
        
          
          
          
          <a href="/docs/content/ansible_intro/" class="chrome:sidebar_item text-sm  chrome:sidebar_item_child">
            Introduction
          </a>
          
        
          
          
          
          <a href="/docs/content/ansible_developers/" class="chrome:sidebar_item text-sm  chrome:sidebar_item_child">
            Goal-oriented Guides
          </a>
          
        
          
          
          
          <a href="/docs/content/ansible_info/" class="chrome:sidebar_item text-sm  chrome:sidebar_item_child">
            References
          </a>
          
        
          
          
          
          <a href="/docs/content/ansible_troubleshoot/" class="chrome:sidebar_item text-sm  chrome:sidebar_item_child">
            Operations
          </a>
          
        
      </div>
    
  
    
      
      
      
      <button
        id="sidebar_item:cdp pvc data services"
        class="chrome:sidebar_item chrome:sidebar_item_collapsible chrome:sidebar_item_expanded"
        onclick="toggleAccordian('sidebar_item:cdp pvc data services')"
      >
        <span class="chrome:sidebar_item_text">CDP PvC Data Services</span>
        <div class="chrome:collapsible_sub_nav_item_icon">
  <svg
    xmlns="http://www.w3.org/2000/svg"
    width="12"
    height="12"
    viewBox="0 0 12 12"
    focusable="false"
    role="presentation"
  >
    <path
      fill="currentColor"
      d="M1.646 3.646a.5.5 0 01.638-.057l.07.057L6 7.293l3.646-3.647a.5.5 0 01.638-.057l.07.057a.5.5 0 01.057.638l-.057.07-4 4a.5.5 0 01-.638.057l-.07-.057-4-4a.5.5 0 010-.708z"
    ></path>
  </svg>
</div>

      </button>
      <div id="sidebar_item:cdp pvc data services:accordian"
        class="chrome:sidebar_item_accordian chrome:sidebar_item_accordian_expanded">
        
          
          
          
          <a href="/docs/content/ecs_env/" class="chrome:sidebar_item text-sm  chrome:sidebar_item_child">
            ECS: Client
          </a>
          
        
          
          
          
          <a href="/docs/content/ecs_lvm/" class="chrome:sidebar_item text-sm  chrome:sidebar_item_child">
            ECS: LVM
          </a>
          
        
          
          
          
          <a href="/docs/content/ecs_gpu/" class="chrome:sidebar_item text-sm chrome:active_sidebar_item chrome:sidebar_item_child">
            ECS: GPU
          </a>
          
        
          
          
          
          <a href="/docs/content/ecs_addnode/" class="chrome:sidebar_item text-sm  chrome:sidebar_item_child">
            ECS: Add ECS Node
          </a>
          
        
          
          
          
          <a href="/docs/content/ecs_removenode/" class="chrome:sidebar_item text-sm  chrome:sidebar_item_child">
            ECS: Remove Node
          </a>
          
        
          
          
          
          <a href="/docs/content/ocp_gpu/" class="chrome:sidebar_item text-sm  chrome:sidebar_item_child">
            OCP: GPU
          </a>
          
        
          
          
          
          <a href="/docs/content/ocp_cdwdisk/" class="chrome:sidebar_item text-sm  chrome:sidebar_item_child">
            OCP: CDW Disk
          </a>
          
        
          
          
          
          <a href="/docs/content/cml_autotls/" class="chrome:sidebar_item text-sm  chrome:sidebar_item_child">
            CML: CSR with AutoTLS
          </a>
          
        
      </div>
    
  
    
      
      
      
      <a href="/docs/development/" class="chrome:sidebar_item ">
        <span class="chrome:sidebar_item_text">Coming Soon</span>
      </a>
      
    
  
</div>
 
        </div>
        <footer class="chrome:footer"><div class="chrome:footer_item">
    <span class="text-lg"
      >Updated: 2022-11-21 15:51</span
    >
  </div>
</footer>

      </div>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>
        let theme = "forest";
        mermaid.initialize({ startOnLoad: true, theme });
        window.mermaid.init(undefined, document.querySelectorAll('.language-mermaid'));
    </script>
    
  </body>
</html>
